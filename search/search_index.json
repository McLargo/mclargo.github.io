{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to McLargo's technical wiki What you can find in here This MUST be a dynamic documentation, constantly updated. Any new tool learn or good practice to include in my daily work must have a space in this project. Also, guidelines/ways of working applicable, no matter my current project. Grouped in different categories: Topic Description Architecture Related to patterns, architecture designs... DevOps Related to deployments, ci/cd... Programming Directly related to programming Project management Project management stuff Utils Misc of other utils required for daily basics With time and more documents covered, relation between them will be enriched. It shouldn't be a just isolated documents, but more of a wiki to cover my professional philosophy and understanding on technical topics. Using markdown to make it easier to build new documents from IDE. Template sample available for this wiki . Using markdown good practices . Highlighted practices are: Don't use more than one blank line. Remove extra spaces at the end of lines. Always uses spaces instead of hard tabs. Only one H1 per document. Header levels should increment by one -- don't skip levels. Limit line length to 100 characters. Mkdocs as site generator for my documentation. Includes nice features as search, customizable layout and easy deployment to github pages, making it 100% online. Repo that generate this website, can be found in my github. What you cannot find in here cheatsheets, they belong to gists . It enables quicker updates. code, that belongs to github . Either private/public repo. mentoring assets interview process/reviews. Those are not public. books or manuals. Check in Google Drive. References mkdocs mkdocs catalog","title":"Welcome to McLargo's technical wiki"},{"location":"#welcome-to-mclargos-technical-wiki","text":"","title":"Welcome to McLargo's technical wiki"},{"location":"#what-you-can-find-in-here","text":"This MUST be a dynamic documentation, constantly updated. Any new tool learn or good practice to include in my daily work must have a space in this project. Also, guidelines/ways of working applicable, no matter my current project. Grouped in different categories: Topic Description Architecture Related to patterns, architecture designs... DevOps Related to deployments, ci/cd... Programming Directly related to programming Project management Project management stuff Utils Misc of other utils required for daily basics With time and more documents covered, relation between them will be enriched. It shouldn't be a just isolated documents, but more of a wiki to cover my professional philosophy and understanding on technical topics. Using markdown to make it easier to build new documents from IDE. Template sample available for this wiki . Using markdown good practices . Highlighted practices are: Don't use more than one blank line. Remove extra spaces at the end of lines. Always uses spaces instead of hard tabs. Only one H1 per document. Header levels should increment by one -- don't skip levels. Limit line length to 100 characters. Mkdocs as site generator for my documentation. Includes nice features as search, customizable layout and easy deployment to github pages, making it 100% online. Repo that generate this website, can be found in my github.","title":"What you can find in here"},{"location":"#what-you-cannot-find-in-here","text":"cheatsheets, they belong to gists . It enables quicker updates. code, that belongs to github . Either private/public repo. mentoring assets interview process/reviews. Those are not public. books or manuals. Check in Google Drive.","title":"What you cannot find in here"},{"location":"#references","text":"mkdocs mkdocs catalog","title":"References"},{"location":"template/","text":"Template Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Head1 Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Head2 Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. References Lorem impsum","title":"Template"},{"location":"template/#template","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.","title":"Template"},{"location":"template/#head1","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.","title":"Head1"},{"location":"template/#head2","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.","title":"Head2"},{"location":"template/#references","text":"Lorem impsum","title":"References"},{"location":"architecture/api-design/","text":"API Design Briefly description of what should be consider when designing and developing an API, some good practices. Design good API from starters is important, as they can grow quickly and it is hard to change them later. Also, new developers can have a quicker ramp up to the project. Good practices RESTful API endpoints in plural use - instead of _ don't use verbs, use names. actions are implicit in HTTP methods basic structure GET ALL `api/<resources>` HTTP status code 200 OK for a successful GET of resource, or empty results. GET BY ID `api/<resources>/<id>` HTTP status code 200 OK for a successful GET of resource. HTTP status code 404 KO for a KO GET of a not found resource. POST `api/<resources>` If a resource has been created on the origin server, the response SHOULD be 201 (Created) and contain an entity which describes the status of the request and refers to the new resource, and a Location header (see section 14.30). PUT `api/<resources>/<id>` HTTP status code 200 OK for a successful PUT of an update to an existing resource. No response body needed. (Per Section 9.6, 204 No Content is even more appropriate.) HTTP status code 400 Bad Request for an unsuccessful PUT, with natural-language text (such as English) in the response body that explains why the PUT failed. (RFC 2616 Section 10.4) HTTP Status Code 2XX -> success codes range 3XX -> redirect codes range 4XX -> client error codes range 5XX -> server error codes range More details But, HTTP status codes are not enough to explain what went wrong. To help your API consumers, include a structured JSON error message, like: Error code: A machine-readable error code that identifies the specific error condition. Error message: A human-readable message that provides a detailed explanation of the error. Error context: Additional information related to the error, such as the request ID, the request parameters that caused the error, or the field(s) in the request that caused the error. Error links: URLs to resources or documentation that provide additional information about the error and how it can be resolved. Timestamp: The time when the error occurred. Secrets Never store unencrypted secrets in .git repositories. If a secret enters a repository, private or public, then it should be considered compromised. Add sensitive files in .gitignore Store secrets safely. Encrypting your secrets using common tools, such as git secret. Storing them within a git repository can be beneficial when working in teams as it keeps secrets synced. Or use \"Secrets as a service\" solutions, such as AWS Secrets Manager, Vault, Google Secrets Manager... References API Design Pragmatic restful API Naming rest API endpoints PUT vs POST","title":"API Design"},{"location":"architecture/api-design/#api-design","text":"Briefly description of what should be consider when designing and developing an API, some good practices. Design good API from starters is important, as they can grow quickly and it is hard to change them later. Also, new developers can have a quicker ramp up to the project.","title":"API Design"},{"location":"architecture/api-design/#good-practices-restful-api","text":"endpoints in plural use - instead of _ don't use verbs, use names. actions are implicit in HTTP methods basic structure GET ALL `api/<resources>` HTTP status code 200 OK for a successful GET of resource, or empty results. GET BY ID `api/<resources>/<id>` HTTP status code 200 OK for a successful GET of resource. HTTP status code 404 KO for a KO GET of a not found resource. POST `api/<resources>` If a resource has been created on the origin server, the response SHOULD be 201 (Created) and contain an entity which describes the status of the request and refers to the new resource, and a Location header (see section 14.30). PUT `api/<resources>/<id>` HTTP status code 200 OK for a successful PUT of an update to an existing resource. No response body needed. (Per Section 9.6, 204 No Content is even more appropriate.) HTTP status code 400 Bad Request for an unsuccessful PUT, with natural-language text (such as English) in the response body that explains why the PUT failed. (RFC 2616 Section 10.4)","title":"Good practices RESTful API"},{"location":"architecture/api-design/#http-status-code","text":"2XX -> success codes range 3XX -> redirect codes range 4XX -> client error codes range 5XX -> server error codes range More details But, HTTP status codes are not enough to explain what went wrong. To help your API consumers, include a structured JSON error message, like: Error code: A machine-readable error code that identifies the specific error condition. Error message: A human-readable message that provides a detailed explanation of the error. Error context: Additional information related to the error, such as the request ID, the request parameters that caused the error, or the field(s) in the request that caused the error. Error links: URLs to resources or documentation that provide additional information about the error and how it can be resolved. Timestamp: The time when the error occurred.","title":"HTTP Status Code"},{"location":"architecture/api-design/#secrets","text":"Never store unencrypted secrets in .git repositories. If a secret enters a repository, private or public, then it should be considered compromised. Add sensitive files in .gitignore Store secrets safely. Encrypting your secrets using common tools, such as git secret. Storing them within a git repository can be beneficial when working in teams as it keeps secrets synced. Or use \"Secrets as a service\" solutions, such as AWS Secrets Manager, Vault, Google Secrets Manager...","title":"Secrets"},{"location":"architecture/api-design/#references","text":"API Design Pragmatic restful API Naming rest API endpoints PUT vs POST","title":"References"},{"location":"architecture/architecture-decision-record/","text":"Architecture decision record Usually, decision on a project are made verbally, or during a discussion in chat/email, across few people (colleagues, customer...) inside a context. If we don't document these decisions, it is going to be hard to remember why decision was made in few months. If those documents are missing, next generation of developers can just blindly accept it (team will be afraid of make any changes they don't understand) or blindly change it (project can be damage or have side effects). With these documents included inside the project, makes them easier to related and maintain. These documents needs to be simply (low effort is put, but very agile), and devs don't get lazy to create them. Help onboarding people to get up to speed in a asynchronous way. A missing decision can be identify during PR review, when new patter/library is added without corresponding ADR document. Even enough small decisions can compound into a future problem that requires a large process or effort Components Title: brief description of the decision Context: explains the context and facts behind the decision Decision: longer statements of the decision made Status: [Proposed, Accepted, Deprecated, Superseded] Consequences: how it will impact the project itself Tips for using Lightweight ADR Use markdown and store along with the component it relates to in source control Number the files sequentially, don't reuse it (just update if required) Keep it brief and use plain, easy to understand language Peer review as you would code For cross cutting decisions that affect multiple components consider making a separate \"architecture\" repository When you make a decision document it immediately! References Lightweight decision records Documenting architecture decision Write decision record","title":"Architecture Decision Records"},{"location":"architecture/architecture-decision-record/#architecture-decision-record","text":"Usually, decision on a project are made verbally, or during a discussion in chat/email, across few people (colleagues, customer...) inside a context. If we don't document these decisions, it is going to be hard to remember why decision was made in few months. If those documents are missing, next generation of developers can just blindly accept it (team will be afraid of make any changes they don't understand) or blindly change it (project can be damage or have side effects). With these documents included inside the project, makes them easier to related and maintain. These documents needs to be simply (low effort is put, but very agile), and devs don't get lazy to create them. Help onboarding people to get up to speed in a asynchronous way. A missing decision can be identify during PR review, when new patter/library is added without corresponding ADR document. Even enough small decisions can compound into a future problem that requires a large process or effort","title":"Architecture decision record"},{"location":"architecture/architecture-decision-record/#components","text":"Title: brief description of the decision Context: explains the context and facts behind the decision Decision: longer statements of the decision made Status: [Proposed, Accepted, Deprecated, Superseded] Consequences: how it will impact the project itself","title":"Components"},{"location":"architecture/architecture-decision-record/#tips-for-using-lightweight-adr","text":"Use markdown and store along with the component it relates to in source control Number the files sequentially, don't reuse it (just update if required) Keep it brief and use plain, easy to understand language Peer review as you would code For cross cutting decisions that affect multiple components consider making a separate \"architecture\" repository When you make a decision document it immediately!","title":"Tips for using Lightweight ADR"},{"location":"architecture/architecture-decision-record/#references","text":"Lightweight decision records Documenting architecture decision Write decision record","title":"References"},{"location":"architecture/asynchronous-messaging/","text":"Asynchronous messaging An asynchronous messaging system is a system that allows communication between two or more parties without the need for the sender to wait for the receiver to respond. The sender sends the message and continues with its own processing. The receiver receives the message and processes it. The receiver can then send a response message to the sender, which can be processed by the sender. Producer and consumer are different processes. There are two main messaging patterns: message queuing and publish/subscribe. Message queuing Multiple producers can send message to the same queue, but once the message is consumed, is removed. Only one consumer consume a specific message. Publish/subscribe Multiple produced can publish a message to a topic, and several consumers can subscribe to the same topic, consuming and handle the message in different ways. Messages can be ephemeral or durable. Used for event-driven architecture . RabbitMQ RabbitMQ supports classic message queuing out of the box. A developer defines named queues, and then publishers can send messages to that named queue. Consumers, in turn, use the same queue to retrieve messages to process them. Kafka Topics Kafka doesn\u2019t implement the notion of a queue. Instead, Kafka stores collections of records in categories called topics . References RabbitMQ vs Kafka","title":"Asynchronous messaging"},{"location":"architecture/asynchronous-messaging/#asynchronous-messaging","text":"An asynchronous messaging system is a system that allows communication between two or more parties without the need for the sender to wait for the receiver to respond. The sender sends the message and continues with its own processing. The receiver receives the message and processes it. The receiver can then send a response message to the sender, which can be processed by the sender. Producer and consumer are different processes. There are two main messaging patterns: message queuing and publish/subscribe.","title":"Asynchronous messaging"},{"location":"architecture/asynchronous-messaging/#message-queuing","text":"Multiple producers can send message to the same queue, but once the message is consumed, is removed. Only one consumer consume a specific message.","title":"Message queuing"},{"location":"architecture/asynchronous-messaging/#publishsubscribe","text":"Multiple produced can publish a message to a topic, and several consumers can subscribe to the same topic, consuming and handle the message in different ways. Messages can be ephemeral or durable. Used for event-driven architecture .","title":"Publish/subscribe"},{"location":"architecture/asynchronous-messaging/#rabbitmq","text":"RabbitMQ supports classic message queuing out of the box. A developer defines named queues, and then publishers can send messages to that named queue. Consumers, in turn, use the same queue to retrieve messages to process them.","title":"RabbitMQ"},{"location":"architecture/asynchronous-messaging/#kafka-topics","text":"Kafka doesn\u2019t implement the notion of a queue. Instead, Kafka stores collections of records in categories called topics .","title":"Kafka Topics"},{"location":"architecture/asynchronous-messaging/#references","text":"RabbitMQ vs Kafka","title":"References"},{"location":"architecture/design-patterns/","text":"Design Patterns In software engineering, a design pattern is a general repeatable solution to a commonly occurring problem in software design. A design pattern isn't a finished design that can be transformed directly into code. It is a description or template for how to solve a problem that can be used in many different situations. Uses of design patterns Utilizing design patterns in the development process can accelerate progress by offering tried-and-tested development models. Effective software design necessitates the consideration of issues that may only surface during later implementation stages. The utilization of design patterns aids in mitigating subtle problems that can lead to significant complications, and it enhances code readability for developers and architects who are acquainted with these patterns. These design patterns offer general solutions, presented in a format that is not bound to specific problems. They serve as templates or blueprints for structuring code to address specific challenges. Furthermore, design patterns enable developers to communicate using widely recognized and comprehensible terms for software interactions. Over time, common design patterns can be refined, rendering them more robust compared to improvised design approaches. Design patterns can be categorized into several different types based on their primary purpose and the problems they solve. Here are some of the most well-known design pattern categories: Creational Patterns These patterns focus on object creation mechanisms, abstracting the process of object instantiation. Common creational patterns include: Singleton Pattern : Ensures a class has only one instance and provides a global point of access to it. Factory Method Pattern : Defines an interface for creating objects, but allows subclasses to alter the type of objects that will be created. Abstract Factory Pattern : Provides an interface for creating families of related or dependent objects without specifying their concrete classes. Structural Patterns These patterns deal with the composition of classes or objects to form larger structures. They help in defining how objects and classes can be combined to form more complex structures. Common structural patterns include: Adapter Pattern : Allows the interface of an existing class to be used as another interface. Decorator Pattern : Attaches additional responsibilities to an object dynamically. Decorators provide a flexible alternative to subclassing for extending functionality. Composite Pattern : Composes objects into tree structures to represent part-whole hierarchies. Clients can treat individual objects and compositions of objects uniformly. Behavioral Patterns These patterns focus on the communication between objects, defining how they interact and distribute responsibilities. Common behavioral patterns include: Observer Pattern : Defines a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically. Strategy Pattern : Defines a family of algorithms, encapsulates each one, and makes them interchangeable. It allows the algorithm to vary independently from clients that use it. Command Pattern : Encapsulates a request as an object, thereby allowing for parameterization of clients with queues, requests, and operations. Architectural Patterns These are higher-level patterns that guide the overall structure and organization of software systems. Examples include: Model-View-Controller (MVC) : Separates an application into three interconnected components - Model (data and business logic), View (user interface), and Controller (user input handling). Model-View-ViewModel (MVVM) : An architectural pattern often used in GUI-based applications, similar to MVC but tailored for modern UI frameworks. Concurrency Patterns These patterns address multi-threading and concurrent programming challenges. Examples include the \"Thread Pool\" and \"Producer-Consumer\" patterns. Anti-Patterns While not traditional design patterns, anti-patterns describe common mistakes or pitfalls in software design and development. Recognizing and avoiding these anti-patterns is crucial for writing maintainable and efficient code. These design patterns are valuable tools for software developers to enhance code quality, maintainability, and reusability by providing well-established solutions to recurring design problems. References Design patterns Design pattern Python","title":"Design Patterns"},{"location":"architecture/design-patterns/#design-patterns","text":"In software engineering, a design pattern is a general repeatable solution to a commonly occurring problem in software design. A design pattern isn't a finished design that can be transformed directly into code. It is a description or template for how to solve a problem that can be used in many different situations.","title":"Design Patterns"},{"location":"architecture/design-patterns/#uses-of-design-patterns","text":"Utilizing design patterns in the development process can accelerate progress by offering tried-and-tested development models. Effective software design necessitates the consideration of issues that may only surface during later implementation stages. The utilization of design patterns aids in mitigating subtle problems that can lead to significant complications, and it enhances code readability for developers and architects who are acquainted with these patterns. These design patterns offer general solutions, presented in a format that is not bound to specific problems. They serve as templates or blueprints for structuring code to address specific challenges. Furthermore, design patterns enable developers to communicate using widely recognized and comprehensible terms for software interactions. Over time, common design patterns can be refined, rendering them more robust compared to improvised design approaches. Design patterns can be categorized into several different types based on their primary purpose and the problems they solve. Here are some of the most well-known design pattern categories:","title":"Uses of design patterns"},{"location":"architecture/design-patterns/#creational-patterns","text":"These patterns focus on object creation mechanisms, abstracting the process of object instantiation. Common creational patterns include: Singleton Pattern : Ensures a class has only one instance and provides a global point of access to it. Factory Method Pattern : Defines an interface for creating objects, but allows subclasses to alter the type of objects that will be created. Abstract Factory Pattern : Provides an interface for creating families of related or dependent objects without specifying their concrete classes.","title":"Creational Patterns"},{"location":"architecture/design-patterns/#structural-patterns","text":"These patterns deal with the composition of classes or objects to form larger structures. They help in defining how objects and classes can be combined to form more complex structures. Common structural patterns include: Adapter Pattern : Allows the interface of an existing class to be used as another interface. Decorator Pattern : Attaches additional responsibilities to an object dynamically. Decorators provide a flexible alternative to subclassing for extending functionality. Composite Pattern : Composes objects into tree structures to represent part-whole hierarchies. Clients can treat individual objects and compositions of objects uniformly.","title":"Structural Patterns"},{"location":"architecture/design-patterns/#behavioral-patterns","text":"These patterns focus on the communication between objects, defining how they interact and distribute responsibilities. Common behavioral patterns include: Observer Pattern : Defines a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically. Strategy Pattern : Defines a family of algorithms, encapsulates each one, and makes them interchangeable. It allows the algorithm to vary independently from clients that use it. Command Pattern : Encapsulates a request as an object, thereby allowing for parameterization of clients with queues, requests, and operations.","title":"Behavioral Patterns"},{"location":"architecture/design-patterns/#architectural-patterns","text":"These are higher-level patterns that guide the overall structure and organization of software systems. Examples include: Model-View-Controller (MVC) : Separates an application into three interconnected components - Model (data and business logic), View (user interface), and Controller (user input handling). Model-View-ViewModel (MVVM) : An architectural pattern often used in GUI-based applications, similar to MVC but tailored for modern UI frameworks.","title":"Architectural Patterns"},{"location":"architecture/design-patterns/#concurrency-patterns","text":"These patterns address multi-threading and concurrent programming challenges. Examples include the \"Thread Pool\" and \"Producer-Consumer\" patterns.","title":"Concurrency Patterns"},{"location":"architecture/design-patterns/#anti-patterns","text":"While not traditional design patterns, anti-patterns describe common mistakes or pitfalls in software design and development. Recognizing and avoiding these anti-patterns is crucial for writing maintainable and efficient code. These design patterns are valuable tools for software developers to enhance code quality, maintainability, and reusability by providing well-established solutions to recurring design problems.","title":"Anti-Patterns"},{"location":"architecture/design-patterns/#references","text":"Design patterns Design pattern Python","title":"References"},{"location":"architecture/event-driven/","text":"Event-driven architecture Generally speaking, a microservice architecture needs that services talk to each other. No matter what approach is used, if Orchestration or Choreography , both can use an Event-driven architecture to communicate with the difference services involved. Events are trigger from services and other services can subscribe to them, and react upon the event information. Communication between services using events is asynchronous . Domain events A domain event is, something that happened in the domain that you want other parts of the same domain (in-process) to be aware of. More into details, we can distinguish between: Events represent a past, something that already happened and can\u2019t be undone. Commands, on the other hand, represent a wish, an action in the future which can be rejected. Database events Changes in database can also be used as events. For example, a database trigger when a row is inserted, updated or deleted, called CDC (Change Data Capture). References Domain event","title":"Event Driven"},{"location":"architecture/event-driven/#event-driven-architecture","text":"Generally speaking, a microservice architecture needs that services talk to each other. No matter what approach is used, if Orchestration or Choreography , both can use an Event-driven architecture to communicate with the difference services involved. Events are trigger from services and other services can subscribe to them, and react upon the event information. Communication between services using events is asynchronous .","title":"Event-driven architecture"},{"location":"architecture/event-driven/#domain-events","text":"A domain event is, something that happened in the domain that you want other parts of the same domain (in-process) to be aware of. More into details, we can distinguish between: Events represent a past, something that already happened and can\u2019t be undone. Commands, on the other hand, represent a wish, an action in the future which can be rejected.","title":"Domain events"},{"location":"architecture/event-driven/#database-events","text":"Changes in database can also be used as events. For example, a database trigger when a row is inserted, updated or deleted, called CDC (Change Data Capture).","title":"Database events"},{"location":"architecture/event-driven/#references","text":"Domain event","title":"References"},{"location":"architecture/grpc/","text":"How does gRPC work? RPC (Remote Procedure Call) is called \"remote\" because it enables communications between remote services when services are deployed to different servers under microservice architecture. From the user\u2019s point of view, it acts like a local function call. The diagram below illustrates the overall data flow for gRPC.","title":"gRPC"},{"location":"architecture/grpc/#how-does-grpc-work","text":"RPC (Remote Procedure Call) is called \"remote\" because it enables communications between remote services when services are deployed to different servers under microservice architecture. From the user\u2019s point of view, it acts like a local function call. The diagram below illustrates the overall data flow for gRPC.","title":"How does gRPC work?"},{"location":"architecture/microservices/","text":"Microservices It is architectural style that focuses on discrete services instead of a monolithic design . Microservices architecture breaks down an application into smaller, independent services that each serve a specific business capability or function. These services are developed and deployed independently, often with their own codebase and database. Principles Independent development and scaling of individual services. Developers can work on small, focused teams, and each service can be scaled independently based on its specific requirements and on demand, making resource allocation more efficient. Each microservice can se different technology stacks and databases for each service, depending on the requirements. This flexibility allows you to choose the best tool for each job. Microservices can be deployed independently, making it easier to implement CI/CD pipelines for each service. This results in faster and more frequent deployments with less risk. More resilient than a monolithic system. If one service fails, it doesn't necessarily impact the entire system, as long as other services can continue to function. Managing the interactions between multiple services can introduce complexity in terms of service discovery, communication, and monitoring. Orchestration Orchestration is an approach where one of the services acts as the coordinator and orchestrator of the overall distributed state change, managing the execution of tasks or services It defines the order and flow of activities and often involves a central point of control that dictates what should happen next. The orchestrator is responsible for making decisions and ensuring that each component of the system performs its tasks in the prescribed sequence. It can be simpler to design and implement for relatively small-scale systems with a limited number of components. However, as the system grows in complexity, the central orchestrator can become a bottleneck and may make the system less scalable and more challenging to maintain. It is easier to implement fault tolerance and error handling because the central orchestrator can monitor the execution of tasks and react to failures by rerouting or retrying steps. It also provides a central point for monitoring and logging, making it easier to track the progress of tasks and diagnose issues. There is less direct communication between services since most interactions are mediated by the orchestrator. This can reduce the need for services to be aware of each other's existence. Choreography Choreography is an approach of service coordination where participants exchange events without a centralized point of control. With this pattern, each service performs a local transaction and publishes events that trigger local transactions in other services. Each component of the system participates in decision-making about a business transaction's workflow, instead of relying on a central point of control. The interaction and coordination between services are determined by a set of predefined rules or contracts. There is no single point of control, and the overall behavior of the system emerges from the interactions between the participating components. It can be more suitable for large-scale and complex distributed systems because it distributes the decision-making and control among individual components. As new services are added or existing ones are modified, the system can remain more flexible and scalable. However, choreographed systems can be harder to visualize and debug, especially as the number of interactions between services increases. Also, relies on the individual services to handle errors and failures gracefully, making it more challenging to ensure consistent error-handling strategies across the entire system. It may require more advanced monitoring and tracing tools to gain insights into the interactions between services and to identify bottlenecks or errors. Finally, it requires services to communicate directly with each other, which can lead to more inter-service communication. This increased communication can result in more complex service discovery and network traffic management. References Orchestration vs choreography Saga pattern","title":"Microservices"},{"location":"architecture/microservices/#microservices","text":"It is architectural style that focuses on discrete services instead of a monolithic design . Microservices architecture breaks down an application into smaller, independent services that each serve a specific business capability or function. These services are developed and deployed independently, often with their own codebase and database.","title":"Microservices"},{"location":"architecture/microservices/#principles","text":"Independent development and scaling of individual services. Developers can work on small, focused teams, and each service can be scaled independently based on its specific requirements and on demand, making resource allocation more efficient. Each microservice can se different technology stacks and databases for each service, depending on the requirements. This flexibility allows you to choose the best tool for each job. Microservices can be deployed independently, making it easier to implement CI/CD pipelines for each service. This results in faster and more frequent deployments with less risk. More resilient than a monolithic system. If one service fails, it doesn't necessarily impact the entire system, as long as other services can continue to function. Managing the interactions between multiple services can introduce complexity in terms of service discovery, communication, and monitoring.","title":"Principles"},{"location":"architecture/microservices/#orchestration","text":"Orchestration is an approach where one of the services acts as the coordinator and orchestrator of the overall distributed state change, managing the execution of tasks or services It defines the order and flow of activities and often involves a central point of control that dictates what should happen next. The orchestrator is responsible for making decisions and ensuring that each component of the system performs its tasks in the prescribed sequence. It can be simpler to design and implement for relatively small-scale systems with a limited number of components. However, as the system grows in complexity, the central orchestrator can become a bottleneck and may make the system less scalable and more challenging to maintain. It is easier to implement fault tolerance and error handling because the central orchestrator can monitor the execution of tasks and react to failures by rerouting or retrying steps. It also provides a central point for monitoring and logging, making it easier to track the progress of tasks and diagnose issues. There is less direct communication between services since most interactions are mediated by the orchestrator. This can reduce the need for services to be aware of each other's existence.","title":"Orchestration"},{"location":"architecture/microservices/#choreography","text":"Choreography is an approach of service coordination where participants exchange events without a centralized point of control. With this pattern, each service performs a local transaction and publishes events that trigger local transactions in other services. Each component of the system participates in decision-making about a business transaction's workflow, instead of relying on a central point of control. The interaction and coordination between services are determined by a set of predefined rules or contracts. There is no single point of control, and the overall behavior of the system emerges from the interactions between the participating components. It can be more suitable for large-scale and complex distributed systems because it distributes the decision-making and control among individual components. As new services are added or existing ones are modified, the system can remain more flexible and scalable. However, choreographed systems can be harder to visualize and debug, especially as the number of interactions between services increases. Also, relies on the individual services to handle errors and failures gracefully, making it more challenging to ensure consistent error-handling strategies across the entire system. It may require more advanced monitoring and tracing tools to gain insights into the interactions between services and to identify bottlenecks or errors. Finally, it requires services to communicate directly with each other, which can lead to more inter-service communication. This increased communication can result in more complex service discovery and network traffic management.","title":"Choreography"},{"location":"architecture/microservices/#references","text":"Orchestration vs choreography Saga pattern","title":"References"},{"location":"architecture/monolithic/","text":"Monolithic architecture In a monolithic architecture, the entire application is built as a single, tightly integrated unit. All components, modules, and functionalities are part of a single codebase and run within the same process. As an alternative to this architecture design, we can find service-oriented architecture (SOA) such as microservices . Principles Developers can easily share code between modules and functionalities, but development and maintenance can be difficult as the application grows. Understanding the entire codebase can become increasingly difficult Scaling can be a challenge, as entire application needs to be scaled to support extra load of a single component. Are built using a single technology stack and database. Any changes or updates of technology affects entire application. Deploying changes to a monolithic application often requires updating the entire application, which can lead to longer deployment times and potential disruptions. Low fault tolerance and resilience, a single component it can potentially bring down the entire application.","title":"Monolithic Architecture"},{"location":"architecture/monolithic/#monolithic-architecture","text":"In a monolithic architecture, the entire application is built as a single, tightly integrated unit. All components, modules, and functionalities are part of a single codebase and run within the same process. As an alternative to this architecture design, we can find service-oriented architecture (SOA) such as microservices .","title":"Monolithic architecture"},{"location":"architecture/monolithic/#principles","text":"Developers can easily share code between modules and functionalities, but development and maintenance can be difficult as the application grows. Understanding the entire codebase can become increasingly difficult Scaling can be a challenge, as entire application needs to be scaled to support extra load of a single component. Are built using a single technology stack and database. Any changes or updates of technology affects entire application. Deploying changes to a monolithic application often requires updating the entire application, which can lead to longer deployment times and potential disruptions. Low fault tolerance and resilience, a single component it can potentially bring down the entire application.","title":"Principles"},{"location":"architecture/solid/","text":"SOLID Principles SOLID is an acronym of the first five object-oriented design (OOD) principles. These principles establish practices that lend to developing software with considerations for maintaining and extending as the project grows. Adopting these practices can also contribute to avoiding code smells, refactoring code, and Agile or Adaptive software development. Single-Responsibility Principle A class should have only a single responsibility. Only one potential change in the software\u2019s specification should be able to affect the specification of the class. Open/Closed Principle Software entities should be open for EXTENSION, but closed for MODIFICATION. Allow behavior to be extended without modifying the source code. Liskov Substitution Principle Objects in a program should be replaceable with instances of their subtypes without altering the correctness of that program. Interface Segregation Principle Many client-specific interfaces are better than one general-purpose interface. No client should be forced to depend on methods it does not use. Dependency Inversion Principle One should depend upon abstractions, not concretions. High-level modules should not depend on low-level modules. Both should depend on abstractions. Abstractions should not depend on details. Details should depend on abstractions. References Skteches for the SOLID principles Writing code with SOLID","title":"Solid Principles"},{"location":"architecture/solid/#solid-principles","text":"SOLID is an acronym of the first five object-oriented design (OOD) principles. These principles establish practices that lend to developing software with considerations for maintaining and extending as the project grows. Adopting these practices can also contribute to avoiding code smells, refactoring code, and Agile or Adaptive software development.","title":"SOLID Principles"},{"location":"architecture/solid/#single-responsibility-principle","text":"A class should have only a single responsibility. Only one potential change in the software\u2019s specification should be able to affect the specification of the class.","title":"Single-Responsibility Principle"},{"location":"architecture/solid/#openclosed-principle","text":"Software entities should be open for EXTENSION, but closed for MODIFICATION. Allow behavior to be extended without modifying the source code.","title":"Open/Closed Principle"},{"location":"architecture/solid/#liskov-substitution-principle","text":"Objects in a program should be replaceable with instances of their subtypes without altering the correctness of that program.","title":"Liskov Substitution Principle"},{"location":"architecture/solid/#interface-segregation-principle","text":"Many client-specific interfaces are better than one general-purpose interface. No client should be forced to depend on methods it does not use.","title":"Interface Segregation Principle"},{"location":"architecture/solid/#dependency-inversion-principle","text":"One should depend upon abstractions, not concretions. High-level modules should not depend on low-level modules. Both should depend on abstractions. Abstractions should not depend on details. Details should depend on abstractions.","title":"Dependency Inversion Principle"},{"location":"architecture/solid/#references","text":"Skteches for the SOLID principles Writing code with SOLID","title":"References"},{"location":"devops/continuous-integration/","text":"Continuous integration Continuous integration (CI) is an agile and DevOps best practice where developers integrate their code changes early and often to the main branch or code repository. The goal is to reduce the risk of seeing \"integration hell\" by waiting for the end of a project or a sprint to merge the work of all developers. Since it automates deployment, it helps teams meet business requirements, improve code quality, and increase security. CI is responsible of maintenance of all team. It is not the responsibility of a single person. Automated testing To get the full benefits of CI, you will need to automate your tests to be able to run them for every change that is made to the main repository. We insist on running tests on every branch of your repository and not just focus on the main branch. This way you will be able to capture issues early and minimize disruptions for your team. Different types of tests, no need to cover all at starter. Unit tests are narrow in scope and typically verify the behavior of individual methods or functions. Integration tests make sure that multiple components behave correctly together. This can involve several classes as well as testing the integration with other services. Acceptance tests are similar to the integration tests but they focus on the business cases rather than the components themselves. UI tests will make sure that the application functions correctly from a user perspective. Run your tests automatically in each commits with a jenkins pipeline. Goal is to keep build green all the time, writing tests as part of the user stories and bugs. No merge is allowed if pipeline is not green. Keep good coverage of your code, at least 80% of your code should be covered by tests. References Continuous integration","title":"Continuous integration"},{"location":"devops/continuous-integration/#continuous-integration","text":"Continuous integration (CI) is an agile and DevOps best practice where developers integrate their code changes early and often to the main branch or code repository. The goal is to reduce the risk of seeing \"integration hell\" by waiting for the end of a project or a sprint to merge the work of all developers. Since it automates deployment, it helps teams meet business requirements, improve code quality, and increase security. CI is responsible of maintenance of all team. It is not the responsibility of a single person.","title":"Continuous integration"},{"location":"devops/continuous-integration/#automated-testing","text":"To get the full benefits of CI, you will need to automate your tests to be able to run them for every change that is made to the main repository. We insist on running tests on every branch of your repository and not just focus on the main branch. This way you will be able to capture issues early and minimize disruptions for your team. Different types of tests, no need to cover all at starter. Unit tests are narrow in scope and typically verify the behavior of individual methods or functions. Integration tests make sure that multiple components behave correctly together. This can involve several classes as well as testing the integration with other services. Acceptance tests are similar to the integration tests but they focus on the business cases rather than the components themselves. UI tests will make sure that the application functions correctly from a user perspective. Run your tests automatically in each commits with a jenkins pipeline. Goal is to keep build green all the time, writing tests as part of the user stories and bugs. No merge is allowed if pipeline is not green. Keep good coverage of your code, at least 80% of your code should be covered by tests.","title":"Automated testing"},{"location":"devops/continuous-integration/#references","text":"Continuous integration","title":"References"},{"location":"devops/docker/","text":"Docker In a nutshell, docker executes containers, which is a customize instance of a docker image. An image is all the instructions to encapsulate libraries, dependencies, environment variables, configuration... in a file called Dockerfile to enable easy initialization of new services, without install any local dependencies that can corrupt your system. It is like a virtual machine, but with the difference of you can type out from commands line and it is not a full OS, just the necessary to run the service. As images are reusable by team or the community, they are stored in a container image registry, which is a place to store images. Docker hub is the most popular, but for self-hosted, Artifactory or quay. After image is build, you can then start multiple containers of the same image, which are independent and can run in parallel. You can start/stop/access those containers. Totally portable to any OS. References Cheatsheet Get started portainer keep bash history","title":"Docker"},{"location":"devops/docker/#docker","text":"In a nutshell, docker executes containers, which is a customize instance of a docker image. An image is all the instructions to encapsulate libraries, dependencies, environment variables, configuration... in a file called Dockerfile to enable easy initialization of new services, without install any local dependencies that can corrupt your system. It is like a virtual machine, but with the difference of you can type out from commands line and it is not a full OS, just the necessary to run the service. As images are reusable by team or the community, they are stored in a container image registry, which is a place to store images. Docker hub is the most popular, but for self-hosted, Artifactory or quay. After image is build, you can then start multiple containers of the same image, which are independent and can run in parallel. You can start/stop/access those containers. Totally portable to any OS.","title":"Docker"},{"location":"devops/docker/#references","text":"Cheatsheet Get started portainer keep bash history","title":"References"},{"location":"devops/features-toggles/","text":"Features Toggles When new functionality is working together another set of changes, it is hard to reduce the impact. For this, we can enable in our code a set of flags to enable certain parts of the code. useNewAlgorithm = False # useNewAlgorithm = True # UNCOMMENT IF YOU ARE WORKING ON THE NEW SR ALGORITHM if useNewAlgorithm: return enhancedSplineReticulation(); return oldFashionedSplineReticulation(); But this is quite hardcoding, and if you are in a collaborative environment, you will still need to deploy your code to test. For that, Toggle Router came to the picture. It could be something fancy with an UI, or much simple. But the idea is to change dynamically those values. Stored in memory, config, database... whatever suits you. Enables many other release feature, like canary release, A/B testing... When to use features flags Improve feature rollout Operational efficiency Learn from experimentation Empower teams with entitlements References Feature Toggles Feature Flags","title":"Features Toggles"},{"location":"devops/features-toggles/#features-toggles","text":"When new functionality is working together another set of changes, it is hard to reduce the impact. For this, we can enable in our code a set of flags to enable certain parts of the code. useNewAlgorithm = False # useNewAlgorithm = True # UNCOMMENT IF YOU ARE WORKING ON THE NEW SR ALGORITHM if useNewAlgorithm: return enhancedSplineReticulation(); return oldFashionedSplineReticulation(); But this is quite hardcoding, and if you are in a collaborative environment, you will still need to deploy your code to test. For that, Toggle Router came to the picture. It could be something fancy with an UI, or much simple. But the idea is to change dynamically those values. Stored in memory, config, database... whatever suits you. Enables many other release feature, like canary release, A/B testing...","title":"Features Toggles"},{"location":"devops/features-toggles/#when-to-use-features-flags","text":"Improve feature rollout Operational efficiency Learn from experimentation Empower teams with entitlements","title":"When to use features flags"},{"location":"devops/features-toggles/#references","text":"Feature Toggles Feature Flags","title":"References"},{"location":"devops/helm/","text":"Helm K8s packaging is a virtual concept. When we have several objects related to the same service, we can group them together in a single package. But there is no object in k8s for that. Helm manages a group of resources in a single unit. This is called a chart , which contains all the object required in the unit to make an application or service to work. Helm charts Once a chart is created, a new folder is created with all the files required for helm to work. This folder is called chart . Explanation of the files: Chart.yaml: contains the metadata for the chart. values.yaml: set of properties that can be used to configure the chart. This contains the default values. charts folder: a Chart can depend on another Chart. If this happens, chart files will be inside this folder. templates folder: contains the templates for the resources that will be created. Is where helm find the templates for all the k8s objects required for your application. May some files are autogenerated by default. If you have predefined k8s files, you can delete them and place your own k8s manifest in a folder called k8s . Usually, some tweaks to the files are required. For example, the name of the service, default values... templates/NOTES.txt: contains the information that will be shown when the chart is installed. Templates A template is an intermediate file that will be used to generate the final k8s file. Representation is with {{ }} . For example, {{ .Values.image.repository }} will check in the values.yaml file, image object, and the value of key repository . In case --set is used, it will override the value in the values.yaml file. Another objects are available to be used in the templates aside from .Values . Release (corresponding to the release data) and Chart (corresponding to the chart metadata). Update a chart template will require an update of the version in charts.yaml . Inside file _helpers.tpl there are some functions to be used in the templates. Use with {{ template \"name\" . }} . The dot is the context. You can also define your custom functions. templates/NOTES.txt can also use templates related to the service, such as how to access the service or see secrets generated by helm. Templates functions range: is used to iterate over a list. Once we are inside the loop, context is modify with the entity we are iterated for. default: in case the value is not provider or empty string, use the other value provided. printf: to concatenate strings. quote: quote a string. pipeline functions like in bash. if/else statements are available. Reference Helm command cheatsheet Helm best practices Helm dashboard Helm 101 Helm functions","title":"Helm Charts"},{"location":"devops/helm/#helm","text":"K8s packaging is a virtual concept. When we have several objects related to the same service, we can group them together in a single package. But there is no object in k8s for that. Helm manages a group of resources in a single unit. This is called a chart , which contains all the object required in the unit to make an application or service to work.","title":"Helm"},{"location":"devops/helm/#helm-charts","text":"Once a chart is created, a new folder is created with all the files required for helm to work. This folder is called chart . Explanation of the files: Chart.yaml: contains the metadata for the chart. values.yaml: set of properties that can be used to configure the chart. This contains the default values. charts folder: a Chart can depend on another Chart. If this happens, chart files will be inside this folder. templates folder: contains the templates for the resources that will be created. Is where helm find the templates for all the k8s objects required for your application. May some files are autogenerated by default. If you have predefined k8s files, you can delete them and place your own k8s manifest in a folder called k8s . Usually, some tweaks to the files are required. For example, the name of the service, default values... templates/NOTES.txt: contains the information that will be shown when the chart is installed.","title":"Helm charts"},{"location":"devops/helm/#templates","text":"A template is an intermediate file that will be used to generate the final k8s file. Representation is with {{ }} . For example, {{ .Values.image.repository }} will check in the values.yaml file, image object, and the value of key repository . In case --set is used, it will override the value in the values.yaml file. Another objects are available to be used in the templates aside from .Values . Release (corresponding to the release data) and Chart (corresponding to the chart metadata). Update a chart template will require an update of the version in charts.yaml . Inside file _helpers.tpl there are some functions to be used in the templates. Use with {{ template \"name\" . }} . The dot is the context. You can also define your custom functions. templates/NOTES.txt can also use templates related to the service, such as how to access the service or see secrets generated by helm.","title":"Templates"},{"location":"devops/helm/#templates-functions","text":"range: is used to iterate over a list. Once we are inside the loop, context is modify with the entity we are iterated for. default: in case the value is not provider or empty string, use the other value provided. printf: to concatenate strings. quote: quote a string. pipeline functions like in bash. if/else statements are available.","title":"Templates functions"},{"location":"devops/helm/#reference","text":"Helm command cheatsheet Helm best practices Helm dashboard Helm 101 Helm functions","title":"Reference"},{"location":"devops/k8s/","text":"Kubernetes It is a high-disponiblity platform. It support different application available to be consume. You can use kubectl (core k8s cli tool) to give instructions to k8s. Instructions can be imperative and declarative (prefer approach). Declarative is where k8s take the lead regarding which are the actions needed to reach the desire state. Act -> observe -> diff -> Act or Desire vs Actual state k8s uses reconciliation to make the actual state looks like the desire state via control loop. Or by using imperative actions. k8s is usually the deployment tool for a microservices architecture, instead of monolithic application . Concepts Data is send to k8s through a yaml file, which is a declarative way to describe. Kind is equal to the object being described. Name must unique within the namespace for each Kind. spec information regarding this object representation. Create new or update resource with kubectl apply -f <file.yaml> Pod Can be reusable. It is a group of containers. It is the smallest unit in k8s. All containers for a pod will run in the same mode. They can talk to each other via localhost, and can share volume resources. A pod can have a container to run a web server application, where the code is located inside the image. But any time there is typo or a fix, it needs to build a new image and restart the container. This is called main container. Using side containers, we can have a shared volume between the main container to read the content from, separation od concern isolating and reusing. Each container can have different resources, like memory, cpu, etc. Services Load balancing for Pods. Use labels to determine target pods. Like a load balancer in front opf our pods. Service are not process running in k8s, is not consuming resources, is more like configuration or metadata. Deployments Deployments are an objects in k8s designed to avoid declaring the exact same pod metadata (except by the name, as it must be unique) when you need to deploy X instance of the same pod. Deployments does not create actually any pods, Instead, deployment create an object in the middle call replica sets.. Replica sets are used to make it easy to roll from one version of a deployment to another. Each time we update a deployment, a new replica set is created that contains the latest configuration. Replica set actually create the pods. Two type of strategies: RollingUpdate: value by default. new ReplicaSet is created, then scaled up as old one is scaled down. Recreate: remove all existing pods in the existing ReplicaSet before creating new ones from new ReplicaSet. Also, it is possible to configure another different deployment strategies as canary releases and blue/green. In a deployment, each endpoint is the IP address of a pod that is backing that service. In case of multiple replicas, we should see 1 endpoint for each pod created for a deployment. More configuration available in the official k8s documentation. StatefulSet It is an extension of deployment. It is used to manage stateful applications. Name is stable, unique pod identifiers and DNS names. Also, possible to set Persistent Storage and one Persistent Volumen per VolumeClaim template. Stick to pod as long as the pod is declared. Pods created in asc order and deleted in desc order. It is possible to scale up, but all pods must be in Running or Read. Labels Characteristics are: map of key/value pairs, try to standardize both organizational and functional (selector on services) indexes and searchable avoid compound labels values Architecture Worker nodes, where the containers are running and application. They have cpu, memory to handle the application. Each of the have docker installed and kubelet , which mainly determine the url of the k8s cluster. Control planes nodes: host the componentes that actually run k8s. like API server. Data stored is in etcd. Scheduler, determine where to run the pods. Controller manager, watch the state of the cluster and make changes to the cluster to match the desire state. Cloud controller manager, manage the interaction with the underlying cloud provider. Networking Traffic can happen in a cluster within a pod, pod to pod, service to pods and external to cluster. within a pod: containers within a pod can connect to each other using localhost. Containers share an IP address accesible throughout the cluster and share common port space. pod to pod: each pod has a unique IP address, so they can communicate with each other. IP address are routable anywhere within the cluster. Not common procedure. service to pod: service is a resource that provides layer-4 load balancing for a group of pods. Service discovery using the cluster's internal DNS. Types are: ClusterIP (virtual IP address that load balance request to backend pods, accessible only in the cluster). NodePort: used for external facing service, exposing a port to access externally. K8s will open the port in each worker node. external to cluster: service type LoadBalancer, external IP address is provisioned by the cloud provider. Creates and manages an external load balancer, managing traffic across all nodes. Another option is Ingress . It is a layer-7 load balancer. It is a resource that balance the traffic for one or more services. Ingress rules to balance traffic to specific service. An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting. An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic. Same rules as Nginx or Traefic can be applied to Ingress, meaning that traffic to a domain or a specific path can be redirect to the corresponding labelled service. Resource organization Have multiple clusters, to have highest level of isolation. Each cluster can have different purposes, like security, environments (development vs uat vs production), by geography... Namespaces are a grouping in k8s. There is no nesting, all our objects in k8s go to same namespace. Namespaces are a way to organize and isolate resources. Names of resources must be unique within a namespace. Security and access can be set per namespaces. Usually 1 namespace per project. kubeconfig It a file that contains all sections for k8s. clusters: url of the API service. Name can be use later for the kubeconfig file. context: where cluster, user and name are glue together. Names are used to identify the context. users: credentials to use to access the cluster. Name can be use later for the kubec config file. Volume It is a way to persist data. It is a directory that is accessible to containers. Volumes are exposed at pod level. By default, containers in a pod write to ephemeral storage (emptyDir), only available during the pod's lifecycle. Once pod is terminated, data is lost. But we can attach Persistent volumes to pods which persist any data written to them. They are network attached storage, independent of pod's life. We can static or dynamic volumes. Static volumes are created manually, dynamic volumes are created automatically when a pod is created. Storage is actually outside the cluster. I your k8s manifest, you will define what are your needs with a Persist Volume Claim. Then, when the request reaches the cluster, it will be matched with a Persist Volume (binding). If there is no match, it will create a new one. Secrets and dynamic configuration All runtime configuration should be able to be injected and overrides. Defaults are ok, but we should be able to override them. Secrets are a way to store sensitive information, like passwords, tokens, keys... They are stored in etcd, encrypted at rest. They are base64 encoded, but not encrypted. We can either provide defaults value if no configuration is provided or required configuration, where startup fails if no configuration is provided. ConfigMap is the object to store configuration. It is a key/value pair. Use to store configuration data and properties. We can load all keys (envFrom.configMapRef) or individual keys (env.valueFrom.configMapKeyRef) as environment variables. Another option is to mount the config map as a mounted volume, similar all keys or individual keys (changes don\u00b4t require restart). For sensitive information, Secret resource stores this kind of data. Must be Base64 encode, and they can be exposed to pod via environment variables or mounted volumes (optional, other secret mechanism system). Network policies Specification of how groups of pods are allowed to communicate with each other or other network endpoints. Use of labels to select pods and define rules which specify what traffic is allowed to the selected pods. By default, traffic is allowed by default. A pod become isolated when defining a network policy that selects them in the namespace. Other pods within the namespace will continue to receive traffic, unless a network policy is defined for them. Workloads A Job is a resource that runs to completion, ensure it completes or retries. Based on pod, and job and pod resources will remain so logs, output can be reviewed. Manually clean up actions are required. Possible to create parallel jobs too. Use cases are batch processing, data migration, backups, etc. some process that require large amount of data to process. Also, another interesting resource is Cronjobs. Schedules one or repeated a specific time or interval. It is based on job, so it is a pod. It is a way to automate recurrent tasks. New Job resources are created for each run. By default, it will clean up Jobs, by default, keeping 3 successful and 1 failed jobs. Jobs should be idempotent. DaemonSet is a resources that ensures that all nodes run a copy of a pod. Usually used for cluster-wide logging and monitor agents. Security context To give a more restrictive access to the pod. It is a way to limit access of the users. Service account A service account provides an identity for processes that run in a Pod, and maps to a ServiceAccount object. When you authenticate to the API server, you identify yourself as a particular user. There is a service account by default. Role-based access control Once a user is authenticated, we need to authorize the user to perform actions. a Role is a collection of permissions (rules). Roles are namespace specific . We can asigne rules to groups, and users belongs to groups. A more flexible way to assign rules. With ClusterRole, we can assign rules to cluster-wide resources instead to restrict to a specific namespaces. Totally reusable across entire cluster. We assign to groups and user with RoleBinding object. It is namespace specific. Get subjects and roles, and bind them together. Similar, but for entire cluster, you have the object ClusterRoleBinding. There are some built-in roles, such as cluster-admin , admin , view and edit . Local k8s cluster minikube quickly sets up a local Kubernetes cluster on macOS, Linux, and Windows. Easy to install and start working. Install and run minikube to have a local cluster for k8s in local. If minikube is running, automatically context/namespace is updated to use then one's from minikube. If you are working with other k8s cluster, keep in mind this change. Always doble-check for namespace or context before deploying to the cluster. To work with cluster outside local, either you stop minikube or change namespace. Careful, when you stop minikube, context/namespace are set to null, so your kube commands won't be pointing to any context. You need to explicitly set again context. minikube command is just a proxy of kubectl commands, with a different command format but doing the same underneath. Readiness and liveness, startup probes Kubernetes simply observes the pod's lifecycle and starts to route traffic to the pod when the containers move from the Pending to Succeeded state. That means, Kubernetes mark a pod as healthy and ready to get request as soon as this happens. But application may receive traffic before is actually ready, because it needs to make database connection or load some data. There is a gap between when the app is ready and when Kubernetes thinks is ready. Risk of receive traffic and return 500 errors. Similar case happens when Kubelet watches for application crashes and restarts the pod to recover. 3 different probes: Liveness probe, determines the health, and kills the pod if it fails the liveness check (deadlock situation, since the underlying process continues to run from Kubernetes's perspective). Readiness probes are used to let kubelet know when the application is ready to accept new traffic. It runs during the pod's entire lifecycle, to deal when the application is temporarily unavailable (wait for it to recover). Startup probes are similar to readiness probes but only executed at startup. They are optimized for slow starting containers or applications with unpredictable initialization processes. 3 different probe handlers: exec: run a command inside the container. Success if return code is 0. TCPSocket: TCP check , success if port is open and accepting connections. HTTPGet: invoke HTTP GET against url, success if 2XX or 3XX code. There are many options in the probe configuration, check in the official k8s documentation. Resource management Units in k8s have a different rate conversion, and different units for CPU and Memory (can be either base 2 or 10). Resource request is the minimum amount of resources that the container needs to work. Helps k8s to schedule the pod in a more efficient way. Only used for scheduling, if the sum of the resource requests less than the capacity of the node. On the other hand, we have resource limits. It is the maximum amount of resources that a pod can get. If exceed this limits, container might be terminated. It is a way to protect against a runaway app. References K8s courses k8s concepts k9s commands minikube handbook minikube/kubectl commands kubernetes probes liveness probes tips","title":"Kubernetes"},{"location":"devops/k8s/#kubernetes","text":"It is a high-disponiblity platform. It support different application available to be consume. You can use kubectl (core k8s cli tool) to give instructions to k8s. Instructions can be imperative and declarative (prefer approach). Declarative is where k8s take the lead regarding which are the actions needed to reach the desire state. Act -> observe -> diff -> Act or Desire vs Actual state k8s uses reconciliation to make the actual state looks like the desire state via control loop. Or by using imperative actions. k8s is usually the deployment tool for a microservices architecture, instead of monolithic application .","title":"Kubernetes"},{"location":"devops/k8s/#concepts","text":"Data is send to k8s through a yaml file, which is a declarative way to describe. Kind is equal to the object being described. Name must unique within the namespace for each Kind. spec information regarding this object representation. Create new or update resource with kubectl apply -f <file.yaml>","title":"Concepts"},{"location":"devops/k8s/#pod","text":"Can be reusable. It is a group of containers. It is the smallest unit in k8s. All containers for a pod will run in the same mode. They can talk to each other via localhost, and can share volume resources. A pod can have a container to run a web server application, where the code is located inside the image. But any time there is typo or a fix, it needs to build a new image and restart the container. This is called main container. Using side containers, we can have a shared volume between the main container to read the content from, separation od concern isolating and reusing. Each container can have different resources, like memory, cpu, etc.","title":"Pod"},{"location":"devops/k8s/#services","text":"Load balancing for Pods. Use labels to determine target pods. Like a load balancer in front opf our pods. Service are not process running in k8s, is not consuming resources, is more like configuration or metadata.","title":"Services"},{"location":"devops/k8s/#deployments","text":"Deployments are an objects in k8s designed to avoid declaring the exact same pod metadata (except by the name, as it must be unique) when you need to deploy X instance of the same pod. Deployments does not create actually any pods, Instead, deployment create an object in the middle call replica sets.. Replica sets are used to make it easy to roll from one version of a deployment to another. Each time we update a deployment, a new replica set is created that contains the latest configuration. Replica set actually create the pods. Two type of strategies: RollingUpdate: value by default. new ReplicaSet is created, then scaled up as old one is scaled down. Recreate: remove all existing pods in the existing ReplicaSet before creating new ones from new ReplicaSet. Also, it is possible to configure another different deployment strategies as canary releases and blue/green. In a deployment, each endpoint is the IP address of a pod that is backing that service. In case of multiple replicas, we should see 1 endpoint for each pod created for a deployment. More configuration available in the official k8s documentation.","title":"Deployments"},{"location":"devops/k8s/#statefulset","text":"It is an extension of deployment. It is used to manage stateful applications. Name is stable, unique pod identifiers and DNS names. Also, possible to set Persistent Storage and one Persistent Volumen per VolumeClaim template. Stick to pod as long as the pod is declared. Pods created in asc order and deleted in desc order. It is possible to scale up, but all pods must be in Running or Read.","title":"StatefulSet"},{"location":"devops/k8s/#labels","text":"Characteristics are: map of key/value pairs, try to standardize both organizational and functional (selector on services) indexes and searchable avoid compound labels values","title":"Labels"},{"location":"devops/k8s/#architecture","text":"Worker nodes, where the containers are running and application. They have cpu, memory to handle the application. Each of the have docker installed and kubelet , which mainly determine the url of the k8s cluster. Control planes nodes: host the componentes that actually run k8s. like API server. Data stored is in etcd. Scheduler, determine where to run the pods. Controller manager, watch the state of the cluster and make changes to the cluster to match the desire state. Cloud controller manager, manage the interaction with the underlying cloud provider.","title":"Architecture"},{"location":"devops/k8s/#networking","text":"Traffic can happen in a cluster within a pod, pod to pod, service to pods and external to cluster. within a pod: containers within a pod can connect to each other using localhost. Containers share an IP address accesible throughout the cluster and share common port space. pod to pod: each pod has a unique IP address, so they can communicate with each other. IP address are routable anywhere within the cluster. Not common procedure. service to pod: service is a resource that provides layer-4 load balancing for a group of pods. Service discovery using the cluster's internal DNS. Types are: ClusterIP (virtual IP address that load balance request to backend pods, accessible only in the cluster). NodePort: used for external facing service, exposing a port to access externally. K8s will open the port in each worker node. external to cluster: service type LoadBalancer, external IP address is provisioned by the cloud provider. Creates and manages an external load balancer, managing traffic across all nodes. Another option is Ingress . It is a layer-7 load balancer. It is a resource that balance the traffic for one or more services. Ingress rules to balance traffic to specific service. An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting. An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic. Same rules as Nginx or Traefic can be applied to Ingress, meaning that traffic to a domain or a specific path can be redirect to the corresponding labelled service.","title":"Networking"},{"location":"devops/k8s/#resource-organization","text":"Have multiple clusters, to have highest level of isolation. Each cluster can have different purposes, like security, environments (development vs uat vs production), by geography... Namespaces are a grouping in k8s. There is no nesting, all our objects in k8s go to same namespace. Namespaces are a way to organize and isolate resources. Names of resources must be unique within a namespace. Security and access can be set per namespaces. Usually 1 namespace per project.","title":"Resource organization"},{"location":"devops/k8s/#kubeconfig","text":"It a file that contains all sections for k8s. clusters: url of the API service. Name can be use later for the kubeconfig file. context: where cluster, user and name are glue together. Names are used to identify the context. users: credentials to use to access the cluster. Name can be use later for the kubec config file.","title":"kubeconfig"},{"location":"devops/k8s/#volume","text":"It is a way to persist data. It is a directory that is accessible to containers. Volumes are exposed at pod level. By default, containers in a pod write to ephemeral storage (emptyDir), only available during the pod's lifecycle. Once pod is terminated, data is lost. But we can attach Persistent volumes to pods which persist any data written to them. They are network attached storage, independent of pod's life. We can static or dynamic volumes. Static volumes are created manually, dynamic volumes are created automatically when a pod is created. Storage is actually outside the cluster. I your k8s manifest, you will define what are your needs with a Persist Volume Claim. Then, when the request reaches the cluster, it will be matched with a Persist Volume (binding). If there is no match, it will create a new one.","title":"Volume"},{"location":"devops/k8s/#secrets-and-dynamic-configuration","text":"All runtime configuration should be able to be injected and overrides. Defaults are ok, but we should be able to override them. Secrets are a way to store sensitive information, like passwords, tokens, keys... They are stored in etcd, encrypted at rest. They are base64 encoded, but not encrypted. We can either provide defaults value if no configuration is provided or required configuration, where startup fails if no configuration is provided. ConfigMap is the object to store configuration. It is a key/value pair. Use to store configuration data and properties. We can load all keys (envFrom.configMapRef) or individual keys (env.valueFrom.configMapKeyRef) as environment variables. Another option is to mount the config map as a mounted volume, similar all keys or individual keys (changes don\u00b4t require restart). For sensitive information, Secret resource stores this kind of data. Must be Base64 encode, and they can be exposed to pod via environment variables or mounted volumes (optional, other secret mechanism system).","title":"Secrets and dynamic configuration"},{"location":"devops/k8s/#network-policies","text":"Specification of how groups of pods are allowed to communicate with each other or other network endpoints. Use of labels to select pods and define rules which specify what traffic is allowed to the selected pods. By default, traffic is allowed by default. A pod become isolated when defining a network policy that selects them in the namespace. Other pods within the namespace will continue to receive traffic, unless a network policy is defined for them.","title":"Network policies"},{"location":"devops/k8s/#workloads","text":"A Job is a resource that runs to completion, ensure it completes or retries. Based on pod, and job and pod resources will remain so logs, output can be reviewed. Manually clean up actions are required. Possible to create parallel jobs too. Use cases are batch processing, data migration, backups, etc. some process that require large amount of data to process. Also, another interesting resource is Cronjobs. Schedules one or repeated a specific time or interval. It is based on job, so it is a pod. It is a way to automate recurrent tasks. New Job resources are created for each run. By default, it will clean up Jobs, by default, keeping 3 successful and 1 failed jobs. Jobs should be idempotent. DaemonSet is a resources that ensures that all nodes run a copy of a pod. Usually used for cluster-wide logging and monitor agents.","title":"Workloads"},{"location":"devops/k8s/#security-context","text":"To give a more restrictive access to the pod. It is a way to limit access of the users.","title":"Security context"},{"location":"devops/k8s/#service-account","text":"A service account provides an identity for processes that run in a Pod, and maps to a ServiceAccount object. When you authenticate to the API server, you identify yourself as a particular user. There is a service account by default.","title":"Service account"},{"location":"devops/k8s/#role-based-access-control","text":"Once a user is authenticated, we need to authorize the user to perform actions. a Role is a collection of permissions (rules). Roles are namespace specific . We can asigne rules to groups, and users belongs to groups. A more flexible way to assign rules. With ClusterRole, we can assign rules to cluster-wide resources instead to restrict to a specific namespaces. Totally reusable across entire cluster. We assign to groups and user with RoleBinding object. It is namespace specific. Get subjects and roles, and bind them together. Similar, but for entire cluster, you have the object ClusterRoleBinding. There are some built-in roles, such as cluster-admin , admin , view and edit .","title":"Role-based access control"},{"location":"devops/k8s/#local-k8s-cluster","text":"minikube quickly sets up a local Kubernetes cluster on macOS, Linux, and Windows. Easy to install and start working. Install and run minikube to have a local cluster for k8s in local. If minikube is running, automatically context/namespace is updated to use then one's from minikube. If you are working with other k8s cluster, keep in mind this change. Always doble-check for namespace or context before deploying to the cluster. To work with cluster outside local, either you stop minikube or change namespace. Careful, when you stop minikube, context/namespace are set to null, so your kube commands won't be pointing to any context. You need to explicitly set again context. minikube command is just a proxy of kubectl commands, with a different command format but doing the same underneath.","title":"Local k8s cluster"},{"location":"devops/k8s/#readiness-and-liveness-startup-probes","text":"Kubernetes simply observes the pod's lifecycle and starts to route traffic to the pod when the containers move from the Pending to Succeeded state. That means, Kubernetes mark a pod as healthy and ready to get request as soon as this happens. But application may receive traffic before is actually ready, because it needs to make database connection or load some data. There is a gap between when the app is ready and when Kubernetes thinks is ready. Risk of receive traffic and return 500 errors. Similar case happens when Kubelet watches for application crashes and restarts the pod to recover. 3 different probes: Liveness probe, determines the health, and kills the pod if it fails the liveness check (deadlock situation, since the underlying process continues to run from Kubernetes's perspective). Readiness probes are used to let kubelet know when the application is ready to accept new traffic. It runs during the pod's entire lifecycle, to deal when the application is temporarily unavailable (wait for it to recover). Startup probes are similar to readiness probes but only executed at startup. They are optimized for slow starting containers or applications with unpredictable initialization processes. 3 different probe handlers: exec: run a command inside the container. Success if return code is 0. TCPSocket: TCP check , success if port is open and accepting connections. HTTPGet: invoke HTTP GET against url, success if 2XX or 3XX code. There are many options in the probe configuration, check in the official k8s documentation.","title":"Readiness and liveness, startup probes"},{"location":"devops/k8s/#resource-management","text":"Units in k8s have a different rate conversion, and different units for CPU and Memory (can be either base 2 or 10). Resource request is the minimum amount of resources that the container needs to work. Helps k8s to schedule the pod in a more efficient way. Only used for scheduling, if the sum of the resource requests less than the capacity of the node. On the other hand, we have resource limits. It is the maximum amount of resources that a pod can get. If exceed this limits, container might be terminated. It is a way to protect against a runaway app.","title":"Resource management"},{"location":"devops/k8s/#references","text":"K8s courses k8s concepts k9s commands minikube handbook minikube/kubectl commands kubernetes probes liveness probes tips","title":"References"},{"location":"devops/observability/","text":"Observability Server observability refers to the practice of monitoring and gaining insights into the performance, health, and behavior of servers and the applications or services running on them. It is a critical aspect of managing and maintaining server infrastructure, especially in modern, complex, and distributed computing environments. Server observability helps organizations detect and diagnose issues, optimize performance, and ensure the reliability of their IT systems. Server observability is an essential practice in modern IT operations, especially for cloud-based and containerized environments, where servers and services are highly dynamic and interconnected. Logging Collecting and analyzing logs generated by server applications and services. Logs can provide detailed information about events, errors, and transactions, helping in troubleshooting and debugging issues. There are different log system out there, so better approach is to use a tool to standardize the output of the logs, such as Fluentd , an Open source data collector. Logs are collected in a central location, where they can be displayed. A good tool for this is Kibana , with ElasticSearch for data storage. Metrics Gathering and analyzing various metrics such as CPU utilization, memory usage, disk I/O, network traffic, and application-specific performance indicators. These metrics provide a real-time view of the server's health and performance. Prometheus is an open-source systems monitoring and alerting toolkit. It comes with a query language built-in, where powerful queries can be written to gain insight into the system's behavior. Alerts regarding this metrics can be send to another system like Sentry or Nagios . For metrics visualization, Grafana is a good option. Tracing Tracing requests as they traverse through various components of a distributed system. Distributed tracing helps in identifying latency bottlenecks and understanding the flow of requests in a microservices architecture. Jaeger is an open source end-to-end distributed system to help tracing request within the system. To implement server observability effectively, organizations often use specialized tools and platforms, such as monitoring and observability solutions like Prometheus, Grafana, Elasticsearch, Kibana, , and many others. These tools help in collecting, storing, analyzing, and visualizing the data needed to maintain and improve server performance and reliability. References Observability","title":"Observability"},{"location":"devops/observability/#observability","text":"Server observability refers to the practice of monitoring and gaining insights into the performance, health, and behavior of servers and the applications or services running on them. It is a critical aspect of managing and maintaining server infrastructure, especially in modern, complex, and distributed computing environments. Server observability helps organizations detect and diagnose issues, optimize performance, and ensure the reliability of their IT systems. Server observability is an essential practice in modern IT operations, especially for cloud-based and containerized environments, where servers and services are highly dynamic and interconnected.","title":"Observability"},{"location":"devops/observability/#logging","text":"Collecting and analyzing logs generated by server applications and services. Logs can provide detailed information about events, errors, and transactions, helping in troubleshooting and debugging issues. There are different log system out there, so better approach is to use a tool to standardize the output of the logs, such as Fluentd , an Open source data collector. Logs are collected in a central location, where they can be displayed. A good tool for this is Kibana , with ElasticSearch for data storage.","title":"Logging"},{"location":"devops/observability/#metrics","text":"Gathering and analyzing various metrics such as CPU utilization, memory usage, disk I/O, network traffic, and application-specific performance indicators. These metrics provide a real-time view of the server's health and performance. Prometheus is an open-source systems monitoring and alerting toolkit. It comes with a query language built-in, where powerful queries can be written to gain insight into the system's behavior. Alerts regarding this metrics can be send to another system like Sentry or Nagios . For metrics visualization, Grafana is a good option.","title":"Metrics"},{"location":"devops/observability/#tracing","text":"Tracing requests as they traverse through various components of a distributed system. Distributed tracing helps in identifying latency bottlenecks and understanding the flow of requests in a microservices architecture. Jaeger is an open source end-to-end distributed system to help tracing request within the system. To implement server observability effectively, organizations often use specialized tools and platforms, such as monitoring and observability solutions like Prometheus, Grafana, Elasticsearch, Kibana, , and many others. These tools help in collecting, storing, analyzing, and visualizing the data needed to maintain and improve server performance and reliability.","title":"Tracing"},{"location":"devops/observability/#references","text":"Observability","title":"References"},{"location":"devops/serverless/","text":"Serverless Computing Instead of run our application in our servers, this approach is focus on let others run (and charge depending on how many resource are used) our code. Servers are hard to maintain, needs to be up-to-date, always up and running. Serverless is less headache. We can forget about security, upgrade, scalability... Some providers of Functions as a Service (FaaS) are AWS, Google and Azure. They can handle out of the box most of the events, including http requests, database events, queuing services, monitoring alerts, file uploads, scheduled events, cronjobs... It is based on a microservice architecture, not monolith. There is no context shared among them. Cold start is the bigger cons. As the container is brought up alive when there is a request, there may be some latency between request and execution. Today, this time has improved a lot. It depends on language, size of the function... References Serveless","title":"Serverless Computing"},{"location":"devops/serverless/#serverless-computing","text":"Instead of run our application in our servers, this approach is focus on let others run (and charge depending on how many resource are used) our code. Servers are hard to maintain, needs to be up-to-date, always up and running. Serverless is less headache. We can forget about security, upgrade, scalability... Some providers of Functions as a Service (FaaS) are AWS, Google and Azure. They can handle out of the box most of the events, including http requests, database events, queuing services, monitoring alerts, file uploads, scheduled events, cronjobs... It is based on a microservice architecture, not monolith. There is no context shared among them. Cold start is the bigger cons. As the container is brought up alive when there is a request, there may be some latency between request and execution. Today, this time has improved a lot. It depends on language, size of the function...","title":"Serverless Computing"},{"location":"devops/serverless/#references","text":"Serveless","title":"References"},{"location":"devops/versioning/","text":"Versioning Versioning releases is important to handle dependencies with new code. Each version must be tagged with an unique identifier. Common versioning Major - The first number in the version. 2 and 3 are Python's famous major versions. The major segment is the most common calendar-based component. Minor - The second number in the version. 7 is the most popular minor version of Python. Micro - The third and usually final number in the version. Sometimes referred to as the \"patch\" segment. Modifier - An optional text tag, such as \"dev\", \"alpha\", \"beta\", \"rc1\", and so on. Calendar versioning YYYY - Full year - 2006, 2016, 2106 YY - Short year - 6, 16, 106 0Y - Zero-padded year - 06, 16, 106 MM - Short month - 1, 2 ... 11, 12 0M - Zero-padded month - 01, 02 ... 11, 12 WW - Short week (since start of year) - 1, 2, 33, 52 0W - Zero-padded week - 01, 02, 33, 52 DD - Short day - 1, 2 ... 30, 31 0D - Zero-padded day - 01, 02 ... 30, 31 For reference, both versioning can be mixed. References Calendar versioning","title":"Versioning"},{"location":"devops/versioning/#versioning","text":"Versioning releases is important to handle dependencies with new code. Each version must be tagged with an unique identifier.","title":"Versioning"},{"location":"devops/versioning/#common-versioning","text":"Major - The first number in the version. 2 and 3 are Python's famous major versions. The major segment is the most common calendar-based component. Minor - The second number in the version. 7 is the most popular minor version of Python. Micro - The third and usually final number in the version. Sometimes referred to as the \"patch\" segment. Modifier - An optional text tag, such as \"dev\", \"alpha\", \"beta\", \"rc1\", and so on.","title":"Common versioning"},{"location":"devops/versioning/#calendar-versioning","text":"YYYY - Full year - 2006, 2016, 2106 YY - Short year - 6, 16, 106 0Y - Zero-padded year - 06, 16, 106 MM - Short month - 1, 2 ... 11, 12 0M - Zero-padded month - 01, 02 ... 11, 12 WW - Short week (since start of year) - 1, 2, 33, 52 0W - Zero-padded week - 01, 02, 33, 52 DD - Short day - 1, 2 ... 30, 31 0D - Zero-padded day - 01, 02 ... 30, 31 For reference, both versioning can be mixed.","title":"Calendar versioning"},{"location":"devops/versioning/#references","text":"Calendar versioning","title":"References"},{"location":"programming/conventional-commits/","text":"Conventional commits Conventional Commits is a generic convention of writing commit messages. It provides a set of simple rules for creating new commits, easier to automate tools on top, such as changelog or semantic versioning. Goal of this convention is to make commit messages more readable for developers or other stakeholder, consistent among developers and easier to automate. References Conventional commits","title":"Conventional commits"},{"location":"programming/conventional-commits/#conventional-commits","text":"Conventional Commits is a generic convention of writing commit messages. It provides a set of simple rules for creating new commits, easier to automate tools on top, such as changelog or semantic versioning. Goal of this convention is to make commit messages more readable for developers or other stakeholder, consistent among developers and easier to automate.","title":"Conventional commits"},{"location":"programming/conventional-commits/#references","text":"Conventional commits","title":"References"},{"location":"programming/development-workflows/","text":"Development workflows Version control system is a must for any serious software development. It allows you to track changes in your code, collaborate with other developers and easily revert changes if needed. The most popular system is git. It is a distributed version control system, which means that every developer has a full copy of the repository on his computer. This allows you to work offline and commit changes locally. When you are ready to share your changes with other developers, you push your commits to the remote repository. To allow work in parallel, there are two different workflows. git flow There is one main development branch, usually called develop . All new features are created for this branch, and several commits can be made. Once they are ready, a pull request is created towards develop . Once the pull request is reviewed by other developers, it is merged into develop . Then, a new release is created, ready to be deployed to production, and later to master branch, with a tag versioning of the release. Ideally, use this workflow when there are a lot of junior developers, or new joiners. ALso, then product is stable and any changes can affect heavily in daily operations by the final user. In the order hand, can cause bottle neck if the product is starting up and need top be iterated quickly. Trunk-based All developers work in the same branch, usually master . When a new feature is needed, a new branch is created from master . Once the feature is ready, it is merged into master . This workflow is ideal for small teams, where there is a lot of trust between developers, and the product is still in development. It allows to iterate quickly, and to have a fast feedback loop. It allows continuous integration, so when build and tests pass, the code is deployed to production every single commit. Smaller commits allow faster code revision, and easier to revert changes if something wrong happens. Increases confidences in developers. Combine with feature flags , where code can be in production but inactive to the final user. Bigger features are not separated for long time to master branch, reducing the risk of conflicts when merging. Almost daily merges to master are done, increasing agile release with CI/CD. References Git flow workflow Trunk-based workflow","title":"Development workflows"},{"location":"programming/development-workflows/#development-workflows","text":"Version control system is a must for any serious software development. It allows you to track changes in your code, collaborate with other developers and easily revert changes if needed. The most popular system is git. It is a distributed version control system, which means that every developer has a full copy of the repository on his computer. This allows you to work offline and commit changes locally. When you are ready to share your changes with other developers, you push your commits to the remote repository. To allow work in parallel, there are two different workflows.","title":"Development workflows"},{"location":"programming/development-workflows/#git-flow","text":"There is one main development branch, usually called develop . All new features are created for this branch, and several commits can be made. Once they are ready, a pull request is created towards develop . Once the pull request is reviewed by other developers, it is merged into develop . Then, a new release is created, ready to be deployed to production, and later to master branch, with a tag versioning of the release. Ideally, use this workflow when there are a lot of junior developers, or new joiners. ALso, then product is stable and any changes can affect heavily in daily operations by the final user. In the order hand, can cause bottle neck if the product is starting up and need top be iterated quickly.","title":"git flow"},{"location":"programming/development-workflows/#trunk-based","text":"All developers work in the same branch, usually master . When a new feature is needed, a new branch is created from master . Once the feature is ready, it is merged into master . This workflow is ideal for small teams, where there is a lot of trust between developers, and the product is still in development. It allows to iterate quickly, and to have a fast feedback loop. It allows continuous integration, so when build and tests pass, the code is deployed to production every single commit. Smaller commits allow faster code revision, and easier to revert changes if something wrong happens. Increases confidences in developers. Combine with feature flags , where code can be in production but inactive to the final user. Bigger features are not separated for long time to master branch, reducing the risk of conflicts when merging. Almost daily merges to master are done, increasing agile release with CI/CD.","title":"Trunk-based"},{"location":"programming/development-workflows/#references","text":"Git flow workflow Trunk-based workflow","title":"References"},{"location":"programming/django/","text":"Django Installation to install django, lets create a requirements.txt file, with desired version of django. django==1.11.13 then, install libraries in file: pip install -r requirements.txt check everything went ok: django-admin -h Starting new project First, on Django, you'll need to create a project by: django-admin startproject <project> that's basic configuration. To check everything looks good, run: python manage.py runserver if we want to test via shell, run: python manage.py shell basic schema of django app A project can contains one or more apps. Also, an app can be used in several projects To create a new app in our project: python manage.py startapp <app> New folder is created. It contains basic schema: <app>/views.py represents the controller. One function per view. get data, processed it, and render template if needed <app>/urls.py where url is mapped to corresponding function in views file <app>/models.py represent the models. Django use an ORM, that means, tables and database corresponds to Model classes. Add app configuration to project/settings.py : INSTALLED_APPS = [ '<app>.apps.<App>Config', Migrations Every changes on models, needs to be migrate to database schema. After add/update information to models.py file, changes needs to be done on database level Make migrations: python manage.py makemigrations <app> See migration on sql: python manage.py sqlmigrate <app> <number> Run migrations: python manage.py migrate django admin - basic commands Creating an admin user: python manage.py createsuperuser Tests needs to be created. Run with: python manage.py test <app>","title":"Django"},{"location":"programming/django/#django","text":"","title":"Django"},{"location":"programming/django/#installation","text":"to install django, lets create a requirements.txt file, with desired version of django. django==1.11.13 then, install libraries in file: pip install -r requirements.txt check everything went ok: django-admin -h","title":"Installation"},{"location":"programming/django/#starting-new-project","text":"First, on Django, you'll need to create a project by: django-admin startproject <project> that's basic configuration. To check everything looks good, run: python manage.py runserver if we want to test via shell, run: python manage.py shell","title":"Starting new project"},{"location":"programming/django/#basic-schema-of-django-app","text":"A project can contains one or more apps. Also, an app can be used in several projects To create a new app in our project: python manage.py startapp <app> New folder is created. It contains basic schema: <app>/views.py represents the controller. One function per view. get data, processed it, and render template if needed <app>/urls.py where url is mapped to corresponding function in views file <app>/models.py represent the models. Django use an ORM, that means, tables and database corresponds to Model classes. Add app configuration to project/settings.py : INSTALLED_APPS = [ '<app>.apps.<App>Config',","title":"basic schema of django app"},{"location":"programming/django/#migrations","text":"Every changes on models, needs to be migrate to database schema. After add/update information to models.py file, changes needs to be done on database level Make migrations: python manage.py makemigrations <app> See migration on sql: python manage.py sqlmigrate <app> <number> Run migrations: python manage.py migrate","title":"Migrations"},{"location":"programming/django/#django-admin-basic-commands","text":"Creating an admin user: python manage.py createsuperuser Tests needs to be created. Run with: python manage.py test <app>","title":"django admin - basic commands"},{"location":"programming/fastapi/","text":"FastApi It is a python framework to build quickly restful API's. It is nice, modern, and with good performance. It has Openapi implementation by default, you don't need to manually create and maintain openapi.yaml. By default, fastApi generates automatically openapi document. Uses pydantic for models, for data validations. Support async by default too, enriching performance even faster. Careful when using async + DB. Reference FastAPI Pydantic models sqlmodel FastAPI def vs async def","title":"FastApi"},{"location":"programming/fastapi/#fastapi","text":"It is a python framework to build quickly restful API's. It is nice, modern, and with good performance. It has Openapi implementation by default, you don't need to manually create and maintain openapi.yaml. By default, fastApi generates automatically openapi document. Uses pydantic for models, for data validations. Support async by default too, enriching performance even faster. Careful when using async + DB.","title":"FastApi"},{"location":"programming/fastapi/#reference","text":"FastAPI Pydantic models sqlmodel FastAPI def vs async def","title":"Reference"},{"location":"programming/flask/","text":"Flask It is nice framework to quickly build API from scratch. Specially to work with databases, it has a robust library to work with SQLAlchemy Schema validation with marshmallow, HTTP code responses standardized. References Flask SQLALchemy Blueprints Marshmallow","title":"Flask"},{"location":"programming/flask/#flask","text":"It is nice framework to quickly build API from scratch. Specially to work with databases, it has a robust library to work with SQLAlchemy Schema validation with marshmallow, HTTP code responses standardized.","title":"Flask"},{"location":"programming/flask/#references","text":"Flask SQLALchemy Blueprints Marshmallow","title":"References"},{"location":"programming/openapi/","text":"OpenAPI Specification OpenAPI Specification is an API description format for REST APIs. It is in a machine-readable format (but also very human friendly), so it can have data, description and linting validation, making sure format is correct. Generates a documentation for humans, which is always up-to-date. Why is needed? It provides human and machine readable information about your REST API you are devolving. It is a nice tool to share information internally and externally. People knows in which stage is the project, are, what kind of response are expected... everything is shared in a common place visible for everyone involved in the project. Contracts are available (and testable). Even if code underneath is not fully ready, contracts are and will (should) not change whatsoever. OpenAPI description should be committed to source control, and, in fact, they should be among the first files to be committed. From there, they should also participate in Continuous Integration processes. Make the OpenAPI Documents Available to the Users. Beautifully-rendered documents can be very useful for the users of an API, but sometimes they might want to access the source OpenAPI description. For instance, to use tools to generate client code for them, or to build automated bindings for some language. Therefore, making the OpenAPI documents available to the users is an added bonus for them. The document can even be made available through the same API to allow runtime discovery. No need to write document by hand, there are nice tools or even frameworks like FastAPI that generates OpenAPI specification on the fly. ## Approaches Approaches There are two main different approaches regarding OpenAPI implementation. Apply one of the approaches regarding the project needs. Use a Design-First Approach Without any line of code, start the API design. OpenAPI specification is created to show the structure of the API, and then code is implemented to match the specification. Some tools can automatically build boilerplate from OpenAPI specification. It can be slower, but if requirements are clear, it is a good starting point. Contracts will be available from the beginning, and models can be reused easily. Use a Code-First Approach API is implemented together with the code, and OpenAPI specification is created with code comments, annotation and description in it. It is faster, but can lead to changes in the contract (may not be an issue if the project is starting up). References Online swagger editor Swagger specification Learn OpenAPI","title":"OpenAPI Specification"},{"location":"programming/openapi/#openapi-specification","text":"OpenAPI Specification is an API description format for REST APIs. It is in a machine-readable format (but also very human friendly), so it can have data, description and linting validation, making sure format is correct. Generates a documentation for humans, which is always up-to-date.","title":"OpenAPI Specification"},{"location":"programming/openapi/#why-is-needed","text":"It provides human and machine readable information about your REST API you are devolving. It is a nice tool to share information internally and externally. People knows in which stage is the project, are, what kind of response are expected... everything is shared in a common place visible for everyone involved in the project. Contracts are available (and testable). Even if code underneath is not fully ready, contracts are and will (should) not change whatsoever. OpenAPI description should be committed to source control, and, in fact, they should be among the first files to be committed. From there, they should also participate in Continuous Integration processes. Make the OpenAPI Documents Available to the Users. Beautifully-rendered documents can be very useful for the users of an API, but sometimes they might want to access the source OpenAPI description. For instance, to use tools to generate client code for them, or to build automated bindings for some language. Therefore, making the OpenAPI documents available to the users is an added bonus for them. The document can even be made available through the same API to allow runtime discovery. No need to write document by hand, there are nice tools or even frameworks like FastAPI that generates OpenAPI specification on the fly. ## Approaches","title":"Why is needed?"},{"location":"programming/openapi/#approaches","text":"There are two main different approaches regarding OpenAPI implementation. Apply one of the approaches regarding the project needs.","title":"Approaches"},{"location":"programming/openapi/#use-a-design-first-approach","text":"Without any line of code, start the API design. OpenAPI specification is created to show the structure of the API, and then code is implemented to match the specification. Some tools can automatically build boilerplate from OpenAPI specification. It can be slower, but if requirements are clear, it is a good starting point. Contracts will be available from the beginning, and models can be reused easily.","title":"Use a Design-First Approach"},{"location":"programming/openapi/#use-a-code-first-approach","text":"API is implemented together with the code, and OpenAPI specification is created with code comments, annotation and description in it. It is faster, but can lead to changes in the contract (may not be an issue if the project is starting up).","title":"Use a Code-First Approach"},{"location":"programming/openapi/#references","text":"Online swagger editor Swagger specification Learn OpenAPI","title":"References"},{"location":"programming/python/","text":"Python TODO: REVIEW Cursos python python 3.7 https://www.udacity.com/course/introduction-to-python--ud1110 https://www.stavros.io/tutorials/python/ https://learnxinyminutes.com/docs/python/ https://www.tutorialspoint.com/python3/index.htm https://knightlab.northwestern.edu/2014/06/05/five-mini-programming-projects-for-the-python-beginner/ https://www.hackerrank.com/challenges/py-set-add/problem https://www.djangoproject.com/ pep8 pep8 use 4 spaces, instad of tab use two line breaks to separate class or methods use one line breaks between methods inside a class how to import library standard libraries 3pp libraries django libraries local importation CAREFUL while doing import it is relative import if it is same app: .{class} better use absolute import: {app}.{class} avoid using from something import * use _ instead of - avoid use of abbreviature while defining variable, classes or methods one empty line at the end of the file 79 max length of line use flake8 and pylint , standard libraries to check pep8 and lint\u00ba description python es un lenguaje (maravilloso) de programaci\u00f3n. Es un lenguage interpretado (no necesita compilaci\u00f3n de un ejecutable como java). Eso permite que podamos jugar con \u00e9l, bien desde la l\u00ednea de comandos, hasta ficheros m\u00e1s complejos o aplicaciones que soporten servicios web. Tutorial base en castellano Basic tutorial from python (english) virtualenvs poetry Poetry is a dependency management. Easy installation, and management. Instead of a mix of files related to installation, everything is managed in file pyproyect.toml . According to pep518 this is the file where the build system dependencies will be stored in TOML (basic key-vales pair format). It can group common configuration, and it can be use for multiple purposes. Learn python in 10 minutes Learn python in 10 minutes async When normally running python code, code is executing sequentially in the same process/thread. That means, process is blocked until code execution stopped. If another process enters, it acts like a queue. Until process 1 doesn't end, process 2 won't start execution. Can causes performance problem, as if for any reason process 1 is waiting (db connection, external request), those milliseconds CPU is waiting doing nothing. Problem is solved with concurrence. In python, is called async , allowing parallel execution of process, within the same CPU. Even, executing is lineal in the code, whatever you build, order is respect. Async works when there are many execution of the same code, each goes to a different thread inside the same process/CPU. Sample of async vs sync References async python Diferentes tipos de variables integer: 2 float: 2.1 string/unicode: 'hello' / u'Hola Ra\u00fal' list: [1, 2, 3] tuple: (1, 2,3, ) dictionary: {'key': 'value'} boolean custom objects Condicionales: if 1 == 1: if 1 != 1: if 1 in [1,2]: if 1 not in [1,2]: else elif Bucles: for item in items: while i < 6: References peps","title":"Python"},{"location":"programming/python/#python","text":"TODO: REVIEW","title":"Python"},{"location":"programming/python/#cursos-python","text":"python 3.7 https://www.udacity.com/course/introduction-to-python--ud1110 https://www.stavros.io/tutorials/python/ https://learnxinyminutes.com/docs/python/ https://www.tutorialspoint.com/python3/index.htm https://knightlab.northwestern.edu/2014/06/05/five-mini-programming-projects-for-the-python-beginner/ https://www.hackerrank.com/challenges/py-set-add/problem https://www.djangoproject.com/","title":"Cursos python"},{"location":"programming/python/#pep8","text":"pep8 use 4 spaces, instad of tab use two line breaks to separate class or methods use one line breaks between methods inside a class how to import library standard libraries 3pp libraries django libraries local importation CAREFUL while doing import it is relative import if it is same app: .{class} better use absolute import: {app}.{class} avoid using from something import * use _ instead of - avoid use of abbreviature while defining variable, classes or methods one empty line at the end of the file 79 max length of line use flake8 and pylint , standard libraries to check pep8 and lint\u00ba","title":"pep8"},{"location":"programming/python/#description","text":"python es un lenguaje (maravilloso) de programaci\u00f3n. Es un lenguage interpretado (no necesita compilaci\u00f3n de un ejecutable como java). Eso permite que podamos jugar con \u00e9l, bien desde la l\u00ednea de comandos, hasta ficheros m\u00e1s complejos o aplicaciones que soporten servicios web. Tutorial base en castellano Basic tutorial from python (english) virtualenvs poetry Poetry is a dependency management. Easy installation, and management. Instead of a mix of files related to installation, everything is managed in file pyproyect.toml . According to pep518 this is the file where the build system dependencies will be stored in TOML (basic key-vales pair format). It can group common configuration, and it can be use for multiple purposes. Learn python in 10 minutes Learn python in 10 minutes","title":"description"},{"location":"programming/python/#async","text":"When normally running python code, code is executing sequentially in the same process/thread. That means, process is blocked until code execution stopped. If another process enters, it acts like a queue. Until process 1 doesn't end, process 2 won't start execution. Can causes performance problem, as if for any reason process 1 is waiting (db connection, external request), those milliseconds CPU is waiting doing nothing. Problem is solved with concurrence. In python, is called async , allowing parallel execution of process, within the same CPU. Even, executing is lineal in the code, whatever you build, order is respect. Async works when there are many execution of the same code, each goes to a different thread inside the same process/CPU. Sample of async vs sync","title":"async"},{"location":"programming/python/#references","text":"async python","title":"References"},{"location":"programming/python/#diferentes-tipos-de-variables","text":"integer: 2 float: 2.1 string/unicode: 'hello' / u'Hola Ra\u00fal' list: [1, 2, 3] tuple: (1, 2,3, ) dictionary: {'key': 'value'} boolean custom objects Condicionales: if 1 == 1: if 1 != 1: if 1 in [1,2]: if 1 not in [1,2]: else elif Bucles: for item in items: while i < 6:","title":"Diferentes tipos de variables"},{"location":"programming/python/#references_1","text":"peps","title":"References"},{"location":"programming/sqlalchemy/","text":"SQL Alchemy References alembic Asyncio scoped session ORM ORM session sample","title":"SQLAlchemy"},{"location":"programming/sqlalchemy/#sql-alchemy","text":"","title":"SQL Alchemy"},{"location":"programming/sqlalchemy/#references","text":"alembic Asyncio scoped session ORM ORM session sample","title":"References"},{"location":"programming/virtualenvs/","text":"Virtualenvs What is virtualenv Most important thing in python, is http://python-guide-pt-br.readthedocs.io/en/latest/dev/virtualenvs/ virtualenv is a tool to create isolated Python environments. virtualenv creates a folder which contains all the necessary executables and libraries to use the packages that a Python project would need. Installation To install virtualenv tool: pip install virtualenv Test your installation: virtualenv --version Basic Usage how to create a virtual environment for a project: cd /home/<user>/.virtualenvs/ virtualenv <my_project> --python=/usr/bin/<python-version> To use a particual virtual env, it needs to be activated: source ~/.virtualenvs/<my_project>/bin/activate Everything you import or install in an activated virtualenv, will be just for this environment To deactivate a virtual environment: deactivate","title":"Virtualenvs"},{"location":"programming/virtualenvs/#virtualenvs","text":"","title":"Virtualenvs"},{"location":"programming/virtualenvs/#what-is-virtualenv","text":"Most important thing in python, is http://python-guide-pt-br.readthedocs.io/en/latest/dev/virtualenvs/ virtualenv is a tool to create isolated Python environments. virtualenv creates a folder which contains all the necessary executables and libraries to use the packages that a Python project would need.","title":"What is virtualenv"},{"location":"programming/virtualenvs/#installation","text":"To install virtualenv tool: pip install virtualenv Test your installation: virtualenv --version","title":"Installation"},{"location":"programming/virtualenvs/#basic-usage","text":"how to create a virtual environment for a project: cd /home/<user>/.virtualenvs/ virtualenv <my_project> --python=/usr/bin/<python-version> To use a particual virtual env, it needs to be activated: source ~/.virtualenvs/<my_project>/bin/activate Everything you import or install in an activated virtualenv, will be just for this environment To deactivate a virtual environment: deactivate","title":"Basic Usage"},{"location":"project-management/c4-model/","text":"C4 model C4 model represents different diagrams for different roles working together for the same ecosystem. Not everyone needs the same level of details on a system/service. Different levels reaches to different roles. And it is easy to zoom-in/zoom-out to understand more/less details. L1 - System contexts Big boxes, where different system of the ecosystem working together L2 - container diagram Zoom in on a system. What is inside, different services running inside it (web page, database mobile app...) and relationships L3 - component diagram Zoom in a container. How the code will be divided, major building blocks and their interactions L4 - Code Zoom in a component, usually is not recommend, as it contains too much details, and it is not worthy. Can be generated on demand by IDE Notations and tips Title, short and meaningful Keep Visual consistency Avoid acronyms Schema of a box Name [What represent] Description Lines, usually uni-directional. Shows data flows, with explicit notation on it Show bi-directional lines when intents are different Add words to make the intent explicit Key/Legend to explain shapes, lines, colors, border... even if they seem obvious Increase the readability of software architecture diagrams, so they can stand alone References C4 modelling tool","title":"C4 Model"},{"location":"project-management/c4-model/#c4-model","text":"C4 model represents different diagrams for different roles working together for the same ecosystem. Not everyone needs the same level of details on a system/service. Different levels reaches to different roles. And it is easy to zoom-in/zoom-out to understand more/less details.","title":"C4 model"},{"location":"project-management/c4-model/#l1-system-contexts","text":"Big boxes, where different system of the ecosystem working together","title":"L1 - System contexts"},{"location":"project-management/c4-model/#l2-container-diagram","text":"Zoom in on a system. What is inside, different services running inside it (web page, database mobile app...) and relationships","title":"L2 - container diagram"},{"location":"project-management/c4-model/#l3-component-diagram","text":"Zoom in a container. How the code will be divided, major building blocks and their interactions","title":"L3 - component diagram"},{"location":"project-management/c4-model/#l4-code","text":"Zoom in a component, usually is not recommend, as it contains too much details, and it is not worthy. Can be generated on demand by IDE","title":"L4 - Code"},{"location":"project-management/c4-model/#notations-and-tips","text":"Title, short and meaningful Keep Visual consistency Avoid acronyms Schema of a box Name [What represent] Description Lines, usually uni-directional. Shows data flows, with explicit notation on it Show bi-directional lines when intents are different Add words to make the intent explicit Key/Legend to explain shapes, lines, colors, border... even if they seem obvious Increase the readability of software architecture diagrams, so they can stand alone","title":"Notations and tips"},{"location":"project-management/c4-model/#references","text":"C4 modelling tool","title":"References"},{"location":"project-management/scrum/","text":"SCRUM Scrum is a lightweight framework for agile development, one of the most used. Agile is a methodology that encourage fast inspection and adaptation. Encourage teamwork, self-organization, to delivery high quality software. Scrum differentiates from other frameworks by having a set of roles, artifacts and time boxes. Scrum is most often used to manage complex software and product development, using iterative and incremental practices. Scrum significantly increases productivity and reduces time to benefits relative to classic waterfall processes. Scrum Roles ScrumMaster Is responsible for running the process smoothly, for removing obstacles that impact productivity, and for organizing and facilitating the critical meetings. The ScrumMaster should maintain a constant awareness of the status of the project (its progress to date) relative to the expected progress, investigate and facilitate resolution of any roadblocks that hold back progress, and generally be flexible enough to identify and deal with any issues that arise. The ScrumMaster is not a manager, but a facilitator. Product Owner The Product Owner is the keeper of the requirements. The Product Owner provides the \"single source of truth\" for the Team, regarding requirements and their planned order of implementation. Also, in charge to determine the objetive of the sprint. Team Is a self-organizing and cross-functional group of people who do the hands-on work of developing and testing the product. Since the Team is responsible for producing the product, it must also have the authority to make decisions about how to perform the work. The Team is therefore self-organizing: Team members decide how to break work into tasks, and how to allocate tasks to individuals, throughout the Sprint. Scrum Artifacts Product Backlog Board where are gather all the requirements of the project. It is a living document, that can be updated at any time. It is a list of features, bugs, technical work or knowledge acquisition. Sprint Backlog List of items to be tackled during a sprint. It is a subset of the Product Backlog. Agreed by the Team and the Product Owner in the planning session. Scrum Events Daily Meeting With a duration of no more of 15 minutes, Team owns the meeting and should speak in order to comment what is the plan for today, focus on meeting the objective of the Sprint. Avoid doing reporting, discussion should be based on the objective of the sprint, as Daily is not a status meeting. Raise if there are any blockers in the near future. Refinement session Meeting to decide what are the tasks planed for the next sprint, so during sprint there is no mismatch. Discussion can happens at this stage, but it should be as clear for everyone. Clear Acceptance Criteria from the business point of view, adding value to the project. It should be handle before the start next sprint. Planning session Where estimation of tickets come to place. Define and confirm the Goal for the sprint. Goal should be always be achievable. Review the capacity of the Team. Just before sprint is started. Review session With stakeholder and after sprint. Show delivery, progress and value added during the sprint. Retro session Discussion about what happened in last sprint. It is a safe space to talk about what went wrong, and how process can be improved. Other Concepts User stories/Use cases Description of a feature in a narrative form. Written by the Product Owner, and responsibility of the same to keep up-to-date. The elements in this User Story are: Name: The Name is a descriptive phrase or sentence. The example uses a basic \"Role-Action-Reason\" organization. It is important to have workable standard of some kind. Description: This is a high-level (low-detail) description of the need to be met. For functional (user-facing) requirements, the description is put in narrative form. For non-functional requirements, the description can be worded in any form that is easy to understand. In both cases, the key is that the level of detail is modest, because the fine details are worked out during the implementation phase, in discussions between team members, product owners, and anyone else who is involved. (This is one of the core concepts of Scrum: Requirements are specified at a level that allows rough estimation of the work required to implement them, not in detail.) Screens and External Documents: If the Story requires user-interface changes (especially non-trivial ones), the Story should contain or link to a prototype of the changes. Any external documents required to implement the Story should also be listed. How to test: The implementation of a Story is defined to be complete if, and only if, it passes all acceptance tests developed for it. This section provides a brief description of how the story will be tested. As for the feature itself, the description of testing methods is short, with the details to be worked out during implementation, but we need at least a summary to guide the estimation process. Story Not all requirements for new development represent user-facing features, but do represent significant work that must be done. These requirements often, but not always, represent work that must be done to support user-facing features. We call these non-functional requirements Technical Stories. Technical Stories have the same elements as User Stories, but need not be cast into narrative form if there is no benefit in doing so. Technical Stories are usually written by Team members, and are added to the Product Backlog. The Product Owner must be familiar with these Stories, and understand the dependencies between these and User Stories in order to rank (sequence) all Stories for implementation. Defect A Defect, or bug report, is a description of a failure of the product to behave in the expected fashion. Defects are stored in a bug-tracking system, which may or may not be physically the same system used to store the Product Backlog. If not, then someone (usually the Product Owner) must enter each Defect into the Product Backlog, for sequencing and scheduling. Sprint Fixed time duration to accomplish an agreed objetive with value by the Team, which can be potentially be deliver to production/final stage. Usually 2 weeks. Story Points Measurement of effort of a certain Story. experience, complexity and load of work are taken into account. It must be taken by the Team. Objetive is to have an agreement of the effort of the Story. TL usually explain task to the team, doubts are solved. TL is not the one who decided, but Team. Notes are taken. Once is clear, values from a Fibonacci sequence are assigned to the Story. 1, 2, 3, 5, 8, 13, 21. 1 is the lowest effort, 21 is the greatest effort. More than 21, should be split in smaller User Stories. Discuss with team regarding any particular difference on estimation, and agreed the effort. Personal experience Not everything is closed at the beginning of the sprint. Flexibility to learn. Focus on objective of the sprint, complete task and give value to the product. Communication is key. It is not about the person, is the Team. References Agile manifesto What is Scrum? How it works? Online platform for scrum meetings JIRA tutorial","title":"SCRUM"},{"location":"project-management/scrum/#scrum","text":"Scrum is a lightweight framework for agile development, one of the most used. Agile is a methodology that encourage fast inspection and adaptation. Encourage teamwork, self-organization, to delivery high quality software. Scrum differentiates from other frameworks by having a set of roles, artifacts and time boxes. Scrum is most often used to manage complex software and product development, using iterative and incremental practices. Scrum significantly increases productivity and reduces time to benefits relative to classic waterfall processes.","title":"SCRUM"},{"location":"project-management/scrum/#scrum-roles","text":"","title":"Scrum Roles"},{"location":"project-management/scrum/#scrummaster","text":"Is responsible for running the process smoothly, for removing obstacles that impact productivity, and for organizing and facilitating the critical meetings. The ScrumMaster should maintain a constant awareness of the status of the project (its progress to date) relative to the expected progress, investigate and facilitate resolution of any roadblocks that hold back progress, and generally be flexible enough to identify and deal with any issues that arise. The ScrumMaster is not a manager, but a facilitator.","title":"ScrumMaster"},{"location":"project-management/scrum/#product-owner","text":"The Product Owner is the keeper of the requirements. The Product Owner provides the \"single source of truth\" for the Team, regarding requirements and their planned order of implementation. Also, in charge to determine the objetive of the sprint.","title":"Product Owner"},{"location":"project-management/scrum/#team","text":"Is a self-organizing and cross-functional group of people who do the hands-on work of developing and testing the product. Since the Team is responsible for producing the product, it must also have the authority to make decisions about how to perform the work. The Team is therefore self-organizing: Team members decide how to break work into tasks, and how to allocate tasks to individuals, throughout the Sprint.","title":"Team"},{"location":"project-management/scrum/#scrum-artifacts","text":"","title":"Scrum Artifacts"},{"location":"project-management/scrum/#product-backlog","text":"Board where are gather all the requirements of the project. It is a living document, that can be updated at any time. It is a list of features, bugs, technical work or knowledge acquisition.","title":"Product Backlog"},{"location":"project-management/scrum/#sprint-backlog","text":"List of items to be tackled during a sprint. It is a subset of the Product Backlog. Agreed by the Team and the Product Owner in the planning session.","title":"Sprint Backlog"},{"location":"project-management/scrum/#scrum-events","text":"","title":"Scrum Events"},{"location":"project-management/scrum/#daily-meeting","text":"With a duration of no more of 15 minutes, Team owns the meeting and should speak in order to comment what is the plan for today, focus on meeting the objective of the Sprint. Avoid doing reporting, discussion should be based on the objective of the sprint, as Daily is not a status meeting. Raise if there are any blockers in the near future.","title":"Daily Meeting"},{"location":"project-management/scrum/#refinement-session","text":"Meeting to decide what are the tasks planed for the next sprint, so during sprint there is no mismatch. Discussion can happens at this stage, but it should be as clear for everyone. Clear Acceptance Criteria from the business point of view, adding value to the project. It should be handle before the start next sprint.","title":"Refinement session"},{"location":"project-management/scrum/#planning-session","text":"Where estimation of tickets come to place. Define and confirm the Goal for the sprint. Goal should be always be achievable. Review the capacity of the Team. Just before sprint is started.","title":"Planning session"},{"location":"project-management/scrum/#review-session","text":"With stakeholder and after sprint. Show delivery, progress and value added during the sprint.","title":"Review session"},{"location":"project-management/scrum/#retro-session","text":"Discussion about what happened in last sprint. It is a safe space to talk about what went wrong, and how process can be improved.","title":"Retro session"},{"location":"project-management/scrum/#other-concepts","text":"","title":"Other Concepts"},{"location":"project-management/scrum/#user-storiesuse-cases","text":"Description of a feature in a narrative form. Written by the Product Owner, and responsibility of the same to keep up-to-date. The elements in this User Story are: Name: The Name is a descriptive phrase or sentence. The example uses a basic \"Role-Action-Reason\" organization. It is important to have workable standard of some kind. Description: This is a high-level (low-detail) description of the need to be met. For functional (user-facing) requirements, the description is put in narrative form. For non-functional requirements, the description can be worded in any form that is easy to understand. In both cases, the key is that the level of detail is modest, because the fine details are worked out during the implementation phase, in discussions between team members, product owners, and anyone else who is involved. (This is one of the core concepts of Scrum: Requirements are specified at a level that allows rough estimation of the work required to implement them, not in detail.) Screens and External Documents: If the Story requires user-interface changes (especially non-trivial ones), the Story should contain or link to a prototype of the changes. Any external documents required to implement the Story should also be listed. How to test: The implementation of a Story is defined to be complete if, and only if, it passes all acceptance tests developed for it. This section provides a brief description of how the story will be tested. As for the feature itself, the description of testing methods is short, with the details to be worked out during implementation, but we need at least a summary to guide the estimation process.","title":"User stories/Use cases"},{"location":"project-management/scrum/#story","text":"Not all requirements for new development represent user-facing features, but do represent significant work that must be done. These requirements often, but not always, represent work that must be done to support user-facing features. We call these non-functional requirements Technical Stories. Technical Stories have the same elements as User Stories, but need not be cast into narrative form if there is no benefit in doing so. Technical Stories are usually written by Team members, and are added to the Product Backlog. The Product Owner must be familiar with these Stories, and understand the dependencies between these and User Stories in order to rank (sequence) all Stories for implementation.","title":"Story"},{"location":"project-management/scrum/#defect","text":"A Defect, or bug report, is a description of a failure of the product to behave in the expected fashion. Defects are stored in a bug-tracking system, which may or may not be physically the same system used to store the Product Backlog. If not, then someone (usually the Product Owner) must enter each Defect into the Product Backlog, for sequencing and scheduling.","title":"Defect"},{"location":"project-management/scrum/#sprint","text":"Fixed time duration to accomplish an agreed objetive with value by the Team, which can be potentially be deliver to production/final stage. Usually 2 weeks.","title":"Sprint"},{"location":"project-management/scrum/#story-points","text":"Measurement of effort of a certain Story. experience, complexity and load of work are taken into account. It must be taken by the Team. Objetive is to have an agreement of the effort of the Story. TL usually explain task to the team, doubts are solved. TL is not the one who decided, but Team. Notes are taken. Once is clear, values from a Fibonacci sequence are assigned to the Story. 1, 2, 3, 5, 8, 13, 21. 1 is the lowest effort, 21 is the greatest effort. More than 21, should be split in smaller User Stories. Discuss with team regarding any particular difference on estimation, and agreed the effort.","title":"Story Points"},{"location":"project-management/scrum/#personal-experience","text":"Not everything is closed at the beginning of the sprint. Flexibility to learn. Focus on objective of the sprint, complete task and give value to the product. Communication is key. It is not about the person, is the Team.","title":"Personal experience"},{"location":"project-management/scrum/#references","text":"Agile manifesto What is Scrum? How it works? Online platform for scrum meetings JIRA tutorial","title":"References"},{"location":"project-management/tech-lead/","text":"Technical leader The job of a tech lead is to make sure the team work with quality. Part of this job is to plan, design, learn and execute technical solutions and improvements. Interact with other tech leads, sharing information and good practices. Also, clarifying technical doubts with stakeholders. Aside from that, a tech lead is also responsible for the team, understanding member\u2019s strengths, weaknesses. Mentoring and guide them. Some actions that a tech lead can do are: Keep in touch with popular tech conferences and events Read tech blogs daily cult.honeypot Attend meet-ups and also maintain constant communication with other Tech Leads around you. Responsibilities Code quality assurance A code quality culture is essential, at organizational or individual level. High code quality ensures your codebase is maintainable, scalable, and efficient, allowing to deliver new features faster. Establish agreed code styles, such as PEP8 . Keep code consistent by establishing conventions for things like naming, spacing and indentation. No linters issues. Find bugs, duplicity of code, bad practices, TODO/FIXME... use tools like pre-commit hooks , jenkins, sonarqube, etc. Good test code coverage with unit and functional test. Over 95% if possible. Tracking high-quality issues to manage tech debt properly. Prioritize and schedule tech debt. Code reviews. Ensure code reviews are done and are effective. Code reviews should be done by a peer, not by the author. The author should be able to explain the code to the reviewer. The reviewer should be able to understand the code without the author's help. Have a definition of done. A checklist of things that need to be done before a ticket is considered done. Track code quality metrics. Code Complexity, N umber of Peer mentoring when needed. Code Comments Establish conventions to ensure comments are useful, improving engineering velocity and code quality. Managing and reducing technical debt While TODO/FIXME are good for single player, but avoid using in teams. Use them to aid your personal code development process and never push them to main code base branch. Focus on the why . Provide context and explain the intent of the code in clases or methods ( docs strings ) Avoid obvious comments. Provide architectural and design direction Follow SOLID principles. Avoid code rewritten or delete not long after creation. Reduce code smells by refactoring code. Control Technical Debt Keeping a low number of technical debt. Ensure code review healthiness Applying good pull request practices. Reducing time to merge. Avoid huge pull request. Commit lint online Docstring pydoc Ship good quality code fast Deploying to production frequently. SMall deployments. Keep a relation of bugs per deployment. Are detected fast by the team or user? Are fixed fast? Motivation From external party working with a client, we are the providers, the ones that gives the solution. Objective of a ticket is to give value to the product, not just close tickets. Adapt our work methodology. Client can guide, give us rules. But our work is our project. And needs to be the best possible. We as external need to be lead References Responsibilities of a Tech Lead How to build quality code","title":"Technical Leader"},{"location":"project-management/tech-lead/#technical-leader","text":"The job of a tech lead is to make sure the team work with quality. Part of this job is to plan, design, learn and execute technical solutions and improvements. Interact with other tech leads, sharing information and good practices. Also, clarifying technical doubts with stakeholders. Aside from that, a tech lead is also responsible for the team, understanding member\u2019s strengths, weaknesses. Mentoring and guide them. Some actions that a tech lead can do are: Keep in touch with popular tech conferences and events Read tech blogs daily cult.honeypot Attend meet-ups and also maintain constant communication with other Tech Leads around you.","title":"Technical leader"},{"location":"project-management/tech-lead/#responsibilities","text":"","title":"Responsibilities"},{"location":"project-management/tech-lead/#code-quality-assurance","text":"A code quality culture is essential, at organizational or individual level. High code quality ensures your codebase is maintainable, scalable, and efficient, allowing to deliver new features faster. Establish agreed code styles, such as PEP8 . Keep code consistent by establishing conventions for things like naming, spacing and indentation. No linters issues. Find bugs, duplicity of code, bad practices, TODO/FIXME... use tools like pre-commit hooks , jenkins, sonarqube, etc. Good test code coverage with unit and functional test. Over 95% if possible. Tracking high-quality issues to manage tech debt properly. Prioritize and schedule tech debt. Code reviews. Ensure code reviews are done and are effective. Code reviews should be done by a peer, not by the author. The author should be able to explain the code to the reviewer. The reviewer should be able to understand the code without the author's help. Have a definition of done. A checklist of things that need to be done before a ticket is considered done. Track code quality metrics. Code Complexity, N umber of Peer mentoring when needed.","title":"Code quality assurance"},{"location":"project-management/tech-lead/#code-comments","text":"Establish conventions to ensure comments are useful, improving engineering velocity and code quality. Managing and reducing technical debt While TODO/FIXME are good for single player, but avoid using in teams. Use them to aid your personal code development process and never push them to main code base branch. Focus on the why . Provide context and explain the intent of the code in clases or methods ( docs strings ) Avoid obvious comments.","title":"Code Comments"},{"location":"project-management/tech-lead/#provide-architectural-and-design-direction","text":"Follow SOLID principles. Avoid code rewritten or delete not long after creation. Reduce code smells by refactoring code.","title":"Provide architectural and design direction"},{"location":"project-management/tech-lead/#control-technical-debt","text":"Keeping a low number of technical debt.","title":"Control Technical Debt"},{"location":"project-management/tech-lead/#ensure-code-review-healthiness","text":"Applying good pull request practices. Reducing time to merge. Avoid huge pull request. Commit lint online Docstring pydoc","title":"Ensure code review healthiness"},{"location":"project-management/tech-lead/#ship-good-quality-code-fast","text":"Deploying to production frequently. SMall deployments. Keep a relation of bugs per deployment. Are detected fast by the team or user? Are fixed fast?","title":"Ship good quality code fast"},{"location":"project-management/tech-lead/#motivation","text":"From external party working with a client, we are the providers, the ones that gives the solution. Objective of a ticket is to give value to the product, not just close tickets. Adapt our work methodology. Client can guide, give us rules. But our work is our project. And needs to be the best possible. We as external need to be lead","title":"Motivation"},{"location":"project-management/tech-lead/#references","text":"Responsibilities of a Tech Lead How to build quality code","title":"References"},{"location":"project-management/waterfall-methodology/","text":"Waterfall methodology Waterfall methodology is a linear project management, when requirements are defined at beginning. Task are defined sequential in a project plan (Gantt Chart) to accommodate those requirements The Phases of the Waterfall Model The waterfall approach has, at least, five to seven phases that follow in strict linear order, where a phase can\u2019t begin until the previous phase has been completed. The specific names of the waterfall steps vary, but they were originally defined by its inventor, Winston W. Royce, in the following way: Requirements The key aspect of the waterfall methodology is that all customer requirements are gathered at the beginning of the project, allowing every other phase to be planned without further customer correspondence until the product is complete. It is assumed that all requirements can be gathered at this waterfall management phase. Design The design phase of the waterfall process is best broken up into two subphases: logical design and physical design. The logical design subphase is when possible solutions are brainstormed and theorized. The physical design subphase is when those theoretical ideas and schemas are made into concrete specifications. Implementation The implementation phase is when programmers assimilate the requirements and specifications from the previous phases and produce actual code. Verification This phase is when the customer reviews the product to make sure that it meets the requirements laid out at the beginning of the waterfall project. This is done by releasing the completed product to the customer. Maintenance The customer is regularly using the product during the maintenance phase, discovering bugs, inadequate features and other errors that occurred during production. The production team applies these fixes as necessary until the customer is satisfied.","title":"Waterfall Methodoloy"},{"location":"project-management/waterfall-methodology/#waterfall-methodology","text":"Waterfall methodology is a linear project management, when requirements are defined at beginning. Task are defined sequential in a project plan (Gantt Chart) to accommodate those requirements","title":"Waterfall methodology"},{"location":"project-management/waterfall-methodology/#the-phases-of-the-waterfall-model","text":"The waterfall approach has, at least, five to seven phases that follow in strict linear order, where a phase can\u2019t begin until the previous phase has been completed. The specific names of the waterfall steps vary, but they were originally defined by its inventor, Winston W. Royce, in the following way:","title":"The Phases of the Waterfall Model"},{"location":"project-management/waterfall-methodology/#requirements","text":"The key aspect of the waterfall methodology is that all customer requirements are gathered at the beginning of the project, allowing every other phase to be planned without further customer correspondence until the product is complete. It is assumed that all requirements can be gathered at this waterfall management phase.","title":"Requirements"},{"location":"project-management/waterfall-methodology/#design","text":"The design phase of the waterfall process is best broken up into two subphases: logical design and physical design. The logical design subphase is when possible solutions are brainstormed and theorized. The physical design subphase is when those theoretical ideas and schemas are made into concrete specifications.","title":"Design"},{"location":"project-management/waterfall-methodology/#implementation","text":"The implementation phase is when programmers assimilate the requirements and specifications from the previous phases and produce actual code.","title":"Implementation"},{"location":"project-management/waterfall-methodology/#verification","text":"This phase is when the customer reviews the product to make sure that it meets the requirements laid out at the beginning of the waterfall project. This is done by releasing the completed product to the customer.","title":"Verification"},{"location":"project-management/waterfall-methodology/#maintenance","text":"The customer is regularly using the product during the maintenance phase, discovering bugs, inadequate features and other errors that occurred during production. The production team applies these fixes as necessary until the customer is satisfied.","title":"Maintenance"},{"location":"project-management/work-remotely/","text":"Work remotely In modern world, and specially after COVID-19 pandemic, work remotely is a reality for IT sector. Luckily, I have been working in companies with high compromise and respect for work remotely. And I like it :) Some key notes to take into consideration: Trust people Data driven Think beyond schedule Promote async communication Better planning with less distractions","title":"Work Remotely"},{"location":"project-management/work-remotely/#work-remotely","text":"In modern world, and specially after COVID-19 pandemic, work remotely is a reality for IT sector. Luckily, I have been working in companies with high compromise and respect for work remotely. And I like it :) Some key notes to take into consideration: Trust people Data driven Think beyond schedule Promote async communication Better planning with less distractions","title":"Work remotely"},{"location":"utils/daemon/","text":"Linux Daemon How to create a daemon in linux Create new file in path /etc/init.d/ Open and copy sample in url. Update some of the variables, like name description and path to python file. On python path, point to the python created in the virtualenv, so it will have installed corresponding libraries. All operations below may be required to be executed with sudo Give writes privilegies chmod +x /etc/init.d/<daemon-name> -v Reload daemon configuration systemctl daemon-reload Start service service <daemon-name> start References Instructions Extra docu","title":"Daemon"},{"location":"utils/daemon/#linux-daemon","text":"","title":"Linux Daemon"},{"location":"utils/daemon/#how-to-create-a-daemon-in-linux","text":"Create new file in path /etc/init.d/ Open and copy sample in url. Update some of the variables, like name description and path to python file. On python path, point to the python created in the virtualenv, so it will have installed corresponding libraries. All operations below may be required to be executed with sudo Give writes privilegies chmod +x /etc/init.d/<daemon-name> -v Reload daemon configuration systemctl daemon-reload Start service service <daemon-name> start","title":"How to create a daemon in linux"},{"location":"utils/daemon/#references","text":"Instructions Extra docu","title":"References"},{"location":"utils/databases/","text":"Databases PostgreSQL Relational database, similar to MySql. Once installed, run psql to access via command line to the instances. An instance of postgresql can have multiple databases. Another interesting command is pg_dump , it helps you to generate a backup of your tables into a .sql file. Connection options: -h, --host=HOSTNAME database server host or socket directory (default: \"/var/run/postgresql\") -p, --port=PORT database server port (default: \"5432\") -U, --username=USERNAME database user name (default: \"<user>\") -w, --no-password never prompt for password -W, --password force password prompt (should happen automatically) Google cloud SQL From Google cloud you can create an instance of mysql/postgres for you desire database. You can connect via socket or url to database. Online configuration, require code changes. In addition, you can have a connection using the Cloud SQL Auth proxy Firestore NoSQL database from Google cloud. python libraries to use and access documents. References Database 101 PostgreSQL PostgreSQL Cheatsheet PostgreSQL Backup MongoDB Google Cloud Postgres Firestore","title":"Databases"},{"location":"utils/databases/#databases","text":"","title":"Databases"},{"location":"utils/databases/#postgresql","text":"Relational database, similar to MySql. Once installed, run psql to access via command line to the instances. An instance of postgresql can have multiple databases. Another interesting command is pg_dump , it helps you to generate a backup of your tables into a .sql file. Connection options: -h, --host=HOSTNAME database server host or socket directory (default: \"/var/run/postgresql\") -p, --port=PORT database server port (default: \"5432\") -U, --username=USERNAME database user name (default: \"<user>\") -w, --no-password never prompt for password -W, --password force password prompt (should happen automatically)","title":"PostgreSQL"},{"location":"utils/databases/#google-cloud-sql","text":"From Google cloud you can create an instance of mysql/postgres for you desire database. You can connect via socket or url to database. Online configuration, require code changes. In addition, you can have a connection using the Cloud SQL Auth proxy","title":"Google cloud SQL"},{"location":"utils/databases/#firestore","text":"NoSQL database from Google cloud. python libraries to use and access documents.","title":"Firestore"},{"location":"utils/databases/#references","text":"Database 101 PostgreSQL PostgreSQL Cheatsheet PostgreSQL Backup MongoDB Google Cloud Postgres Firestore","title":"References"},{"location":"utils/ide/","text":"IDE (Integrated Development Environment) An IDE, or Integrated Development Environment, enables programmers to consolidate the different aspects of writing a computer program. It helps you editing your source code, debugging, and auto-completing. Sublime Simple, light and robust. It has everything I need to work with python. Installation by default on my .dotfiles VS Code New, popular and complete. Let's give it a try to this IDE. References VSCode Runme Sublime setup Sublime cheatsheet","title":"IDE"},{"location":"utils/ide/#ide-integrated-development-environment","text":"An IDE, or Integrated Development Environment, enables programmers to consolidate the different aspects of writing a computer program. It helps you editing your source code, debugging, and auto-completing.","title":"IDE (Integrated Development Environment)"},{"location":"utils/ide/#sublime","text":"Simple, light and robust. It has everything I need to work with python. Installation by default on my .dotfiles","title":"Sublime"},{"location":"utils/ide/#vs-code","text":"New, popular and complete. Let's give it a try to this IDE.","title":"VS Code"},{"location":"utils/ide/#references","text":"VSCode Runme Sublime setup Sublime cheatsheet","title":"References"},{"location":"utils/regex/","text":"REGEX Regex is short for Regular Expression. It helps to match, find or manage text. Start by typing OK in the Regex field to proceed to the first step and access the more detailed description. What is Regular Expressions Regex? Regular Expressions are a string of characters that express a search pattern. Often abbreviated as Regex or Regexp. It is especially used to find or replace words in texts. In addition, we can test whether a text complies with the rules we set. For example, let's say you have a list of filenames. And you only want to find files with the pdf extension. Following typing an expression ^\\w+\\.pdf$ will work. The meaning of the definitions in this expression will become clearer as the steps progress. Basic Matchers The character or word we want to find is written directly. It is similar to a normal search process. For example, to find the word curious in the text, type the same. The period . allows selecting any character, including special characters and spaces. Character Sets [abc] If one of the characters in a word can be various characters, we write it in square brackets [] with all alternative characters. For example, to write an expression that can find all the words in the text, type the characters a, e, i, o, u adjacently within square brackets [] . Negated Character Sets [^abc] To find all words in the text below, except for ber and bor, type e and o side by side after the caret ^ character inside square brackets [] . Character Sets: Alphanumeric Range Letter Range [a-z] To find the letters in the specified range, the starting letter and the ending letter are written in square brackets [] with a dash between them - . It is case-sensitive. Type the expression that will select all lowercase letters between e and o , including themselves. Character Sets: Digit Range Number Range [0-9] To find the numbers in the specified range, the starting number and the ending number are written in square brackets [] with a dash - between them. Write an expression that will select all numbers between 3 and 6, including themselves. Repetitions Some special characters are used to specify how many times a character will be repeated in the text. These special characters are the plus + , the asterisk * , and the question mark ? . Repetitions: Asterisk Asterisk * We put an asterisk * after a character to indicate that the character may either not match at all or can match many times. For example, indicate that the letter e should never occur in the text, or it can occur once or more side by side. Repetitions: The Plus Plus Sign + To indicate that a character can occur one or more times, we put a plus sign + after a character. For example, indicate that the letter e can occur one or more times in the text. Repetitions: The Question Mark Question Mark ? To indicate that a character is optional, we put a ? question mark after a character. For example, indicate that the following letter u is optional. Repetitions: Curly Braces To express a certain number of occurrences of a character, we write curly braces {n} along with how many times we want it to occur at the end. For example, indicate that the following letter e can occur only 2 times. To express at least a certain number of occurrences of a character, we write the end of the character at least how many times we want it to occur, with a comma , at the end, and inside curly braces {n, } . For example, indicate that the following letter e can occur at least 3 times. To express the occurrence of a character in a certain number range, we write curly braces {x,y} with the interval we want to go to the end. For example, indicate that the following letter e can only occur between 1 and 3. Grouping Parentheses ( ): Grouping We can group an expression and use these groups to reference or enforce some rules. To group an expression, we enclose () in parentheses. For now just group haa below. Group References Referencing a Group The words ha and haa are grouped below. The first group is used by writing \\1 to avoid rewriting. Here 1 denotes the order of grouping. Type \\2 at the end of the expression to refer to the second group. Non-capturing Groupping Parentheses (?: ) : Non-capturing Grouping You can group an expression and ensure that it is not captured by references. For example, below are two groups. However, the first group reference we denote with \\1 actually indicates the second group, as the first is a non-capturing group. Pipe Character | It allows to specify that an expression can be in different expressions. Thus, all possible statements are written separated by the pipe sign | . This differs from charset [abc] , charsets operate at the character level. Alternatives are at the expression level. For example, the following expression would select both cat and Cat. Add another pipe sign | to the end of the expression and type rat so that all words are selected. ### Escape Character \\ There are special characters that we use when writing regex. { } [ ] / \\ + * . $^ | ? Before we can select these characters themselves, we need to use an escape character \\ . For example, to select the dot . and asterisk * characters in the text, let's add an escape character \\ before it. Start of The String Caret Sign ^ : Selecting by Line Start. We were using [0-9] to find numbers. To find only numbers at the beginning of a line, prefix this expression with the ^ sign. End of The String Dollar Sign $ : Selecting by End of Line. Let's use the $ sign after the html value to find the html texts only at the end of the line. Alphanumeric Word Character \\w : Letter, Number and Underscore The expression \\w is used to find letters, numbers and underscore characters. Let's use the expression \\w to find word characters in the text. Non-alphanumeric Except Word Character \\W The expression \\W is used to find characters other than letters, numbers, and underscores. Digits Number Character \\d \\d is used to find only number characters. Non-digits Except Number Character \\D \\D is used to find non-numeric characters. Whitespace Characters Space Character \\s \\s is used to find only space characters. Non-whitespace Characters Except Space Character \\S \\S is used to find non-space characters. Lookarounds If we want the phrase we're writing to come before or after another phrase, we need to \"lookaround\". Take the next step to learn how to \"lookaround\". Lookarounds: Positive Lookahead Positive Lookahead: (?=) For example, we want to select the hour value in the text. Therefore, to select only the numerical values that have PM after them, we need to write the positive look-ahead expression (?=) after our expression. Include PM after the = sign inside the parentheses. Lookarounds: Negative Lookahead Negative Lookahead: (?!) For example, we want to select numbers other than the hour value in the text. Therefore, we need to write the negative look-ahead (?!) expression after our expression to select only the numerical values that do not have PM after them. Include PM after the ! sign inside the parentheses. Lookarounds: Positive Lookbehind Positive Lookbehind: (?<=) For example, we want to select the price value in the text. Therefore, to select only the number values that preceded by $ , we need to write the positive lookbehind expression (?<=) before our expression. Add \\$ after the = sign inside the brackets. Lookarounds: Negative Lookbehind Negative Lookbehind: (?<!) For example, we want to select numbers in the text other than the price value. Therefore, to select only numeric values that are not preceded by $ , we need to write the negative lookbehind (?<!) before our expression. Add \\$ after the ! inside the brackets. Flags Flags change the output of the expression. That's why flags are also called modifiers. Determines whether the typed expression treats text as separate lines, is case sensitive, or finds all matches. Continue to the next step to learn the flags. Flags: Global The global flag causes the expression to select all matches. If not used it will only select the first match. Now enable the global flag to be able to select all matches. /g all matches Flags: Multiline Regex sees all text as one line. But we use the multiline flag to handle each line separately. In this way, the expressions we write according to the end of the linework separately for each line. Now enable the multiline flag to find all matches. /m multiline Flags: Case Insensitive In order to remove the case-sensitiveness of the expression we have written, we must activate the case-insensitive flag. /i case insensitve Greedy Matching Regex does a greedy match by default. This means that the matchmaking will be as long as possible. Check out the example below. It refers to any match that ends in r and can be any character preceded by it. But it does not stop at the first letter r . Lazy Matching Lazy matchmaking, unlike greedy matching, stops at the first matching. For example, in the example below, add a ? after * to find the first match that ends with the letter r and is preceded by any character. It means that this match will stop at the first letter r . References Learn Regex Cheatsheet Regex Crossword Regex library","title":"Regex"},{"location":"utils/regex/#regex","text":"Regex is short for Regular Expression. It helps to match, find or manage text. Start by typing OK in the Regex field to proceed to the first step and access the more detailed description.","title":"REGEX"},{"location":"utils/regex/#what-is-regular-expressions-regex","text":"Regular Expressions are a string of characters that express a search pattern. Often abbreviated as Regex or Regexp. It is especially used to find or replace words in texts. In addition, we can test whether a text complies with the rules we set. For example, let's say you have a list of filenames. And you only want to find files with the pdf extension. Following typing an expression ^\\w+\\.pdf$ will work. The meaning of the definitions in this expression will become clearer as the steps progress.","title":"What is Regular Expressions Regex?"},{"location":"utils/regex/#basic-matchers","text":"The character or word we want to find is written directly. It is similar to a normal search process. For example, to find the word curious in the text, type the same. The period . allows selecting any character, including special characters and spaces.","title":"Basic Matchers"},{"location":"utils/regex/#character-sets-abc","text":"If one of the characters in a word can be various characters, we write it in square brackets [] with all alternative characters. For example, to write an expression that can find all the words in the text, type the characters a, e, i, o, u adjacently within square brackets [] .","title":"Character Sets [abc]"},{"location":"utils/regex/#negated-character-sets-abc","text":"To find all words in the text below, except for ber and bor, type e and o side by side after the caret ^ character inside square brackets [] .","title":"Negated Character Sets [^abc]"},{"location":"utils/regex/#character-sets-alphanumeric-range","text":"Letter Range [a-z] To find the letters in the specified range, the starting letter and the ending letter are written in square brackets [] with a dash between them - . It is case-sensitive. Type the expression that will select all lowercase letters between e and o , including themselves.","title":"Character Sets: Alphanumeric Range"},{"location":"utils/regex/#character-sets-digit-range","text":"Number Range [0-9] To find the numbers in the specified range, the starting number and the ending number are written in square brackets [] with a dash - between them. Write an expression that will select all numbers between 3 and 6, including themselves.","title":"Character Sets: Digit Range"},{"location":"utils/regex/#repetitions","text":"Some special characters are used to specify how many times a character will be repeated in the text. These special characters are the plus + , the asterisk * , and the question mark ? .","title":"Repetitions"},{"location":"utils/regex/#repetitions-asterisk","text":"Asterisk * We put an asterisk * after a character to indicate that the character may either not match at all or can match many times. For example, indicate that the letter e should never occur in the text, or it can occur once or more side by side.","title":"Repetitions: Asterisk"},{"location":"utils/regex/#repetitions-the-plus","text":"Plus Sign + To indicate that a character can occur one or more times, we put a plus sign + after a character. For example, indicate that the letter e can occur one or more times in the text.","title":"Repetitions: The Plus"},{"location":"utils/regex/#repetitions-the-question-mark","text":"Question Mark ? To indicate that a character is optional, we put a ? question mark after a character. For example, indicate that the following letter u is optional.","title":"Repetitions: The Question Mark"},{"location":"utils/regex/#repetitions-curly-braces","text":"To express a certain number of occurrences of a character, we write curly braces {n} along with how many times we want it to occur at the end. For example, indicate that the following letter e can occur only 2 times. To express at least a certain number of occurrences of a character, we write the end of the character at least how many times we want it to occur, with a comma , at the end, and inside curly braces {n, } . For example, indicate that the following letter e can occur at least 3 times. To express the occurrence of a character in a certain number range, we write curly braces {x,y} with the interval we want to go to the end. For example, indicate that the following letter e can only occur between 1 and 3.","title":"Repetitions: Curly Braces"},{"location":"utils/regex/#grouping","text":"","title":"Grouping"},{"location":"utils/regex/#parentheses-grouping","text":"We can group an expression and use these groups to reference or enforce some rules. To group an expression, we enclose () in parentheses. For now just group haa below.","title":"Parentheses ( ): Grouping"},{"location":"utils/regex/#group-references","text":"Referencing a Group The words ha and haa are grouped below. The first group is used by writing \\1 to avoid rewriting. Here 1 denotes the order of grouping. Type \\2 at the end of the expression to refer to the second group.","title":"Group References"},{"location":"utils/regex/#non-capturing-groupping","text":"Parentheses (?: ) : Non-capturing Grouping You can group an expression and ensure that it is not captured by references. For example, below are two groups. However, the first group reference we denote with \\1 actually indicates the second group, as the first is a non-capturing group.","title":"Non-capturing Groupping"},{"location":"utils/regex/#pipe-character","text":"It allows to specify that an expression can be in different expressions. Thus, all possible statements are written separated by the pipe sign | . This differs from charset [abc] , charsets operate at the character level. Alternatives are at the expression level. For example, the following expression would select both cat and Cat. Add another pipe sign | to the end of the expression and type rat so that all words are selected. ### Escape Character \\ There are special characters that we use when writing regex. { } [ ] / \\ + * . $^ | ? Before we can select these characters themselves, we need to use an escape character \\ . For example, to select the dot . and asterisk * characters in the text, let's add an escape character \\ before it.","title":"Pipe Character |"},{"location":"utils/regex/#start-of-the-string","text":"Caret Sign ^ : Selecting by Line Start. We were using [0-9] to find numbers. To find only numbers at the beginning of a line, prefix this expression with the ^ sign.","title":"Start of The String"},{"location":"utils/regex/#end-of-the-string","text":"Dollar Sign $ : Selecting by End of Line. Let's use the $ sign after the html value to find the html texts only at the end of the line.","title":"End of The String"},{"location":"utils/regex/#alphanumeric","text":"Word Character \\w : Letter, Number and Underscore The expression \\w is used to find letters, numbers and underscore characters. Let's use the expression \\w to find word characters in the text.","title":"Alphanumeric"},{"location":"utils/regex/#non-alphanumeric","text":"Except Word Character \\W The expression \\W is used to find characters other than letters, numbers, and underscores.","title":"Non-alphanumeric"},{"location":"utils/regex/#digits","text":"Number Character \\d \\d is used to find only number characters.","title":"Digits"},{"location":"utils/regex/#non-digits","text":"Except Number Character \\D \\D is used to find non-numeric characters.","title":"Non-digits"},{"location":"utils/regex/#whitespace-characters","text":"Space Character \\s \\s is used to find only space characters.","title":"Whitespace Characters"},{"location":"utils/regex/#non-whitespace-characters","text":"Except Space Character \\S \\S is used to find non-space characters.","title":"Non-whitespace Characters"},{"location":"utils/regex/#lookarounds","text":"If we want the phrase we're writing to come before or after another phrase, we need to \"lookaround\". Take the next step to learn how to \"lookaround\".","title":"Lookarounds"},{"location":"utils/regex/#lookarounds-positive-lookahead","text":"Positive Lookahead: (?=) For example, we want to select the hour value in the text. Therefore, to select only the numerical values that have PM after them, we need to write the positive look-ahead expression (?=) after our expression. Include PM after the = sign inside the parentheses.","title":"Lookarounds: Positive Lookahead"},{"location":"utils/regex/#lookarounds-negative-lookahead","text":"Negative Lookahead: (?!) For example, we want to select numbers other than the hour value in the text. Therefore, we need to write the negative look-ahead (?!) expression after our expression to select only the numerical values that do not have PM after them. Include PM after the ! sign inside the parentheses.","title":"Lookarounds: Negative Lookahead"},{"location":"utils/regex/#lookarounds-positive-lookbehind","text":"Positive Lookbehind: (?<=) For example, we want to select the price value in the text. Therefore, to select only the number values that preceded by $ , we need to write the positive lookbehind expression (?<=) before our expression. Add \\$ after the = sign inside the brackets.","title":"Lookarounds: Positive Lookbehind"},{"location":"utils/regex/#lookarounds-negative-lookbehind","text":"Negative Lookbehind: (?<!) For example, we want to select numbers in the text other than the price value. Therefore, to select only numeric values that are not preceded by $ , we need to write the negative lookbehind (?<!) before our expression. Add \\$ after the ! inside the brackets.","title":"Lookarounds: Negative Lookbehind"},{"location":"utils/regex/#flags","text":"Flags change the output of the expression. That's why flags are also called modifiers. Determines whether the typed expression treats text as separate lines, is case sensitive, or finds all matches. Continue to the next step to learn the flags.","title":"Flags"},{"location":"utils/regex/#flags-global","text":"The global flag causes the expression to select all matches. If not used it will only select the first match. Now enable the global flag to be able to select all matches. /g all matches","title":"Flags: Global"},{"location":"utils/regex/#flags-multiline","text":"Regex sees all text as one line. But we use the multiline flag to handle each line separately. In this way, the expressions we write according to the end of the linework separately for each line. Now enable the multiline flag to find all matches. /m multiline","title":"Flags: Multiline"},{"location":"utils/regex/#flags-case-insensitive","text":"In order to remove the case-sensitiveness of the expression we have written, we must activate the case-insensitive flag. /i case insensitve","title":"Flags: Case Insensitive"},{"location":"utils/regex/#greedy-matching","text":"Regex does a greedy match by default. This means that the matchmaking will be as long as possible. Check out the example below. It refers to any match that ends in r and can be any character preceded by it. But it does not stop at the first letter r .","title":"Greedy Matching"},{"location":"utils/regex/#lazy-matching","text":"Lazy matchmaking, unlike greedy matching, stops at the first matching. For example, in the example below, add a ? after * to find the first match that ends with the letter r and is preceded by any character. It means that this match will stop at the first letter r .","title":"Lazy Matching"},{"location":"utils/regex/#references","text":"Learn Regex Cheatsheet Regex Crossword Regex library","title":"References"},{"location":"utils/supervisor/","text":"Supervisor Service How to create new supervisor service First of all, you need to install supervisor service: sudo apt-get install supervisor -y Path to copy configurations files: /etc/supervisor/conf.d/* After copy file, reread supervisor configuration: sudo supervisorctl -c /etc/supervisor/supervisord.conf reread sudo supervisorctl -c /etc/supervisor/supervisord.conf reload If something went wrong, sock may be deleted in /var/run/supervisor.conf, and supervisorctl status will show an error. To solve, check new configuration files, if something is buggy, fix it or remove conf and run command again. Then, run, to relaoad new configuration and start service again with: sudo supervisorctl reload Now, your supervisor service should be up and running References Some interesting samples and documentation Configuration Sample Documentation","title":"Supervisor"},{"location":"utils/supervisor/#supervisor-service","text":"","title":"Supervisor Service"},{"location":"utils/supervisor/#how-to-create-new-supervisor-service","text":"First of all, you need to install supervisor service: sudo apt-get install supervisor -y Path to copy configurations files: /etc/supervisor/conf.d/* After copy file, reread supervisor configuration: sudo supervisorctl -c /etc/supervisor/supervisord.conf reread sudo supervisorctl -c /etc/supervisor/supervisord.conf reload If something went wrong, sock may be deleted in /var/run/supervisor.conf, and supervisorctl status will show an error. To solve, check new configuration files, if something is buggy, fix it or remove conf and run command again. Then, run, to relaoad new configuration and start service again with: sudo supervisorctl reload Now, your supervisor service should be up and running","title":"How to create new supervisor service"},{"location":"utils/supervisor/#references","text":"Some interesting samples and documentation Configuration Sample Documentation","title":"References"},{"location":"utils/transmission/","text":"Transmission Transmission is a torrent client easy to install and configure to access via interface from outside raspberry. How to install and configure Execute following command in raspberry via ssh: sudo apt-get install transmission-daemon Transmission daemon needs to be updated with some of our configuration. For this, we need to edit configuration file. First, stop daemon sudo systemctl stop transmission-daemon Open and edit configuration file path should be either ~/.config/transmission-daemon/services.json or /etc/transmission/daemon.json After file is updated, we need to restart daemon so configuration is loaded and also start transmission daemon sudo systemctl daemon-reload sudo systemctl start transmission-daemon Now everything should be ready to go, and transmission will be accessible via interface from another computer sharing network with raspberry. References Edit transmission config file Useful info","title":"Transmission"},{"location":"utils/transmission/#transmission","text":"Transmission is a torrent client easy to install and configure to access via interface from outside raspberry.","title":"Transmission"},{"location":"utils/transmission/#how-to-install-and-configure","text":"Execute following command in raspberry via ssh: sudo apt-get install transmission-daemon Transmission daemon needs to be updated with some of our configuration. For this, we need to edit configuration file. First, stop daemon sudo systemctl stop transmission-daemon Open and edit configuration file path should be either ~/.config/transmission-daemon/services.json or /etc/transmission/daemon.json After file is updated, we need to restart daemon so configuration is loaded and also start transmission daemon sudo systemctl daemon-reload sudo systemctl start transmission-daemon Now everything should be ready to go, and transmission will be accessible via interface from another computer sharing network with raspberry.","title":"How to install and configure"},{"location":"utils/transmission/#references","text":"Edit transmission config file Useful info","title":"References"},{"location":"utils/ubuntu/","text":"Ubuntu I am using ubuntu, a linux distro based on Debian. It is open source, with a big community and support for common issues. Oh my zsh Ubuntu has a default shell, to run different commands, but there are others like zsh. To add more power to the shell, I used Oh my zsh on top. It is open source, big collaborative community, customizable and with many plugins to enable to enrich the experience of working with a terminal. dotfiles Private files in a Linux os. Usually, they contain configuration used by different tools, like terminal, profile, k8s, git... It can be used to speed up installation of a new OS from scratch. Totally customizable with my needs. References Linux commands cheatsheet Oh my zsh Bash/zsh shortcuts Getting started with .dotfiles","title":"Ubuntu"},{"location":"utils/ubuntu/#ubuntu","text":"I am using ubuntu, a linux distro based on Debian. It is open source, with a big community and support for common issues.","title":"Ubuntu"},{"location":"utils/ubuntu/#oh-my-zsh","text":"Ubuntu has a default shell, to run different commands, but there are others like zsh. To add more power to the shell, I used Oh my zsh on top. It is open source, big collaborative community, customizable and with many plugins to enable to enrich the experience of working with a terminal.","title":"Oh my zsh"},{"location":"utils/ubuntu/#dotfiles","text":"Private files in a Linux os. Usually, they contain configuration used by different tools, like terminal, profile, k8s, git... It can be used to speed up installation of a new OS from scratch. Totally customizable with my needs.","title":"dotfiles"},{"location":"utils/ubuntu/#references","text":"Linux commands cheatsheet Oh my zsh Bash/zsh shortcuts Getting started with .dotfiles","title":"References"},{"location":"utils/working-environment/","text":"Working environment Structure Create a folder named work in home directory: mkdir $HOME/work Inside, create following structure: mkdir $HOME/work/src : to store repositories mkdir $HOME/work/projects/<company> : to store info from company, onboarding, one to one... folder for Company. Store date in the name of the file, such as meeting-20XX-XX-XX.md mkdir $HOME/work/projects/<projecy-name> : One extra folder for each project. Some common subfolders may be: scripts , documents , sublime mkdir $HOME/work/utils : to store programs, binary, or other packages References Project template","title":"Working environment"},{"location":"utils/working-environment/#working-environment","text":"","title":"Working environment"},{"location":"utils/working-environment/#structure","text":"Create a folder named work in home directory: mkdir $HOME/work Inside, create following structure: mkdir $HOME/work/src : to store repositories mkdir $HOME/work/projects/<company> : to store info from company, onboarding, one to one... folder for Company. Store date in the name of the file, such as meeting-20XX-XX-XX.md mkdir $HOME/work/projects/<projecy-name> : One extra folder for each project. Some common subfolders may be: scripts , documents , sublime mkdir $HOME/work/utils : to store programs, binary, or other packages","title":"Structure"},{"location":"utils/working-environment/#references","text":"Project template","title":"References"}]}