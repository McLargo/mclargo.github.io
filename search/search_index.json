{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to McLargo's technical wiki What you can find in here This MUST be a dynamic documentation, constantly updated. Any new tool learn or good practice to include in my daily work must have a space in this project. Also, guidelines/ways of working applicable, no matter my current project. Grouped in different categories: Topic Description Architecture Related to patterns, architecture designs... DevOps Related to deployments, ci/cd... Programming Directly related to programming Project management Project management stuff Utils Misc of other utils required for daily basics With time and more documents covered, relation between them will be enriched. It shouldn't be a just isolated documents, but more of a wiki to cover my professional philosophy and understanding on technical topics. Using markdown to make it easier to build new documents from IDE. Template sample available for this wiki . Using markdown good practices . Highlighted practices are: Don't use more than one blank line. Remove extra spaces at the end of lines. Always uses spaces instead of hard tabs. Only one H1 per document. Header levels should increment by one -- don't skip levels. Limit line length to 100 characters. Mkdocs as site generator for my documentation. Includes nice features as search, customizable layout and easy deployment to github pages, making it 100% online. Repo that generate this website, can be found in my github. What you cannot find in here cheatsheets, they belong to gists . It enables quicker updates. code, that belongs to github . Either private/public repo. mentoring assets interview process/reviews. Those are not public. books or manuals. Check in Google Drive. References mkdocs mkdocs catalog","title":"Welcome to McLargo's technical wiki"},{"location":"#welcome-to-mclargos-technical-wiki","text":"","title":"Welcome to McLargo's technical wiki"},{"location":"#what-you-can-find-in-here","text":"This MUST be a dynamic documentation, constantly updated. Any new tool learn or good practice to include in my daily work must have a space in this project. Also, guidelines/ways of working applicable, no matter my current project. Grouped in different categories: Topic Description Architecture Related to patterns, architecture designs... DevOps Related to deployments, ci/cd... Programming Directly related to programming Project management Project management stuff Utils Misc of other utils required for daily basics With time and more documents covered, relation between them will be enriched. It shouldn't be a just isolated documents, but more of a wiki to cover my professional philosophy and understanding on technical topics. Using markdown to make it easier to build new documents from IDE. Template sample available for this wiki . Using markdown good practices . Highlighted practices are: Don't use more than one blank line. Remove extra spaces at the end of lines. Always uses spaces instead of hard tabs. Only one H1 per document. Header levels should increment by one -- don't skip levels. Limit line length to 100 characters. Mkdocs as site generator for my documentation. Includes nice features as search, customizable layout and easy deployment to github pages, making it 100% online. Repo that generate this website, can be found in my github.","title":"What you can find in here"},{"location":"#what-you-cannot-find-in-here","text":"cheatsheets, they belong to gists . It enables quicker updates. code, that belongs to github . Either private/public repo. mentoring assets interview process/reviews. Those are not public. books or manuals. Check in Google Drive.","title":"What you cannot find in here"},{"location":"#references","text":"mkdocs mkdocs catalog","title":"References"},{"location":"template/","text":"Template Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Head1 Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. Head2 Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. References Lorem ipsum","title":"Template"},{"location":"template/#template","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.","title":"Template"},{"location":"template/#head1","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.","title":"Head1"},{"location":"template/#head2","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.","title":"Head2"},{"location":"template/#references","text":"Lorem ipsum","title":"References"},{"location":"architecture/abs/","text":"Adaptive Bitrate Streaming Adaptive Bitrate Streaming (ABS) is a streaming technique used to deliver video to the users. The video is encoded in multiple bitrates and resolutions in the server. Not only user can switch between qualities manually. It allows the client who dynamically choose the best quality based on the network conditions. If quality is poor, low quality video is streamed. When network conditions improve, the quality is increased dynamically. Fundamentals Some of the key concepts in ABS are: Chunk : small segment of video file. Usually, between 2-10 seconds. Manifest : file that contains information about the video chunks. Multiple bitrates : video is encoded in multiple bitrates and resolutions. Adaptive algorithm : algorithm executed by the client video player that decides which quality to stream based on network conditions. Protocols Some of the most popular protocols used for ABS are: HTTP Live Streaming (HLS) : Developed by Apple. Dynamic Adaptive Streaming over HTTP (DASH) : Developed by MPEG.","title":"Adaptive Bitrate Streaming"},{"location":"architecture/abs/#adaptive-bitrate-streaming","text":"Adaptive Bitrate Streaming (ABS) is a streaming technique used to deliver video to the users. The video is encoded in multiple bitrates and resolutions in the server. Not only user can switch between qualities manually. It allows the client who dynamically choose the best quality based on the network conditions. If quality is poor, low quality video is streamed. When network conditions improve, the quality is increased dynamically.","title":"Adaptive Bitrate Streaming"},{"location":"architecture/abs/#fundamentals","text":"Some of the key concepts in ABS are: Chunk : small segment of video file. Usually, between 2-10 seconds. Manifest : file that contains information about the video chunks. Multiple bitrates : video is encoded in multiple bitrates and resolutions. Adaptive algorithm : algorithm executed by the client video player that decides which quality to stream based on network conditions.","title":"Fundamentals"},{"location":"architecture/abs/#protocols","text":"Some of the most popular protocols used for ABS are: HTTP Live Streaming (HLS) : Developed by Apple. Dynamic Adaptive Streaming over HTTP (DASH) : Developed by MPEG.","title":"Protocols"},{"location":"architecture/api-gateway/","text":"API Gateway API Gateway is the entry point for all the public request to your backend services. It acts like a reverse proxy, being available to all users by an public IP, and routes the request to the appropriate service. Backend services are hidden to internet by a firewall, so they are protected from direct access from the internet. It is also responsible for the authentication and authorization of the request, logging, caching, load balancing, and rate limiting... You can also apply some transformations to the request and response, like http to grpc, json to xml, headers... For more complex transformation, some API gateways can be configured with a stream processing engine, like Benthos . Apply business logic is discouraged, because it makes the gateway more complex and harder to maintain. Some popular API Gateway services are: nginx, HAProxy and Kong. AWS, Azure and GCP also provide their own API Gateway services.","title":"API Gateway"},{"location":"architecture/api-gateway/#api-gateway","text":"API Gateway is the entry point for all the public request to your backend services. It acts like a reverse proxy, being available to all users by an public IP, and routes the request to the appropriate service. Backend services are hidden to internet by a firewall, so they are protected from direct access from the internet. It is also responsible for the authentication and authorization of the request, logging, caching, load balancing, and rate limiting... You can also apply some transformations to the request and response, like http to grpc, json to xml, headers... For more complex transformation, some API gateways can be configured with a stream processing engine, like Benthos . Apply business logic is discouraged, because it makes the gateway more complex and harder to maintain. Some popular API Gateway services are: nginx, HAProxy and Kong. AWS, Azure and GCP also provide their own API Gateway services.","title":"API Gateway"},{"location":"architecture/architecture-decision-record/","text":"Architecture Decision Record Usually, decision on a project are made verbally, or during a discussion in chat/email, across few people (colleagues, customer...) inside a context. If we don't document these decisions, it is going to be hard to remember why decision was made in few months. If those documents are missing, next generation of developers can just blindly accept it (team will be afraid of make any changes they don't understand) or blindly change it (project can be damage or have side effects). With these documents included inside the project, makes them easier to related and maintain. These documents needs to be simply (low effort is put, but very agile), and developers don't get lazy to create them. Help onboarding people to get up to speed in a asynchronous way. A missing decision can be identify during PR review, when new patter/library is added without corresponding ADR document. Even enough small decisions can compound into a future problem that requires a large process or effort Components Title: brief description of the decision Context: explains the context and facts behind the decision Decision: longer statements of the decision made Status: [Proposed, Accepted, Deprecated, Superseded] Consequences: how it will impact the project itself Tips for using Lightweight ADR Use markdown and store along with the component it relates to in source control Number the files sequentially, don't reuse it (just update if required) Keep it brief and use plain, easy to understand language Peer review as you would code For cross cutting decisions that affect multiple components consider making a separate \"architecture\" repository When you make a decision document it immediately! References Lightweight decision records Documenting architecture decision Write decision record","title":"Architecture Decision Records"},{"location":"architecture/architecture-decision-record/#architecture-decision-record","text":"Usually, decision on a project are made verbally, or during a discussion in chat/email, across few people (colleagues, customer...) inside a context. If we don't document these decisions, it is going to be hard to remember why decision was made in few months. If those documents are missing, next generation of developers can just blindly accept it (team will be afraid of make any changes they don't understand) or blindly change it (project can be damage or have side effects). With these documents included inside the project, makes them easier to related and maintain. These documents needs to be simply (low effort is put, but very agile), and developers don't get lazy to create them. Help onboarding people to get up to speed in a asynchronous way. A missing decision can be identify during PR review, when new patter/library is added without corresponding ADR document. Even enough small decisions can compound into a future problem that requires a large process or effort","title":"Architecture Decision Record"},{"location":"architecture/architecture-decision-record/#components","text":"Title: brief description of the decision Context: explains the context and facts behind the decision Decision: longer statements of the decision made Status: [Proposed, Accepted, Deprecated, Superseded] Consequences: how it will impact the project itself","title":"Components"},{"location":"architecture/architecture-decision-record/#tips-for-using-lightweight-adr","text":"Use markdown and store along with the component it relates to in source control Number the files sequentially, don't reuse it (just update if required) Keep it brief and use plain, easy to understand language Peer review as you would code For cross cutting decisions that affect multiple components consider making a separate \"architecture\" repository When you make a decision document it immediately!","title":"Tips for using Lightweight ADR"},{"location":"architecture/architecture-decision-record/#references","text":"Lightweight decision records Documenting architecture decision Write decision record","title":"References"},{"location":"architecture/architecture-patterns/","text":"Architecture patterns Vertical slice architecture Vertical slices architecture isolate all logic of a feature by grouping functionality together based on workflow or business processes. That means, when adding or changing a feature in an application, it requires development into almost all layers in the application. It is a way of organizing code that is more maintainable and scalable, with faster delivery times. Domain-Driven Design Domain-Driven Design (DDD) is a software development approach that focuses on generating a deep understanding of the business domain. It is a way of thinking and a set of priorities, aimed at accelerating software projects that have to deal with complex domains. Key concepts of DDD are: Ubiquitous language : A language structured around the domain model and used by all team members to connect all the activities of the team with the software. Bounded context : A boundary within which a particular domain model is defined and applicable. It is a way to keep the model clean and focused. Layers : DDD can be implemented in layers, where the domain model is at the center of the architecture. Presentation layer : The user interface, responsible to interact with the application or user. Application layer : orchestrates the domain objects to perform the required operations or use cases. Domain layer : The core of the application, where the business logic resides. Infrastructure layer : The external systems that the application interacts, like persistence, messaging, etc. Hexagonal architecture Also known as ports and adapters, is a way of organizing code that isolates the core business logic from the external world. The core business logic is at the center of the hexagon, and the external world is at the edges. Two important concepts in hexagonal architecture are ports and adapters: Ports : Interfaces that define the way the application core interacts with the outside world. It can be an input/driving port or an output/driven port. Are agnostic to the implementation. Adapters : Implementations of the ports that connect the application core. They are primary/driving adapter when starts an action and secondary/driven adapter when represent connections to the backend, reacting to the primary adapter. This gives the flexibility to make changes on the adapters (implementation) easily. For example you can swap out Oracle or SQL Server, for Mongo or something else. Or enable easily an additional entry point for a new consumer. Your business rules are not bound to the any of these changes outside the core. BFF (Backend for Frontend) BFF is a pattern that consists of creating a backend service customized for a frontend application. Backend is usually a composition of multiple services, each of them providing different functionalities and supporting different protocols. The BFF is responsible for aggregating the data from these services and providing a single endpoint for the frontend specific needs. Having this intermediate transformation layer between the frontend and the backend services, provides the following benefits: Performance : The BFF can aggregate data from multiple services and reduce the number of requests made by the frontend. Security : The BFF can handle authentication and authorization to each backend, so frontend can be stateless. Flexibility : The BFF can be customized for each frontend, providing only the data and format needed by the frontend, reducing the payload size. Efficiency : The BFF can be optimized for the frontend, providing complex common business logic to the frontend. Cache : The BFF can cache data from the backend services, reducing the load on the backend services. References Hexagonal architecture Ports and adapters Vertical slice Backend-for-Frontend","title":"Architecture Patterns"},{"location":"architecture/architecture-patterns/#architecture-patterns","text":"","title":"Architecture patterns"},{"location":"architecture/architecture-patterns/#vertical-slice-architecture","text":"Vertical slices architecture isolate all logic of a feature by grouping functionality together based on workflow or business processes. That means, when adding or changing a feature in an application, it requires development into almost all layers in the application. It is a way of organizing code that is more maintainable and scalable, with faster delivery times.","title":"Vertical slice architecture"},{"location":"architecture/architecture-patterns/#domain-driven-design","text":"Domain-Driven Design (DDD) is a software development approach that focuses on generating a deep understanding of the business domain. It is a way of thinking and a set of priorities, aimed at accelerating software projects that have to deal with complex domains. Key concepts of DDD are: Ubiquitous language : A language structured around the domain model and used by all team members to connect all the activities of the team with the software. Bounded context : A boundary within which a particular domain model is defined and applicable. It is a way to keep the model clean and focused. Layers : DDD can be implemented in layers, where the domain model is at the center of the architecture. Presentation layer : The user interface, responsible to interact with the application or user. Application layer : orchestrates the domain objects to perform the required operations or use cases. Domain layer : The core of the application, where the business logic resides. Infrastructure layer : The external systems that the application interacts, like persistence, messaging, etc.","title":"Domain-Driven Design"},{"location":"architecture/architecture-patterns/#hexagonal-architecture","text":"Also known as ports and adapters, is a way of organizing code that isolates the core business logic from the external world. The core business logic is at the center of the hexagon, and the external world is at the edges. Two important concepts in hexagonal architecture are ports and adapters: Ports : Interfaces that define the way the application core interacts with the outside world. It can be an input/driving port or an output/driven port. Are agnostic to the implementation. Adapters : Implementations of the ports that connect the application core. They are primary/driving adapter when starts an action and secondary/driven adapter when represent connections to the backend, reacting to the primary adapter. This gives the flexibility to make changes on the adapters (implementation) easily. For example you can swap out Oracle or SQL Server, for Mongo or something else. Or enable easily an additional entry point for a new consumer. Your business rules are not bound to the any of these changes outside the core.","title":"Hexagonal architecture"},{"location":"architecture/architecture-patterns/#bff-backend-for-frontend","text":"BFF is a pattern that consists of creating a backend service customized for a frontend application. Backend is usually a composition of multiple services, each of them providing different functionalities and supporting different protocols. The BFF is responsible for aggregating the data from these services and providing a single endpoint for the frontend specific needs. Having this intermediate transformation layer between the frontend and the backend services, provides the following benefits: Performance : The BFF can aggregate data from multiple services and reduce the number of requests made by the frontend. Security : The BFF can handle authentication and authorization to each backend, so frontend can be stateless. Flexibility : The BFF can be customized for each frontend, providing only the data and format needed by the frontend, reducing the payload size. Efficiency : The BFF can be optimized for the frontend, providing complex common business logic to the frontend. Cache : The BFF can cache data from the backend services, reducing the load on the backend services.","title":"BFF (Backend for Frontend)"},{"location":"architecture/architecture-patterns/#references","text":"Hexagonal architecture Ports and adapters Vertical slice Backend-for-Frontend","title":"References"},{"location":"architecture/asynchronous-messaging/","text":"Asynchronous Messaging An asynchronous messaging system is a system that allows communication between two or more parties without the need for the sender to wait for the receiver to respond. The sender sends the message and continues with its own processing. The receiver receives the message and processes it. The receiver can then send a response message to the sender, which can be processed by the sender. Producer and consumer are different processes. There are two main messaging patterns: message queuing and publish/subscribe. Message queuing Multiple producers can send message to the same queue, but once the message is consumed, is removed. Only one consumer consume a specific message. Publish/subscribe Multiple produced can publish a message to a topic, and several consumers can subscribe to the same topic, consuming and handle the message in different ways. Messages can be ephemeral or durable. Used for event-driven architecture . RabbitMQ RabbitMQ supports classic message queuing out of the box. A developer defines named queues, and then publishers can send messages to that named queue. Consumers, in turn, use the same queue to retrieve messages to process them. Kafka Topics Kafka doesn\u2019t implement the notion of a queue. Instead, Kafka stores collections of records in categories called topics . References RabbitMQ vs Kafka","title":"Asynchronous Messaging"},{"location":"architecture/asynchronous-messaging/#asynchronous-messaging","text":"An asynchronous messaging system is a system that allows communication between two or more parties without the need for the sender to wait for the receiver to respond. The sender sends the message and continues with its own processing. The receiver receives the message and processes it. The receiver can then send a response message to the sender, which can be processed by the sender. Producer and consumer are different processes. There are two main messaging patterns: message queuing and publish/subscribe.","title":"Asynchronous Messaging"},{"location":"architecture/asynchronous-messaging/#message-queuing","text":"Multiple producers can send message to the same queue, but once the message is consumed, is removed. Only one consumer consume a specific message.","title":"Message queuing"},{"location":"architecture/asynchronous-messaging/#publishsubscribe","text":"Multiple produced can publish a message to a topic, and several consumers can subscribe to the same topic, consuming and handle the message in different ways. Messages can be ephemeral or durable. Used for event-driven architecture .","title":"Publish/subscribe"},{"location":"architecture/asynchronous-messaging/#rabbitmq","text":"RabbitMQ supports classic message queuing out of the box. A developer defines named queues, and then publishers can send messages to that named queue. Consumers, in turn, use the same queue to retrieve messages to process them.","title":"RabbitMQ"},{"location":"architecture/asynchronous-messaging/#kafka-topics","text":"Kafka doesn\u2019t implement the notion of a queue. Instead, Kafka stores collections of records in categories called topics .","title":"Kafka Topics"},{"location":"architecture/asynchronous-messaging/#references","text":"RabbitMQ vs Kafka","title":"References"},{"location":"architecture/concurrency-patterns/","text":"Concurrency Patterns Concurrent development is a powerful tool that can be used to improve the performance of applications. However, it can also introduce a lot of complexity and potential bugs. We will discuss some common concurrency patterns that can be used to simplify concurrent development. In Go, concurrency is achieved using goroutines and channels. Worker Pool Worker pools involve creating a fixed number of workers that process tasks from a common queue. This pattern is useful when you have a large number of tasks that need to be processed concurrently, but you want to limit the number of workers (goroutines) that are created. Fan-out, Fan-in Multiple workers can be used to process tasks concurrently, and the results can be combined into a single channel. Pipeline It involves a series of stages connected by channels. Each stage is a separate goroutine that processes the data and passes it to the next stage. Pub/Sub Allows messages to be published into more than one channel or topic. Then, each subscriber can receive messages from the channel or topic they are interested in. Rate Limiting Controls the rate at which tasks are processed. It can be used to limit the of requests sent to an external service. Semaphore Limits the number of goroutines that can access a shared resource at the same time. It is useful to control concurrent access to shared resources. References Concurrency patterns in Go","title":"Concurrency patterns"},{"location":"architecture/concurrency-patterns/#concurrency-patterns","text":"Concurrent development is a powerful tool that can be used to improve the performance of applications. However, it can also introduce a lot of complexity and potential bugs. We will discuss some common concurrency patterns that can be used to simplify concurrent development. In Go, concurrency is achieved using goroutines and channels.","title":"Concurrency Patterns"},{"location":"architecture/concurrency-patterns/#worker-pool","text":"Worker pools involve creating a fixed number of workers that process tasks from a common queue. This pattern is useful when you have a large number of tasks that need to be processed concurrently, but you want to limit the number of workers (goroutines) that are created.","title":"Worker Pool"},{"location":"architecture/concurrency-patterns/#fan-out-fan-in","text":"Multiple workers can be used to process tasks concurrently, and the results can be combined into a single channel.","title":"Fan-out, Fan-in"},{"location":"architecture/concurrency-patterns/#pipeline","text":"It involves a series of stages connected by channels. Each stage is a separate goroutine that processes the data and passes it to the next stage.","title":"Pipeline"},{"location":"architecture/concurrency-patterns/#pubsub","text":"Allows messages to be published into more than one channel or topic. Then, each subscriber can receive messages from the channel or topic they are interested in.","title":"Pub/Sub"},{"location":"architecture/concurrency-patterns/#rate-limiting","text":"Controls the rate at which tasks are processed. It can be used to limit the of requests sent to an external service.","title":"Rate Limiting"},{"location":"architecture/concurrency-patterns/#semaphore","text":"Limits the number of goroutines that can access a shared resource at the same time. It is useful to control concurrent access to shared resources.","title":"Semaphore"},{"location":"architecture/concurrency-patterns/#references","text":"Concurrency patterns in Go","title":"References"},{"location":"architecture/data-storage/","text":"Data Storage Data storage is the process of storing data in persistent storage systems, usually huge amounts of data for long periods of time. Data can be originated in different formats and from different sources, to finally are grouped and stored together in a data storage system. Types of data storage Data lakes Data lakes are a storage repository that can store vast amounts of raw data (organized or unorganized) in its native format until it is needed. It is a place to store every type of data. As schema is not defined, data lakes are more flexible and able to capture and store data faster than data warehouses. A data swamp is a data lake containing unstructured, ungoverned data that has gotten out of hand. Data warehouses Data warehouses are a repository for structured, filtered data that has already been processed for a specific purpose, mainly for business data analysis and KPI. Data has already been structured to provide answers to pre-determined questions for analysis. Preprocessing data There are two popular frameworks for building data processing architectures: Extract-Transform-Load (ETL). It has been the traditional approach with data warehousing where you extract data from the sources, transform the data in your data pipelines (clean and aggregate it) and then load it into your data warehouse. Extract-Load-Transform (ELT). It is a newer paradigm, where you extract the raw, unstructured data and load it in your data warehouse. Then, you run the transform step on the data in the data warehouse. With this approach, you can have more flexibility in how you do your data transformations compared to using data pipelines. You need a modern data warehouse to support this. References Data lakes vs data warehouses","title":"Data Storage"},{"location":"architecture/data-storage/#data-storage","text":"Data storage is the process of storing data in persistent storage systems, usually huge amounts of data for long periods of time. Data can be originated in different formats and from different sources, to finally are grouped and stored together in a data storage system.","title":"Data Storage"},{"location":"architecture/data-storage/#types-of-data-storage","text":"","title":"Types of data storage"},{"location":"architecture/data-storage/#data-lakes","text":"Data lakes are a storage repository that can store vast amounts of raw data (organized or unorganized) in its native format until it is needed. It is a place to store every type of data. As schema is not defined, data lakes are more flexible and able to capture and store data faster than data warehouses. A data swamp is a data lake containing unstructured, ungoverned data that has gotten out of hand.","title":"Data lakes"},{"location":"architecture/data-storage/#data-warehouses","text":"Data warehouses are a repository for structured, filtered data that has already been processed for a specific purpose, mainly for business data analysis and KPI. Data has already been structured to provide answers to pre-determined questions for analysis.","title":"Data warehouses"},{"location":"architecture/data-storage/#preprocessing-data","text":"There are two popular frameworks for building data processing architectures: Extract-Transform-Load (ETL). It has been the traditional approach with data warehousing where you extract data from the sources, transform the data in your data pipelines (clean and aggregate it) and then load it into your data warehouse. Extract-Load-Transform (ELT). It is a newer paradigm, where you extract the raw, unstructured data and load it in your data warehouse. Then, you run the transform step on the data in the data warehouse. With this approach, you can have more flexibility in how you do your data transformations compared to using data pipelines. You need a modern data warehouse to support this.","title":"Preprocessing data"},{"location":"architecture/data-storage/#references","text":"Data lakes vs data warehouses","title":"References"},{"location":"architecture/database-patterns/","text":"Database Patterns There are several database patterns, each one with its own use case. Depending on the architecture/use case, you can choose one or another. Master-slave replication There are different database in the cluster, the master and the slave(s). The master database is the node where all the writes operations (CREATE, UPDATE, DELETE) are performed. The slaves databases are the nodes where all the read operations (SELECT) are performed. Data is sync asynchronously to each slave database from the master. Replica databases MUST be read-only , as they are only used for read. Any write in the replica can lead to data inconsistency. Easy to scale out, as you can add more slave nodes to the cluster at any time. Additionally, slave nodes can be configured to be used only for special operations/users, such as reports or heavy query operations, adding extra control. Sharding Involves breaking up a very large data intro smaller, more manageable pieces called shards. Usually, each shard is held in different nodes. Shards are separated, not sharing any of the same data, except replicating table commonly used in all shards. Sharding is implemented at application code. Developers needs to be really careful, because if it is not done correctly, it can lead to data inconsistency/loss. There are two types of sharding: horizontal and vertical. Horizontal Sharding Horizontal sharding is a type of database partitioning that separates very large table into multiple smaller tables, aka partitions. Each partition has the same schema, but entirely different rows. It can be scaled out by adding more nodes to the cluster, and each node can have a different subset of data. A query is faster in a shared architecture, as it only needs to scan a single shard. In a non-shared architecture, the query needs to scan the entire table. Vertical Sharding Type of database partitioning that separates a table with many columns into smaller tables with fewer columns. Each new table has the same number of rows, but fewer columns, grouping columns that are frequently accessed together. It can be scaled up by adding more resources, such as RAM or CPU to the node. References Database sharding","title":"Database Patterns"},{"location":"architecture/database-patterns/#database-patterns","text":"There are several database patterns, each one with its own use case. Depending on the architecture/use case, you can choose one or another.","title":"Database Patterns"},{"location":"architecture/database-patterns/#master-slave-replication","text":"There are different database in the cluster, the master and the slave(s). The master database is the node where all the writes operations (CREATE, UPDATE, DELETE) are performed. The slaves databases are the nodes where all the read operations (SELECT) are performed. Data is sync asynchronously to each slave database from the master. Replica databases MUST be read-only , as they are only used for read. Any write in the replica can lead to data inconsistency. Easy to scale out, as you can add more slave nodes to the cluster at any time. Additionally, slave nodes can be configured to be used only for special operations/users, such as reports or heavy query operations, adding extra control.","title":"Master-slave replication"},{"location":"architecture/database-patterns/#sharding","text":"Involves breaking up a very large data intro smaller, more manageable pieces called shards. Usually, each shard is held in different nodes. Shards are separated, not sharing any of the same data, except replicating table commonly used in all shards. Sharding is implemented at application code. Developers needs to be really careful, because if it is not done correctly, it can lead to data inconsistency/loss. There are two types of sharding: horizontal and vertical.","title":"Sharding"},{"location":"architecture/database-patterns/#horizontal-sharding","text":"Horizontal sharding is a type of database partitioning that separates very large table into multiple smaller tables, aka partitions. Each partition has the same schema, but entirely different rows. It can be scaled out by adding more nodes to the cluster, and each node can have a different subset of data. A query is faster in a shared architecture, as it only needs to scan a single shard. In a non-shared architecture, the query needs to scan the entire table.","title":"Horizontal Sharding"},{"location":"architecture/database-patterns/#vertical-sharding","text":"Type of database partitioning that separates a table with many columns into smaller tables with fewer columns. Each new table has the same number of rows, but fewer columns, grouping columns that are frequently accessed together. It can be scaled up by adding more resources, such as RAM or CPU to the node.","title":"Vertical Sharding"},{"location":"architecture/database-patterns/#references","text":"Database sharding","title":"References"},{"location":"architecture/design-patterns/","text":"Design Patterns In software engineering, a design pattern is a general repeatable solution to a commonly occurring problem in software design. A design pattern isn't a finished design that can be transformed directly into code. It is a description or template for how to solve a problem that can be used in many different situations. Uses of design patterns Utilizing design patterns in the development process can accelerate progress by offering tried-and-tested development models. Effective software design necessitates the consideration of issues that may only surface during later implementation stages. The utilization of design patterns aids in mitigating subtle problems that can lead to significant complications, and it enhances code readability for developers and architects who are acquainted with these patterns. These design patterns offer general solutions, presented in a format that is not bound to specific problems. They serve as templates or blueprints for structuring code to address specific challenges. Furthermore, design patterns enable developers to communicate using widely recognized and comprehensible terms for software interactions. Over time, common design patterns can be refined, rendering them more robust compared to improvised design approaches. Design patterns can be categorized into several different types based on their primary purpose and the problems they solve. Here are some of the most well-known design pattern categories: Creational Patterns These patterns focus on object creation mechanisms, abstracting the process of object instantiation. Common creational patterns include: Singleton Pattern : Ensures a class has only one instance and provides a global point of access to it. Factory Method Pattern : Defines an interface for creating objects, but allows subclasses to alter the type of objects that will be created. Abstract Factory Pattern : Provides an interface for creating families of related or dependent objects without specifying their concrete classes. Structural Patterns These patterns deal with the composition of classes or objects to form larger structures. They help in defining how objects and classes can be combined to form more complex structures. Common structural patterns include: Adapter Pattern : Allows the interface of an existing class to be used as another interface. Decorator Pattern : Attaches additional responsibilities to an object dynamically. Decorators provide a flexible alternative to subclassing for extending functionality. Composite Pattern : Composes objects into tree structures to represent part-whole hierarchies. Clients can treat individual objects and compositions of objects uniformly. Behavioral Patterns These patterns focus on the communication between objects, defining how they interact and distribute responsibilities. Common behavioral patterns include: Observer Pattern : Defines a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically. Strategy Pattern : Defines a family of algorithms, encapsulates each one, and makes them interchangeable. It allows the algorithm to vary independently from clients that use it. Command Pattern : Encapsulates a request as an object, thereby allowing for parameterization of clients with queues, requests, and operations. Architectural Patterns These are higher-level patterns that guide the overall structure and organization of software systems. Examples include: Model-View-Controller (MVC) : Separates an application into three interconnected components - Model (data and business logic), View (user interface), and Controller (user input handling). Model-View-ViewModel (MVVM) : An architectural pattern often used in GUI-based applications, similar to MVC but tailored for modern UI frameworks. Concurrency Patterns These patterns address multi-threading and concurrent programming challenges. Examples include the \"Thread Pool\" and \"Producer-Consumer\" patterns. Anti-Patterns While not traditional design patterns, anti-patterns describe common mistakes or pitfalls in software design and development. Recognizing and avoiding these anti-patterns is crucial for writing maintainable and efficient code. These design patterns are valuable tools for software developers to enhance code quality, maintainability, and reusability by providing well-established solutions to recurring design problems. References Design patterns Design pattern Python","title":"Design Patterns"},{"location":"architecture/design-patterns/#design-patterns","text":"In software engineering, a design pattern is a general repeatable solution to a commonly occurring problem in software design. A design pattern isn't a finished design that can be transformed directly into code. It is a description or template for how to solve a problem that can be used in many different situations.","title":"Design Patterns"},{"location":"architecture/design-patterns/#uses-of-design-patterns","text":"Utilizing design patterns in the development process can accelerate progress by offering tried-and-tested development models. Effective software design necessitates the consideration of issues that may only surface during later implementation stages. The utilization of design patterns aids in mitigating subtle problems that can lead to significant complications, and it enhances code readability for developers and architects who are acquainted with these patterns. These design patterns offer general solutions, presented in a format that is not bound to specific problems. They serve as templates or blueprints for structuring code to address specific challenges. Furthermore, design patterns enable developers to communicate using widely recognized and comprehensible terms for software interactions. Over time, common design patterns can be refined, rendering them more robust compared to improvised design approaches. Design patterns can be categorized into several different types based on their primary purpose and the problems they solve. Here are some of the most well-known design pattern categories:","title":"Uses of design patterns"},{"location":"architecture/design-patterns/#creational-patterns","text":"These patterns focus on object creation mechanisms, abstracting the process of object instantiation. Common creational patterns include: Singleton Pattern : Ensures a class has only one instance and provides a global point of access to it. Factory Method Pattern : Defines an interface for creating objects, but allows subclasses to alter the type of objects that will be created. Abstract Factory Pattern : Provides an interface for creating families of related or dependent objects without specifying their concrete classes.","title":"Creational Patterns"},{"location":"architecture/design-patterns/#structural-patterns","text":"These patterns deal with the composition of classes or objects to form larger structures. They help in defining how objects and classes can be combined to form more complex structures. Common structural patterns include: Adapter Pattern : Allows the interface of an existing class to be used as another interface. Decorator Pattern : Attaches additional responsibilities to an object dynamically. Decorators provide a flexible alternative to subclassing for extending functionality. Composite Pattern : Composes objects into tree structures to represent part-whole hierarchies. Clients can treat individual objects and compositions of objects uniformly.","title":"Structural Patterns"},{"location":"architecture/design-patterns/#behavioral-patterns","text":"These patterns focus on the communication between objects, defining how they interact and distribute responsibilities. Common behavioral patterns include: Observer Pattern : Defines a one-to-many dependency between objects so that when one object changes state, all its dependents are notified and updated automatically. Strategy Pattern : Defines a family of algorithms, encapsulates each one, and makes them interchangeable. It allows the algorithm to vary independently from clients that use it. Command Pattern : Encapsulates a request as an object, thereby allowing for parameterization of clients with queues, requests, and operations.","title":"Behavioral Patterns"},{"location":"architecture/design-patterns/#architectural-patterns","text":"These are higher-level patterns that guide the overall structure and organization of software systems. Examples include: Model-View-Controller (MVC) : Separates an application into three interconnected components - Model (data and business logic), View (user interface), and Controller (user input handling). Model-View-ViewModel (MVVM) : An architectural pattern often used in GUI-based applications, similar to MVC but tailored for modern UI frameworks.","title":"Architectural Patterns"},{"location":"architecture/design-patterns/#concurrency-patterns","text":"These patterns address multi-threading and concurrent programming challenges. Examples include the \"Thread Pool\" and \"Producer-Consumer\" patterns.","title":"Concurrency Patterns"},{"location":"architecture/design-patterns/#anti-patterns","text":"While not traditional design patterns, anti-patterns describe common mistakes or pitfalls in software design and development. Recognizing and avoiding these anti-patterns is crucial for writing maintainable and efficient code. These design patterns are valuable tools for software developers to enhance code quality, maintainability, and reusability by providing well-established solutions to recurring design problems.","title":"Anti-Patterns"},{"location":"architecture/design-patterns/#references","text":"Design patterns Design pattern Python","title":"References"},{"location":"architecture/event-driven/","text":"Event-driven Architecture Generally speaking, a microservice architecture needs that services talk to each other. No matter what approach is used, if Orchestration or Choreography , both can use an Event-driven architecture to communicate with the difference services involved. Events are trigger from services and other services can subscribe to them, and react upon the event information. Communication between services using events is asynchronous . Core components of event-driven architecture The core components of an EDA are: Producer: upstream service that generates events and publishes them to the event broker. Consumers: one or more downstream services that subscribe to events from the event broker. Brokers: a message broker that receives events from producers, storage them and delivers them to consumers. Each event might be delivered to one or more consumers. Events An event is a message that represents a change in state or an action in the system. All events are messages, but not all messages are events. Usually are represented in a key/value format, where the key is the event type that identifies the event and the value is the event payload, which contains the information about the event. Events are immutable. Once an event is published, it can't be changed. If a change is needed, a new event must be published with new data. Domain events A domain event is, something that happened in the domain that you want other parts of the same domain (in-process) to be aware of. More into details, we can distinguish between: Events represent a past, something that already happened and can\u2019t be undone. Commands, on the other hand, represent a wish, an action in the future which can be rejected. Database events Changes in database can also be used as events. For example, a database trigger when a row is inserted, updated or deleted, called CDC (Change Data Capture). Event producers Event producer are the part of the system that generates events and publishes them to the event broker. They can be any part of the system, like a service, a database, a user interface, etc. It decides when to publish an event, serialize the event in the corresponding format and send it to the event broker. Events can be transmitted immediately, or in batched mode, depending on the configuration of the event producer. Event broker It's an immutable log of events. It receives events from producers and stores them. The event broker is responsible for delivering events to consumers. Event stream An event stream will be a sequence of events that are related to each other. It can be consumed by multiple consumers. Consumers can read the events in the stream in the order they were published, as many times as they want (in case consumer crashes and needs to recover to get the last picture). Messages can be deleted from the stream after a certain period of time, or after a certain number of messages. Kafka or Pulsar are examples of event brokers that support event streams. Event queue Traditional message queues are used to deliver messages to a single consumer. Once a message is consumed, it's removed from the queue. Multiple consumers consume different messages from the queue. RabbitMQ and ActiveMQ are examples of event brokers that support event queues. Event consumers Event consumers are the part of the system that subscribes to events from the event broker. In case of event stream, each consumer will have a offset to know which event has been read (offset is the position of the last event read, and can be updated). On the order hand, an event queue message will be consumed and removed from the queue. It can send an acknowledgment to the broker to confirm that the message has been processed. References Domain event","title":"Event Driven"},{"location":"architecture/event-driven/#event-driven-architecture","text":"Generally speaking, a microservice architecture needs that services talk to each other. No matter what approach is used, if Orchestration or Choreography , both can use an Event-driven architecture to communicate with the difference services involved. Events are trigger from services and other services can subscribe to them, and react upon the event information. Communication between services using events is asynchronous .","title":"Event-driven Architecture"},{"location":"architecture/event-driven/#core-components-of-event-driven-architecture","text":"The core components of an EDA are: Producer: upstream service that generates events and publishes them to the event broker. Consumers: one or more downstream services that subscribe to events from the event broker. Brokers: a message broker that receives events from producers, storage them and delivers them to consumers. Each event might be delivered to one or more consumers.","title":"Core components of event-driven architecture"},{"location":"architecture/event-driven/#events","text":"An event is a message that represents a change in state or an action in the system. All events are messages, but not all messages are events. Usually are represented in a key/value format, where the key is the event type that identifies the event and the value is the event payload, which contains the information about the event. Events are immutable. Once an event is published, it can't be changed. If a change is needed, a new event must be published with new data.","title":"Events"},{"location":"architecture/event-driven/#domain-events","text":"A domain event is, something that happened in the domain that you want other parts of the same domain (in-process) to be aware of. More into details, we can distinguish between: Events represent a past, something that already happened and can\u2019t be undone. Commands, on the other hand, represent a wish, an action in the future which can be rejected.","title":"Domain events"},{"location":"architecture/event-driven/#database-events","text":"Changes in database can also be used as events. For example, a database trigger when a row is inserted, updated or deleted, called CDC (Change Data Capture).","title":"Database events"},{"location":"architecture/event-driven/#event-producers","text":"Event producer are the part of the system that generates events and publishes them to the event broker. They can be any part of the system, like a service, a database, a user interface, etc. It decides when to publish an event, serialize the event in the corresponding format and send it to the event broker. Events can be transmitted immediately, or in batched mode, depending on the configuration of the event producer.","title":"Event producers"},{"location":"architecture/event-driven/#event-broker","text":"It's an immutable log of events. It receives events from producers and stores them. The event broker is responsible for delivering events to consumers.","title":"Event broker"},{"location":"architecture/event-driven/#event-stream","text":"An event stream will be a sequence of events that are related to each other. It can be consumed by multiple consumers. Consumers can read the events in the stream in the order they were published, as many times as they want (in case consumer crashes and needs to recover to get the last picture). Messages can be deleted from the stream after a certain period of time, or after a certain number of messages. Kafka or Pulsar are examples of event brokers that support event streams.","title":"Event stream"},{"location":"architecture/event-driven/#event-queue","text":"Traditional message queues are used to deliver messages to a single consumer. Once a message is consumed, it's removed from the queue. Multiple consumers consume different messages from the queue. RabbitMQ and ActiveMQ are examples of event brokers that support event queues.","title":"Event queue"},{"location":"architecture/event-driven/#event-consumers","text":"Event consumers are the part of the system that subscribes to events from the event broker. In case of event stream, each consumer will have a offset to know which event has been read (offset is the position of the last event read, and can be updated). On the order hand, an event queue message will be consumed and removed from the queue. It can send an acknowledgment to the broker to confirm that the message has been processed.","title":"Event consumers"},{"location":"architecture/event-driven/#references","text":"Domain event","title":"References"},{"location":"architecture/grpc/","text":"gRPC gRPC is a remote procedure call (RPC) framework that uses HTTP/2 for transport and protobuf. Used to communicate services in a microservices architecture. From the user's point of view, it acts like a local function call, but instead the call is executed on a remote server. Benefits of gRPC Scalability: it uses a binary serialization format (Protocol Buffers), meaning smaller message sizes and faster processing times compared to other protocols such as json or xml. Strong typing: you define the structure and types of the messages in a .proto file. Bi-directional streaming: gRPC supports streaming requests and responses. The two streams operate independently, so the client and server can read and write in any order. Other features: gRPC supports out of the box authentication, deadlines, load balancing, service discovery and more. How does gRPC work? The diagram below illustrates the overall data flow for gRPC. References gRPC concepts","title":"gRPC"},{"location":"architecture/grpc/#grpc","text":"gRPC is a remote procedure call (RPC) framework that uses HTTP/2 for transport and protobuf. Used to communicate services in a microservices architecture. From the user's point of view, it acts like a local function call, but instead the call is executed on a remote server.","title":"gRPC"},{"location":"architecture/grpc/#benefits-of-grpc","text":"Scalability: it uses a binary serialization format (Protocol Buffers), meaning smaller message sizes and faster processing times compared to other protocols such as json or xml. Strong typing: you define the structure and types of the messages in a .proto file. Bi-directional streaming: gRPC supports streaming requests and responses. The two streams operate independently, so the client and server can read and write in any order. Other features: gRPC supports out of the box authentication, deadlines, load balancing, service discovery and more.","title":"Benefits of gRPC"},{"location":"architecture/grpc/#how-does-grpc-work","text":"The diagram below illustrates the overall data flow for gRPC.","title":"How does gRPC work?"},{"location":"architecture/grpc/#references","text":"gRPC concepts","title":"References"},{"location":"architecture/microservices/","text":"Microservices It is architectural style that focuses on discrete services instead of a monolithic design . Microservices architecture breaks down an application into smaller, independent services that each serve a specific business capability or function. These services are developed and deployed independently, often with their own codebase and database. Principles Independent development and scaling of individual services. Developers can work on small, focused teams, and each service can be scaled independently based on its specific requirements and on demand, making resource allocation more efficient. Each microservice can se different technology stacks and databases for each service, depending on the requirements. This flexibility allows you to choose the best tool for each job. Microservices can be deployed independently, making it easier to implement CI/CD pipelines for each service. This results in faster and more frequent deployments with less risk. More resilient than a monolithic system. If one service fails, it doesn't necessarily impact the entire system, as long as other services can continue to function. Managing the interactions between multiple services can introduce complexity in terms of service discovery, communication, and monitoring. Orchestration Orchestration is an approach where one of the services acts as the coordinator and orchestrator of the overall distributed state change, managing the execution of tasks or services It defines the order and flow of activities and often involves a central point of control that dictates what should happen next. The orchestrator is responsible for making decisions and ensuring that each component of the system performs its tasks in the prescribed sequence. It can be simpler to design and implement for relatively small-scale systems with a limited number of components. However, as the system grows in complexity, the central orchestrator can become a bottleneck and may make the system less scalable and more challenging to maintain. It is easier to implement fault tolerance and error handling because the central orchestrator can monitor the execution of tasks and react to failures by rerouting or retrying steps. It also provides a central point for monitoring and logging, making it easier to track the progress of tasks and diagnose issues. There is less direct communication between services since most interactions are mediated by the orchestrator. This can reduce the need for services to be aware of each other's existence. Choreography Choreography is an approach of service coordination where participants exchange events without a centralized point of control. With this pattern, each service performs a local transaction and publishes events that trigger local transactions in other services. Each component of the system participates in decision-making about a business transaction's workflow, instead of relying on a central point of control. The interaction and coordination between services are determined by a set of predefined rules or contracts. There is no single point of control, and the overall behavior of the system emerges from the interactions between the participating components. It can be more suitable for large-scale and complex distributed systems because it distributes the decision-making and control among individual components. As new services are added or existing ones are modified, the system can remain more flexible and scalable. However, choreographed systems can be harder to visualize and debug, especially as the number of interactions between services increases. Also, relies on the individual services to handle errors and failures gracefully, making it more challenging to ensure consistent error-handling strategies across the entire system. It may require more advanced monitoring and tracing tools to gain insights into the interactions between services and to identify bottlenecks or errors. Finally, it requires services to communicate directly with each other, which can lead to more inter-service communication. This increased communication can result in more complex service discovery and network traffic management. Architecture styles Services in a microservice architecture needs to communicate between each other. There are different architectural styles to achieve this communication: REST : Representational State Transfer is an architectural style that uses standard HTTP methods (GET, POST, PUT, DELETE) to interact with resources. RESTful services are stateless, meaning that each request from a client contains all the information needed to process the request. REST is simple to understand and implement, and it can be used over any protocol that supports request/response semantics. RPC: Remote Procedure Call is a protocol that allows a program to execute code on a remote server. A popular RPC framework is gRPC . References Orchestration vs choreography Saga pattern","title":"Microservices"},{"location":"architecture/microservices/#microservices","text":"It is architectural style that focuses on discrete services instead of a monolithic design . Microservices architecture breaks down an application into smaller, independent services that each serve a specific business capability or function. These services are developed and deployed independently, often with their own codebase and database.","title":"Microservices"},{"location":"architecture/microservices/#principles","text":"Independent development and scaling of individual services. Developers can work on small, focused teams, and each service can be scaled independently based on its specific requirements and on demand, making resource allocation more efficient. Each microservice can se different technology stacks and databases for each service, depending on the requirements. This flexibility allows you to choose the best tool for each job. Microservices can be deployed independently, making it easier to implement CI/CD pipelines for each service. This results in faster and more frequent deployments with less risk. More resilient than a monolithic system. If one service fails, it doesn't necessarily impact the entire system, as long as other services can continue to function. Managing the interactions between multiple services can introduce complexity in terms of service discovery, communication, and monitoring.","title":"Principles"},{"location":"architecture/microservices/#orchestration","text":"Orchestration is an approach where one of the services acts as the coordinator and orchestrator of the overall distributed state change, managing the execution of tasks or services It defines the order and flow of activities and often involves a central point of control that dictates what should happen next. The orchestrator is responsible for making decisions and ensuring that each component of the system performs its tasks in the prescribed sequence. It can be simpler to design and implement for relatively small-scale systems with a limited number of components. However, as the system grows in complexity, the central orchestrator can become a bottleneck and may make the system less scalable and more challenging to maintain. It is easier to implement fault tolerance and error handling because the central orchestrator can monitor the execution of tasks and react to failures by rerouting or retrying steps. It also provides a central point for monitoring and logging, making it easier to track the progress of tasks and diagnose issues. There is less direct communication between services since most interactions are mediated by the orchestrator. This can reduce the need for services to be aware of each other's existence.","title":"Orchestration"},{"location":"architecture/microservices/#choreography","text":"Choreography is an approach of service coordination where participants exchange events without a centralized point of control. With this pattern, each service performs a local transaction and publishes events that trigger local transactions in other services. Each component of the system participates in decision-making about a business transaction's workflow, instead of relying on a central point of control. The interaction and coordination between services are determined by a set of predefined rules or contracts. There is no single point of control, and the overall behavior of the system emerges from the interactions between the participating components. It can be more suitable for large-scale and complex distributed systems because it distributes the decision-making and control among individual components. As new services are added or existing ones are modified, the system can remain more flexible and scalable. However, choreographed systems can be harder to visualize and debug, especially as the number of interactions between services increases. Also, relies on the individual services to handle errors and failures gracefully, making it more challenging to ensure consistent error-handling strategies across the entire system. It may require more advanced monitoring and tracing tools to gain insights into the interactions between services and to identify bottlenecks or errors. Finally, it requires services to communicate directly with each other, which can lead to more inter-service communication. This increased communication can result in more complex service discovery and network traffic management.","title":"Choreography"},{"location":"architecture/microservices/#architecture-styles","text":"Services in a microservice architecture needs to communicate between each other. There are different architectural styles to achieve this communication: REST : Representational State Transfer is an architectural style that uses standard HTTP methods (GET, POST, PUT, DELETE) to interact with resources. RESTful services are stateless, meaning that each request from a client contains all the information needed to process the request. REST is simple to understand and implement, and it can be used over any protocol that supports request/response semantics. RPC: Remote Procedure Call is a protocol that allows a program to execute code on a remote server. A popular RPC framework is gRPC .","title":"Architecture styles"},{"location":"architecture/microservices/#references","text":"Orchestration vs choreography Saga pattern","title":"References"},{"location":"architecture/monolithic/","text":"Monolithic Architecture In a monolithic architecture, the entire application is built as a single, tightly integrated unit. All components, modules, and functionalities are part of a single codebase and run within the same process. As an alternative to this architecture design, we can find service-oriented architecture (SOA) such as microservices . Principles Developers can easily share code between modules and functionalities, but development and maintenance can be difficult as the application grows. Understanding the entire codebase can become increasingly difficult Scaling can be a challenge, as entire application needs to be scaled to support extra load of a single component. Are built using a single technology stack and database. Any changes or updates of technology affects entire application. Deploying changes to a monolithic application often requires updating the entire application, which can lead to longer deployment times and potential disruptions. Low fault tolerance and resilience, a single component it can potentially bring down the entire application.","title":"Monolithic Architecture"},{"location":"architecture/monolithic/#monolithic-architecture","text":"In a monolithic architecture, the entire application is built as a single, tightly integrated unit. All components, modules, and functionalities are part of a single codebase and run within the same process. As an alternative to this architecture design, we can find service-oriented architecture (SOA) such as microservices .","title":"Monolithic Architecture"},{"location":"architecture/monolithic/#principles","text":"Developers can easily share code between modules and functionalities, but development and maintenance can be difficult as the application grows. Understanding the entire codebase can become increasingly difficult Scaling can be a challenge, as entire application needs to be scaled to support extra load of a single component. Are built using a single technology stack and database. Any changes or updates of technology affects entire application. Deploying changes to a monolithic application often requires updating the entire application, which can lead to longer deployment times and potential disruptions. Low fault tolerance and resilience, a single component it can potentially bring down the entire application.","title":"Principles"},{"location":"architecture/rest/","text":"REST REST (Representational State Transfer) is an architectural style that defines a set of constraints to be used for creating web services. RESTful web services allow clients to access and manipulate resources using a uniform and predefined set of stateless operations. It uses standard HTTP methods (GET, POST, PUT, DELETE) to interact with resources. It is commonly used to build RESTful Apis. Design/develop good API from starters is important, as they can grow quickly and it is hard to change them later. Also, new developers can have a quicker ramp up to the project. RESTful API Making Requests Rest requires the client to send a request to the server to execute an action. A request is made up of: HTTP verb, which defines what kind of operation to perform. headers, which allows the client to pass along information about the request. path to a resource. message /payload containing data (optional). HTTP Verbs There are 4 basic HTTP verbs (commonly called CRUD) used in requests: GET: retrieve a specific resource (by id) or a collection of resources POST: create a new resource PUT: update a specific resource (by id) DELETE: remove a specific resource by id Headers and Accept parameters In the header of the request, you can specify the type of data you want to send or receive. Or any other metadata you want to pass along with the request, such as authentication token. Paths A path to the resource you want to interact with. It is a string that follows the URL. Aside from the ids of the resources, it can contain parameters to filter, sort or paginate the results. Message / Payload The message is the data you want to send to the server. It can be in different formats, such as JSON, XML, or form data. The format send is specified in the headers. Responses Content Types When the server sends a response, it includes a content type header that tells the client what kind of data is being sent. HTTP Status Code There are several HTTP status codes that the server can send back to the client. They are divided into ranges: 2XX -> success codes range 200 OK 201 Created 202 Accepted 204 No Content 3XX -> redirect codes range 4XX -> client error codes range 400 Bad Request 401 Unauthorized 403 Forbidden 404 Not Found 405 Method Not Allowed 409 Conflict 429 Too Many Requests 5XX -> server error codes range But, HTTP status codes are not enough to explain what went wrong. To help your API consumers, include a structured JSON error message, like: Error code: A machine-readable error code that identifies the specific error condition. Error message: A human-readable message that provides a detailed explanation of the error. Error context: Additional information related to the error, such as the request ID, the request parameters that caused the error, or the field(s) in the request that caused the error. Error links: URLs to resources or documentation that provide additional information about the error and how it can be resolved. Timestamp: The time when the error occurred. Good practices RESTful API parametrize your API endpoints to version them. e.g. /api/v1/users group your endpoints by resources and scopes (e.g. /api/v1/users , /api/v1/admin/users ) resources in plural. e.g. /users , /products use - instead of _ don't use verbs, use names. Actions are implicit in HTTP methods basic structure GET ALL `api/<resources>` HTTP status code 200 OK for a successful GET of resource, or empty results. GET BY ID `api/<resources>/<id>` HTTP status code 200 OK for a successful GET of resource. HTTP status code 404 KO for a KO GET of a not found resource. POST `api/<resources>` If a resource has been created on the origin server, the response SHOULD be 201 (Created) and contain an entity which describes the status of the request and refers to the new resource, and a Location header (see section 14.30). PUT `api/<resources>/<id>` HTTP status code 200 OK for a successful PUT of an update to an existing resource. No response body needed. (Per Section 9.6, 204 No Content is even more appropriate.) HTTP status code 400 Bad Request for an unsuccessful PUT, with natural-language text (such as English) in the response body that explains why the PUT failed. (RFC 2616 Section 10.4) Secrets Never store unencrypted secrets in .git repositories. If a secret enters a repository, private or public, then it should be considered compromised. Add sensitive files in .gitignore Store secrets safely. Encrypting your secrets using common tools, such as git secret. Storing them within a git repository can be beneficial when working in teams as it keeps secrets synced. A better solution is to use a \"Secrets as a service\", such as AWS Secrets Manager, Vault, infiscal , Google Secrets Manager...and forget about secrets management. Good to implement a secret rotation policy. This is a good practice to avoid security issues. Finally, in pre-commit you can add a hook to check if there are secrets in the code you are trying to push to the repository. References API Design Pragmatic restful API Naming rest API endpoints PUT vs POST","title":"REST"},{"location":"architecture/rest/#rest","text":"REST (Representational State Transfer) is an architectural style that defines a set of constraints to be used for creating web services. RESTful web services allow clients to access and manipulate resources using a uniform and predefined set of stateless operations. It uses standard HTTP methods (GET, POST, PUT, DELETE) to interact with resources. It is commonly used to build RESTful Apis. Design/develop good API from starters is important, as they can grow quickly and it is hard to change them later. Also, new developers can have a quicker ramp up to the project.","title":"REST"},{"location":"architecture/rest/#restful-api","text":"","title":"RESTful API"},{"location":"architecture/rest/#making-requests","text":"Rest requires the client to send a request to the server to execute an action. A request is made up of: HTTP verb, which defines what kind of operation to perform. headers, which allows the client to pass along information about the request. path to a resource. message /payload containing data (optional).","title":"Making Requests"},{"location":"architecture/rest/#http-verbs","text":"There are 4 basic HTTP verbs (commonly called CRUD) used in requests: GET: retrieve a specific resource (by id) or a collection of resources POST: create a new resource PUT: update a specific resource (by id) DELETE: remove a specific resource by id","title":"HTTP Verbs"},{"location":"architecture/rest/#headers-and-accept-parameters","text":"In the header of the request, you can specify the type of data you want to send or receive. Or any other metadata you want to pass along with the request, such as authentication token.","title":"Headers and Accept parameters"},{"location":"architecture/rest/#paths","text":"A path to the resource you want to interact with. It is a string that follows the URL. Aside from the ids of the resources, it can contain parameters to filter, sort or paginate the results.","title":"Paths"},{"location":"architecture/rest/#message-payload","text":"The message is the data you want to send to the server. It can be in different formats, such as JSON, XML, or form data. The format send is specified in the headers.","title":"Message / Payload"},{"location":"architecture/rest/#responses","text":"","title":"Responses"},{"location":"architecture/rest/#content-types","text":"When the server sends a response, it includes a content type header that tells the client what kind of data is being sent.","title":"Content Types"},{"location":"architecture/rest/#http-status-code","text":"There are several HTTP status codes that the server can send back to the client. They are divided into ranges: 2XX -> success codes range 200 OK 201 Created 202 Accepted 204 No Content 3XX -> redirect codes range 4XX -> client error codes range 400 Bad Request 401 Unauthorized 403 Forbidden 404 Not Found 405 Method Not Allowed 409 Conflict 429 Too Many Requests 5XX -> server error codes range But, HTTP status codes are not enough to explain what went wrong. To help your API consumers, include a structured JSON error message, like: Error code: A machine-readable error code that identifies the specific error condition. Error message: A human-readable message that provides a detailed explanation of the error. Error context: Additional information related to the error, such as the request ID, the request parameters that caused the error, or the field(s) in the request that caused the error. Error links: URLs to resources or documentation that provide additional information about the error and how it can be resolved. Timestamp: The time when the error occurred.","title":"HTTP Status Code"},{"location":"architecture/rest/#good-practices-restful-api","text":"parametrize your API endpoints to version them. e.g. /api/v1/users group your endpoints by resources and scopes (e.g. /api/v1/users , /api/v1/admin/users ) resources in plural. e.g. /users , /products use - instead of _ don't use verbs, use names. Actions are implicit in HTTP methods basic structure GET ALL `api/<resources>` HTTP status code 200 OK for a successful GET of resource, or empty results. GET BY ID `api/<resources>/<id>` HTTP status code 200 OK for a successful GET of resource. HTTP status code 404 KO for a KO GET of a not found resource. POST `api/<resources>` If a resource has been created on the origin server, the response SHOULD be 201 (Created) and contain an entity which describes the status of the request and refers to the new resource, and a Location header (see section 14.30). PUT `api/<resources>/<id>` HTTP status code 200 OK for a successful PUT of an update to an existing resource. No response body needed. (Per Section 9.6, 204 No Content is even more appropriate.) HTTP status code 400 Bad Request for an unsuccessful PUT, with natural-language text (such as English) in the response body that explains why the PUT failed. (RFC 2616 Section 10.4)","title":"Good practices RESTful API"},{"location":"architecture/rest/#secrets","text":"Never store unencrypted secrets in .git repositories. If a secret enters a repository, private or public, then it should be considered compromised. Add sensitive files in .gitignore Store secrets safely. Encrypting your secrets using common tools, such as git secret. Storing them within a git repository can be beneficial when working in teams as it keeps secrets synced. A better solution is to use a \"Secrets as a service\", such as AWS Secrets Manager, Vault, infiscal , Google Secrets Manager...and forget about secrets management. Good to implement a secret rotation policy. This is a good practice to avoid security issues. Finally, in pre-commit you can add a hook to check if there are secrets in the code you are trying to push to the repository.","title":"Secrets"},{"location":"architecture/rest/#references","text":"API Design Pragmatic restful API Naming rest API endpoints PUT vs POST","title":"References"},{"location":"architecture/service-mesh/","text":"Service Mesh A service mesh is an infrastructure layer that handles communication between the microservices/machines in your backend. As microservices are added to the backend, the number of connections between them grows exponentially. This can lead to a lot of complexity and potential issues. A service mesh can help manage this complexity by providing a way to manage and monitor the communication between microservices. Features Service Discovery: for each microservice, new instances are constantly being spun up/down. The service mesh keeps track of the IP addresses/port number of these instances and routes requests to/from them. Load balancing: the service mesh can distribute the load across the different instances of a microservice, specially to one with less load. Observability: the service mesh can provide metrics, logs and traces about the communication between microservices. Resilience: the service mesh can provide features such as circuit breaking, retries, timeouts, rate limits, etc. Security: the service mesh can provide features such as encryption, authentication and, authorization from service to service. Also you can set limits on which services can talk to each other. Deployment: the service mesh can provide features such as canary deployments , A/B deployments , etc. Architecture Data Plane : the data plane is the part of the service mesh that is responsible for handling the actual traffic (inbound/outbound communication) between microservices. Consists of a set of proxies deployed alongside every instance. Envoy is a proxy sample. Control Plane : the control plane is the part of the service mesh that is responsible for managing the configuration of the data plane, aside from handling service discovery. You can configure things like retries, rate limiting policies, health checks, etc. in the control plane. The control plane Istio is a sample.","title":"Service Mesh"},{"location":"architecture/service-mesh/#service-mesh","text":"A service mesh is an infrastructure layer that handles communication between the microservices/machines in your backend. As microservices are added to the backend, the number of connections between them grows exponentially. This can lead to a lot of complexity and potential issues. A service mesh can help manage this complexity by providing a way to manage and monitor the communication between microservices.","title":"Service Mesh"},{"location":"architecture/service-mesh/#features","text":"Service Discovery: for each microservice, new instances are constantly being spun up/down. The service mesh keeps track of the IP addresses/port number of these instances and routes requests to/from them. Load balancing: the service mesh can distribute the load across the different instances of a microservice, specially to one with less load. Observability: the service mesh can provide metrics, logs and traces about the communication between microservices. Resilience: the service mesh can provide features such as circuit breaking, retries, timeouts, rate limits, etc. Security: the service mesh can provide features such as encryption, authentication and, authorization from service to service. Also you can set limits on which services can talk to each other. Deployment: the service mesh can provide features such as canary deployments , A/B deployments , etc.","title":"Features"},{"location":"architecture/service-mesh/#architecture","text":"Data Plane : the data plane is the part of the service mesh that is responsible for handling the actual traffic (inbound/outbound communication) between microservices. Consists of a set of proxies deployed alongside every instance. Envoy is a proxy sample. Control Plane : the control plane is the part of the service mesh that is responsible for managing the configuration of the data plane, aside from handling service discovery. You can configure things like retries, rate limiting policies, health checks, etc. in the control plane. The control plane Istio is a sample.","title":"Architecture"},{"location":"architecture/solid/","text":"SOLID Principles SOLID is an acronym of the first five object-oriented design (OOD) principles. These principles establish practices that lend to developing software with considerations for maintaining and extending as the project grows. Adopting these practices can also contribute to avoiding code smells, refactoring code, and Agile or Adaptive software development. Single-Responsibility Principle A class should have only a single responsibility. Only one potential change in the software\u2019s specification should be able to affect the specification of the class. Open/Closed Principle Software entities should be open for EXTENSION, but closed for MODIFICATION. Allow behavior to be extended without modifying the source code. Liskov Substitution Principle Objects in a program should be replaceable with instances of their subtypes without altering the correctness of that program. Interface Segregation Principle Many client-specific interfaces are better than one general-purpose interface. No client should be forced to depend on methods it does not use. Dependency Inversion Principle One should depend upon abstractions, not concretions. High-level modules should not depend on low-level modules. Both should depend on abstractions. Abstractions should not depend on details. Details should depend on abstractions. References Sketches for the SOLID principles Writing code with SOLID SOLID in Golang","title":"Solid Principles"},{"location":"architecture/solid/#solid-principles","text":"SOLID is an acronym of the first five object-oriented design (OOD) principles. These principles establish practices that lend to developing software with considerations for maintaining and extending as the project grows. Adopting these practices can also contribute to avoiding code smells, refactoring code, and Agile or Adaptive software development.","title":"SOLID Principles"},{"location":"architecture/solid/#single-responsibility-principle","text":"A class should have only a single responsibility. Only one potential change in the software\u2019s specification should be able to affect the specification of the class.","title":"Single-Responsibility Principle"},{"location":"architecture/solid/#openclosed-principle","text":"Software entities should be open for EXTENSION, but closed for MODIFICATION. Allow behavior to be extended without modifying the source code.","title":"Open/Closed Principle"},{"location":"architecture/solid/#liskov-substitution-principle","text":"Objects in a program should be replaceable with instances of their subtypes without altering the correctness of that program.","title":"Liskov Substitution Principle"},{"location":"architecture/solid/#interface-segregation-principle","text":"Many client-specific interfaces are better than one general-purpose interface. No client should be forced to depend on methods it does not use.","title":"Interface Segregation Principle"},{"location":"architecture/solid/#dependency-inversion-principle","text":"One should depend upon abstractions, not concretions. High-level modules should not depend on low-level modules. Both should depend on abstractions. Abstractions should not depend on details. Details should depend on abstractions.","title":"Dependency Inversion Principle"},{"location":"architecture/solid/#references","text":"Sketches for the SOLID principles Writing code with SOLID SOLID in Golang","title":"References"},{"location":"architecture/state-pattern/","text":"State pattern The State Pattern is a behavioral design pattern that allows an object to alter its behavior when its internal state changes. The object will appear to change its class at runtime, specially when yo have multiple conditional statements that switch the object behavior based on its state. You can add new states to your system without altering existing code. Key components Context : The object that has a current state, delegating all state-specific work to this object. State : interface defining the behavior associated with a particular state of the context. Concrete State : classes that implement the State interface, defining the behavior associated with a particular state of the context. Example Let's consider a traffic light system with three states: Red , Yellow , and Green . from abc import ABC, abstractmethod # State interface class TrafficLightState(ABC): @abstractmethod def change(self): pass # Concrete states class RedLight(TrafficLightState): def change(self): return \"Yellow\" class YellowLight(TrafficLightState): def change(self): return \"Green\" class GreenLight(TrafficLightState): def change(self): return \"Red\" # Context class TrafficLight: def __init__(self): self.state = RedLight() def change(self): self.state = { RedLight: YellowLight(), YellowLight: GreenLight(), GreenLight: RedLight() }[type(self.state)] def current_state(self): return type(self.state).__name__ # Usage traffic_light = TrafficLight() print(traffic_light.current_state()) # Red traffic_light.change() print(traffic_light.current_state()) # Yellow traffic_light.change() print(traffic_light.current_state()) # Green traffic_light.change() print(traffic_light.current_state()) # Red Advantages Simplicity : It simplifies complex conditional statements by encapsulating state-specific behavior. Maintainability : Adding new states or modifying existing ones is easier without affecting other parts of the code. Readability : The code becomes more readable and self-explanatory as state transitions are clearly defined. Testing : Testing individual states becomes straightforward, enhancing the overall code quality. Use Cases Document Editing Traffic Light Vending Machine Booking System eCommerce Checkout References State Pattern in python Transitioning - python library","title":"State Pattern"},{"location":"architecture/state-pattern/#state-pattern","text":"The State Pattern is a behavioral design pattern that allows an object to alter its behavior when its internal state changes. The object will appear to change its class at runtime, specially when yo have multiple conditional statements that switch the object behavior based on its state. You can add new states to your system without altering existing code.","title":"State pattern"},{"location":"architecture/state-pattern/#key-components","text":"Context : The object that has a current state, delegating all state-specific work to this object. State : interface defining the behavior associated with a particular state of the context. Concrete State : classes that implement the State interface, defining the behavior associated with a particular state of the context.","title":"Key components"},{"location":"architecture/state-pattern/#example","text":"Let's consider a traffic light system with three states: Red , Yellow , and Green . from abc import ABC, abstractmethod # State interface class TrafficLightState(ABC): @abstractmethod def change(self): pass # Concrete states class RedLight(TrafficLightState): def change(self): return \"Yellow\" class YellowLight(TrafficLightState): def change(self): return \"Green\" class GreenLight(TrafficLightState): def change(self): return \"Red\" # Context class TrafficLight: def __init__(self): self.state = RedLight() def change(self): self.state = { RedLight: YellowLight(), YellowLight: GreenLight(), GreenLight: RedLight() }[type(self.state)] def current_state(self): return type(self.state).__name__ # Usage traffic_light = TrafficLight() print(traffic_light.current_state()) # Red traffic_light.change() print(traffic_light.current_state()) # Yellow traffic_light.change() print(traffic_light.current_state()) # Green traffic_light.change() print(traffic_light.current_state()) # Red","title":"Example"},{"location":"architecture/state-pattern/#advantages","text":"Simplicity : It simplifies complex conditional statements by encapsulating state-specific behavior. Maintainability : Adding new states or modifying existing ones is easier without affecting other parts of the code. Readability : The code becomes more readable and self-explanatory as state transitions are clearly defined. Testing : Testing individual states becomes straightforward, enhancing the overall code quality.","title":"Advantages"},{"location":"architecture/state-pattern/#use-cases","text":"Document Editing Traffic Light Vending Machine Booking System eCommerce Checkout","title":"Use Cases"},{"location":"architecture/state-pattern/#references","text":"State Pattern in python Transitioning - python library","title":"References"},{"location":"architecture/websocket/","text":"Websocket Websocket provides a communication protocol that enables bidirectional communication within the client and server over a persistent connection at the initial handshake. Let see more into details: enables bidirectional communication within the client and server: full-duplex communication, client/server can send message at any time. over a persistent connection at the initial handshake: the connection is initiated by the client through HTTP request, and the server accepts it. After the handshake, the connection is kept open until the client or server closes it. It reduces the overhead of creating a new connection for each message and lowers the latency. Use cases Most of the cases where real-time communication is needed, websocket can be the perfect solution to your problem. Here are some examples: Chats: websocket enables real-time communication between users. Online games: websocket facilitates real-time communication between players and servers. Financial applications: websocket can be used to provide real-time updates on stock prices, currency exchange rates, etc. Collaborative applications: websocket can be used to enable real-time editing of online documents, spreadsheets, etc. References Python websocket","title":"Websocket"},{"location":"architecture/websocket/#websocket","text":"Websocket provides a communication protocol that enables bidirectional communication within the client and server over a persistent connection at the initial handshake. Let see more into details: enables bidirectional communication within the client and server: full-duplex communication, client/server can send message at any time. over a persistent connection at the initial handshake: the connection is initiated by the client through HTTP request, and the server accepts it. After the handshake, the connection is kept open until the client or server closes it. It reduces the overhead of creating a new connection for each message and lowers the latency.","title":"Websocket"},{"location":"architecture/websocket/#use-cases","text":"Most of the cases where real-time communication is needed, websocket can be the perfect solution to your problem. Here are some examples: Chats: websocket enables real-time communication between users. Online games: websocket facilitates real-time communication between players and servers. Financial applications: websocket can be used to provide real-time updates on stock prices, currency exchange rates, etc. Collaborative applications: websocket can be used to enable real-time editing of online documents, spreadsheets, etc.","title":"Use cases"},{"location":"architecture/websocket/#references","text":"Python websocket","title":"References"},{"location":"devops/continuous-integration/","text":"Continuous Integration Continuous integration (CI) is an agile and DevOps best practice where developers integrate their code changes early and often to the main branch or code repository. The goal is to reduce the risk of seeing \"integration hell\" by waiting for the end of a project or a sprint to merge the work of all developers. Since it automates deployment, it helps teams meet business requirements, improve code quality, and increase security. CI is responsible of maintenance of all team. It is not the responsibility of a single person. Automated testing To get the full benefits of CI, you will need to automate your tests to be able to run them for every change that is made to the main repository. We insist on running tests on every branch of your repository and not just focus on the main branch. This way you will be able to capture issues early and minimize disruptions for your team. Different types of tests, no need to cover all at starter. Unit tests are narrow in scope and typically verify the behavior of individual methods or functions. Integration tests make sure that multiple components behave correctly together. This can involve several classes as well as testing the integration with other services. Acceptance tests are similar to the integration tests but they focus on the business cases rather than the components themselves. UI tests will make sure that the application functions correctly from a user perspective. Run your tests automatically in each commits with a jenkins pipeline. Goal is to keep build green all the time, writing tests as part of the user stories and bugs. No merge is allowed if pipeline is not green. Keep good coverage of your code, at least 80% of your code should be covered by tests. References Continuous integration","title":"Continuous Integration"},{"location":"devops/continuous-integration/#continuous-integration","text":"Continuous integration (CI) is an agile and DevOps best practice where developers integrate their code changes early and often to the main branch or code repository. The goal is to reduce the risk of seeing \"integration hell\" by waiting for the end of a project or a sprint to merge the work of all developers. Since it automates deployment, it helps teams meet business requirements, improve code quality, and increase security. CI is responsible of maintenance of all team. It is not the responsibility of a single person.","title":"Continuous Integration"},{"location":"devops/continuous-integration/#automated-testing","text":"To get the full benefits of CI, you will need to automate your tests to be able to run them for every change that is made to the main repository. We insist on running tests on every branch of your repository and not just focus on the main branch. This way you will be able to capture issues early and minimize disruptions for your team. Different types of tests, no need to cover all at starter. Unit tests are narrow in scope and typically verify the behavior of individual methods or functions. Integration tests make sure that multiple components behave correctly together. This can involve several classes as well as testing the integration with other services. Acceptance tests are similar to the integration tests but they focus on the business cases rather than the components themselves. UI tests will make sure that the application functions correctly from a user perspective. Run your tests automatically in each commits with a jenkins pipeline. Goal is to keep build green all the time, writing tests as part of the user stories and bugs. No merge is allowed if pipeline is not green. Keep good coverage of your code, at least 80% of your code should be covered by tests.","title":"Automated testing"},{"location":"devops/continuous-integration/#references","text":"Continuous integration","title":"References"},{"location":"devops/deployment-strategies/","text":"Deployment strategies How to deploy your application in production is a crucial decision. Depending on your needs, you can choose from several deployment strategies. But ideally, what you want is to deploy your application with zero downtime. This means that your application is always available to users, even when you are deploying new features or fixing bugs. Blue-green deployment In a blue-green deployment, you have two identical environments. One is the blue, which contains the current version of your application and it is serving real users. The other is green, which contains the new version deployed. Once validation is completed, you switch the router to point to the green environment and start sending user traffic to the environment. This way, you can deploy your application with zero downtime. Once the green environment is live, you can keep the blue environment as a backup, or terminate it to save costs. Canary deployment In a canary deployment, you deploy a new version of your application to a small group of users. This gives you a live environment where you can monitor and test the new app while minimizing the impact to the user base. If the canary deployment is successful, you can gradually increase the traffic to the new version until it is fully deployed. Rolling deployment In a rolling deployment, you gradually replace instances of the old version of your application with the new version. This continues until all instances are on the new version. A/B testing In A/B testing, you deploy two versions of your application to different groups of users. This allows you to compare the performance of the two versions and make data-driven decisions about which version to keep.","title":"Deployment Strategies"},{"location":"devops/deployment-strategies/#deployment-strategies","text":"How to deploy your application in production is a crucial decision. Depending on your needs, you can choose from several deployment strategies. But ideally, what you want is to deploy your application with zero downtime. This means that your application is always available to users, even when you are deploying new features or fixing bugs.","title":"Deployment strategies"},{"location":"devops/deployment-strategies/#blue-green-deployment","text":"In a blue-green deployment, you have two identical environments. One is the blue, which contains the current version of your application and it is serving real users. The other is green, which contains the new version deployed. Once validation is completed, you switch the router to point to the green environment and start sending user traffic to the environment. This way, you can deploy your application with zero downtime. Once the green environment is live, you can keep the blue environment as a backup, or terminate it to save costs.","title":"Blue-green deployment"},{"location":"devops/deployment-strategies/#canary-deployment","text":"In a canary deployment, you deploy a new version of your application to a small group of users. This gives you a live environment where you can monitor and test the new app while minimizing the impact to the user base. If the canary deployment is successful, you can gradually increase the traffic to the new version until it is fully deployed.","title":"Canary deployment"},{"location":"devops/deployment-strategies/#rolling-deployment","text":"In a rolling deployment, you gradually replace instances of the old version of your application with the new version. This continues until all instances are on the new version.","title":"Rolling deployment"},{"location":"devops/deployment-strategies/#ab-testing","text":"In A/B testing, you deploy two versions of your application to different groups of users. This allows you to compare the performance of the two versions and make data-driven decisions about which version to keep.","title":"A/B testing"},{"location":"devops/docker/","text":"Docker In a nutshell, docker executes containers, which is a customize instance of a docker image. An image is all the instructions to encapsulate libraries, dependencies, environment variables, configuration... in a file called Dockerfile to enable easy initialization of new services, without install any local dependencies that can corrupt your system. It is like a virtual machine, but with the difference of you can type out from commands line and it is not a full OS, just the necessary to run the service. As images are reusable by team or the community, they are stored in a container image registry, which is a place to store images. Docker hub is the most popular, but for self-hosted, Artifactory or quay. After image is build, you can then start multiple containers of the same image, which are independent and can run in parallel. You can start/stop/access those containers. Totally portable to any OS. References Cheatsheet Get started portainer keep bash history","title":"Docker"},{"location":"devops/docker/#docker","text":"In a nutshell, docker executes containers, which is a customize instance of a docker image. An image is all the instructions to encapsulate libraries, dependencies, environment variables, configuration... in a file called Dockerfile to enable easy initialization of new services, without install any local dependencies that can corrupt your system. It is like a virtual machine, but with the difference of you can type out from commands line and it is not a full OS, just the necessary to run the service. As images are reusable by team or the community, they are stored in a container image registry, which is a place to store images. Docker hub is the most popular, but for self-hosted, Artifactory or quay. After image is build, you can then start multiple containers of the same image, which are independent and can run in parallel. You can start/stop/access those containers. Totally portable to any OS.","title":"Docker"},{"location":"devops/docker/#references","text":"Cheatsheet Get started portainer keep bash history","title":"References"},{"location":"devops/features-toggles/","text":"Features Toggles When new functionality is working together another set of changes, it is hard to reduce the impact. For this, we can enable in our code a set of flags to enable certain parts of the code. useNewAlgorithm = False # useNewAlgorithm = True # UNCOMMENT IF YOU ARE WORKING ON THE NEW SR ALGORITHM if useNewAlgorithm: return enhancedSplineReticulation(); return oldFashionedSplineReticulation(); But this is quite hardcoding, and if you are in a collaborative environment, you will still need to deploy your code to test. For that, Toggle Router came to the picture. It could be something fancy with an UI, or much simple. But the idea is to change dynamically those values. Stored in memory, config, database... whatever suits you. Enables many other release feature and deployment strategies, like canary releases , blue/green or A/B testing . When to use features flags Improve feature rollout Operational efficiency Learn from experimentation Empower teams with entitlements References Feature Toggles Feature Flags","title":"Features Toggles"},{"location":"devops/features-toggles/#features-toggles","text":"When new functionality is working together another set of changes, it is hard to reduce the impact. For this, we can enable in our code a set of flags to enable certain parts of the code. useNewAlgorithm = False # useNewAlgorithm = True # UNCOMMENT IF YOU ARE WORKING ON THE NEW SR ALGORITHM if useNewAlgorithm: return enhancedSplineReticulation(); return oldFashionedSplineReticulation(); But this is quite hardcoding, and if you are in a collaborative environment, you will still need to deploy your code to test. For that, Toggle Router came to the picture. It could be something fancy with an UI, or much simple. But the idea is to change dynamically those values. Stored in memory, config, database... whatever suits you. Enables many other release feature and deployment strategies, like canary releases , blue/green or A/B testing .","title":"Features Toggles"},{"location":"devops/features-toggles/#when-to-use-features-flags","text":"Improve feature rollout Operational efficiency Learn from experimentation Empower teams with entitlements","title":"When to use features flags"},{"location":"devops/features-toggles/#references","text":"Feature Toggles Feature Flags","title":"References"},{"location":"devops/helm/","text":"Helm K8s packaging is a virtual concept. When we have several objects related to the same service, we can group them together in a single package. But there is no object in k8s for that. Helm manages a group of resources in a single unit. This is called a chart , which contains all the object required in the unit to make an application or service to work. Helm charts Once a chart is created, a new folder is created with all the files required for helm to work. This folder is called chart . Explanation of the files: Chart.yaml: contains the metadata for the chart. values.yaml: set of properties that can be used to configure the chart. This contains the default values. charts folder: a Chart can depend on another Chart. If this happens, chart files will be inside this folder. templates folder: contains the templates for the resources that will be created. Is where helm find the templates for all the k8s objects required for your application. May some files are autogenerated by default. If you have predefined k8s files, you can delete them and place your own k8s manifest in a folder called k8s . Usually, some tweaks to the files are required. For example, the name of the service, default values... templates/NOTES.txt: contains the information that will be shown when the chart is installed. Templates A template is an intermediate file that will be used to generate the final k8s file. Representation is with {{ }} . For example, {{ .Values.image.repository }} will check in the values.yaml file, image object, and the value of key repository . In case --set is used, it will override the value in the values.yaml file. Another objects are available to be used in the templates aside from .Values . Release (corresponding to the release data) and Chart (corresponding to the chart metadata). Update a chart template will require an update of the version in charts.yaml . Inside file _helpers.tpl there are some functions to be used in the templates. Use with {{ template \"name\" . }} . The dot is the context. You can also define your custom functions. templates/NOTES.txt can also use templates related to the service, such as how to access the service or see secrets generated by helm. Templates functions range: is used to iterate over a list. Once we are inside the loop, context is modify with the entity we are iterated for. default: in case the value is not provider or empty string, use the other value provided. printf: to concatenate strings. quote: quote a string. pipeline functions like in bash. if/else statements are available. References Helm command cheatsheet Helm best practices Helm dashboard Helm 101 Helm functions","title":"Helm Charts"},{"location":"devops/helm/#helm","text":"K8s packaging is a virtual concept. When we have several objects related to the same service, we can group them together in a single package. But there is no object in k8s for that. Helm manages a group of resources in a single unit. This is called a chart , which contains all the object required in the unit to make an application or service to work.","title":"Helm"},{"location":"devops/helm/#helm-charts","text":"Once a chart is created, a new folder is created with all the files required for helm to work. This folder is called chart . Explanation of the files: Chart.yaml: contains the metadata for the chart. values.yaml: set of properties that can be used to configure the chart. This contains the default values. charts folder: a Chart can depend on another Chart. If this happens, chart files will be inside this folder. templates folder: contains the templates for the resources that will be created. Is where helm find the templates for all the k8s objects required for your application. May some files are autogenerated by default. If you have predefined k8s files, you can delete them and place your own k8s manifest in a folder called k8s . Usually, some tweaks to the files are required. For example, the name of the service, default values... templates/NOTES.txt: contains the information that will be shown when the chart is installed.","title":"Helm charts"},{"location":"devops/helm/#templates","text":"A template is an intermediate file that will be used to generate the final k8s file. Representation is with {{ }} . For example, {{ .Values.image.repository }} will check in the values.yaml file, image object, and the value of key repository . In case --set is used, it will override the value in the values.yaml file. Another objects are available to be used in the templates aside from .Values . Release (corresponding to the release data) and Chart (corresponding to the chart metadata). Update a chart template will require an update of the version in charts.yaml . Inside file _helpers.tpl there are some functions to be used in the templates. Use with {{ template \"name\" . }} . The dot is the context. You can also define your custom functions. templates/NOTES.txt can also use templates related to the service, such as how to access the service or see secrets generated by helm.","title":"Templates"},{"location":"devops/helm/#templates-functions","text":"range: is used to iterate over a list. Once we are inside the loop, context is modify with the entity we are iterated for. default: in case the value is not provider or empty string, use the other value provided. printf: to concatenate strings. quote: quote a string. pipeline functions like in bash. if/else statements are available.","title":"Templates functions"},{"location":"devops/helm/#references","text":"Helm command cheatsheet Helm best practices Helm dashboard Helm 101 Helm functions","title":"References"},{"location":"devops/k8s/","text":"Kubernetes It is a high-disponiblity platform. It support different application available to be consume. You can use kubectl (core k8s cli tool) to give instructions to k8s. Instructions can be imperative and declarative (prefer approach). Declarative is where k8s take the lead regarding which are the actions needed to reach the desire state. Act -> observe -> diff -> Act or Desire vs Actual state k8s uses reconciliation to make the actual state looks like the desire state via control loop. Or by using imperative actions. k8s is usually the deployment tool for a microservices architecture, instead of monolithic application . Concepts Data is send to k8s through a yaml file, which is a declarative way to describe. Kind is equal to the object being described. Name must unique within the namespace for each Kind. spec information regarding this object representation. Create new or update resource with kubectl apply -f <file.yaml> Pod Can be reusable. It is a group of containers. It is the smallest unit in k8s. All containers for a pod will run in the same mode. They can talk to each other via localhost, and can share volume resources. A pod can have a container to run a web server application, where the code is located inside the image. But any time there is typo or a fix, it needs to build a new image and restart the container. This is called main container. Using side containers, we can have a shared volume between the main container to read the content from, separation od concern isolating and reusing. Each container can have different resources, like memory, cpu, etc. Services Load balancing for Pods. Use labels to determine target pods. Like a load balancer in front opf our pods. Service are not process running in k8s, is not consuming resources, is more like configuration or metadata. Deployments Deployments are an objects in k8s designed to avoid declaring the exact same pod metadata (except by the name, as it must be unique) when you need to deploy X instance of the same pod. Deployments does not create actually any pods, Instead, deployment create an object in the middle call replica sets.. Replica sets are used to make it easy to roll from one version of a deployment to another. Each time we update a deployment, a new replica set is created that contains the latest configuration. Replica set actually create the pods. Two type of strategies: RollingUpdate: value by default. new ReplicaSet is created, then scaled up as old one is scaled down. Recreate: remove all existing pods in the existing ReplicaSet before creating new ones from new ReplicaSet. Also, it is possible to configure another different deployment strategies . In a deployment, each endpoint is the IP address of a pod that is backing that service. In case of multiple replicas, we should see 1 endpoint for each pod created for a deployment. More configuration available in the official k8s documentation. StatefulSet It is an extension of deployment. It is used to manage stateful applications. Name is stable, unique pod identifiers and DNS names. Also, possible to set Persistent Storage and one Persistent Volume per VolumeClaim template. Stick to pod as long as the pod is declared. Pods created in asc order and deleted in desc order. It is possible to scale up, but all pods must be in Running or Read. Labels Characteristics are: map of key/value pairs, try to standardize both organizational and functional (selector on services) indexes and searchable avoid compound labels values Architecture Worker nodes, where the containers are running and application. They have cpu, memory to handle the application. Each of the have docker installed and kubelet , which mainly determine the url of the k8s cluster. Control planes nodes: host the components that actually run k8s. like API server. Data stored is in etcd. Scheduler, determine where to run the pods. Controller manager, watch the state of the cluster and make changes to the cluster to match the desire state. Cloud controller manager, manage the interaction with the underlying cloud provider. Networking Traffic can happen in a cluster within a pod, pod to pod, service to pods and external to cluster. within a pod: containers within a pod can connect to each other using localhost. Containers share an IP address accessible throughout the cluster and share common port space. pod to pod: each pod has a unique IP address, so they can communicate with each other. IP address are routable anywhere within the cluster. Not common procedure. service to pod: service is a resource that provides layer-4 load balancing for a group of pods. Service discovery using the cluster's internal DNS. Types are: ClusterIP (virtual IP address that load balance request to backend pods, accessible only in the cluster). NodePort: used for external facing service, exposing a port to access externally. K8s will open the port in each worker node. external to cluster: service type LoadBalancer, external IP address is provisioned by the cloud provider. Creates and manages an external load balancer, managing traffic across all nodes. Another option is Ingress . It is a layer-7 load balancer. It is a resource that balance the traffic for one or more services. Ingress rules to balance traffic to specific service. An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting. An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic. Same rules as Nginx or Traefic can be applied to Ingress, meaning that traffic to a domain or a specific path can be redirect to the corresponding labelled service. Resource organization Have multiple clusters, to have highest level of isolation. Each cluster can have different purposes, like security, environments (development vs uat vs production), by geography... Namespaces are a grouping in k8s. There is no nesting, all our objects in k8s go to same namespace. Namespaces are a way to organize and isolate resources. Names of resources must be unique within a namespace. Security and access can be set per namespaces. Usually 1 namespace per project. kubeconfig It a file that contains all sections for k8s. clusters: url of the API service. Name can be use later for the kubeconfig file. context: where cluster, user and name are glue together. Names are used to identify the context. users: credentials to use to access the cluster. Name can be use later for the kubec config file. Volume It is a way to persist data. It is a directory that is accessible to containers. Volumes are exposed at pod level. By default, containers in a pod write to ephemeral storage (emptyDir), only available during the pod's lifecycle. Once pod is terminated, data is lost. But we can attach Persistent volumes to pods which persist any data written to them. They are network attached storage, independent of pod's life. We can static or dynamic volumes. Static volumes are created manually, dynamic volumes are created automatically when a pod is created. Storage is actually outside the cluster. I your k8s manifest, you will define what are your needs with a Persist Volume Claim. Then, when the request reaches the cluster, it will be matched with a Persist Volume (binding). If there is no match, it will create a new one. Secrets and dynamic configuration All runtime configuration should be able to be injected and overrides. Defaults are ok, but we should be able to override them. Secrets are a way to store sensitive information, like passwords, tokens, keys... They are stored in etcd, encrypted at rest. They are base64 encoded, but not encrypted. We can either provide defaults value if no configuration is provided or required configuration, where startup fails if no configuration is provided. ConfigMap is the object to store configuration. It is a key/value pair. Use to store configuration data and properties. We can load all keys (envFrom.configMapRef) or individual keys (env.valueFrom.configMapKeyRef) as environment variables. Another option is to mount the config map as a mounted volume, similar all keys or individual keys (changes don\u00b4t require restart). For sensitive information, Secret resource stores this kind of data. Must be Base64 encode, and they can be exposed to pod via environment variables or mounted volumes (optional, other secret mechanism system). Network policies Specification of how groups of pods are allowed to communicate with each other or other network endpoints. Use of labels to select pods and define rules which specify what traffic is allowed to the selected pods. By default, traffic is allowed by default. A pod become isolated when defining a network policy that selects them in the namespace. Other pods within the namespace will continue to receive traffic, unless a network policy is defined for them. Workloads A Job is a resource that runs to completion, ensure it completes or retries. Based on pod, and job and pod resources will remain so logs, output can be reviewed. Manually clean up actions are required. Possible to create parallel jobs too. Use cases are batch processing, data migration, backups, etc. some process that require large amount of data to process. Also, another interesting resource is Cronjobs. Schedules one or repeated a specific time or interval. It is based on job, so it is a pod. It is a way to automate recurrent tasks. New Job resources are created for each run. By default, it will clean up Jobs, by default, keeping 3 successful and 1 failed jobs. Jobs should be idempotent. DaemonSet is a resources that ensures that all nodes run a copy of a pod. Usually used for cluster-wide logging and monitor agents. Security context To give a more restrictive access to the pod. It is a way to limit access of the users. Service account A service account provides an identity for processes that run in a Pod, and maps to a ServiceAccount object. When you authenticate to the API server, you identify yourself as a particular user. There is a service account by default. Role-based access control Once a user is authenticated, we need to authorize the user to perform actions. a Role is a collection of permissions (rules). Roles are namespace specific . We can assign rules to groups, and users belongs to groups. A more flexible way to assign rules. With ClusterRole, we can assign rules to cluster-wide resources instead to restrict to a specific namespaces. Totally reusable across entire cluster. We assign to groups and user with RoleBinding object. It is namespace specific. Get subjects and roles, and bind them together. Similar, but for entire cluster, you have the object ClusterRoleBinding. There are some built-in roles, such as cluster-admin , admin , view and edit . Local k8s cluster minikube quickly sets up a local Kubernetes cluster on macOS, Linux, and Windows. Easy to install and start working. Install and run minikube to have a local cluster for k8s in local. If minikube is running, automatically context/namespace is updated to use then one's from minikube. If you are working with other k8s cluster, keep in mind this change. Always double-check for namespace or context before deploying to the cluster. To work with cluster outside local, either you stop minikube or change namespace. Careful, when you stop minikube, context/namespace are set to null, so your kube commands won't be pointing to any context. You need to explicitly set again context. minikube command is just a proxy of kubectl commands, with a different command format but doing the same underneath. Readiness and liveness, startup probes Kubernetes simply observes the pod's lifecycle and starts to route traffic to the pod when the containers move from the Pending to Succeeded state. That means, Kubernetes mark a pod as healthy and ready to get request as soon as this happens. But application may receive traffic before is actually ready, because it needs to make database connection or load some data. There is a gap between when the app is ready and when Kubernetes thinks is ready. Risk of receive traffic and return 500 errors. Similar case happens when Kubelet watches for application crashes and restarts the pod to recover. 3 different probes: Liveness probe, determines the health, and kills the pod if it fails the liveness check (deadlock situation, since the underlying process continues to run from Kubernetes's perspective). Readiness probes are used to let kubelet know when the application is ready to accept new traffic. It runs during the pod's entire lifecycle, to deal when the application is temporarily unavailable (wait for it to recover). Startup probes are similar to readiness probes but only executed at startup. They are optimized for slow starting containers or applications with unpredictable initialization processes. 3 different probe handlers: exec: run a command inside the container. Success if return code is 0. TCPSocket: TCP check , success if port is open and accepting connections. HTTPGet: invoke HTTP GET against url, success if 2XX or 3XX code. There are many options in the probe configuration, check in the official k8s documentation. Resource management Units in k8s have a different rate conversion, and different units for CPU and Memory (can be either base 2 or 10). Resource request is the minimum amount of resources that the container needs to work. Helps k8s to schedule the pod in a more efficient way. Only used for scheduling, if the sum of the resource requests less than the capacity of the node. On the other hand, we have resource limits. It is the maximum amount of resources that a pod can get. If exceed this limits, container might be terminated. It is a way to protect against a runaway app. References K8s courses k8s concepts k9s commands minikube handbook minikube/kubectl commands kubernetes probes liveness probes tips","title":"Kubernetes"},{"location":"devops/k8s/#kubernetes","text":"It is a high-disponiblity platform. It support different application available to be consume. You can use kubectl (core k8s cli tool) to give instructions to k8s. Instructions can be imperative and declarative (prefer approach). Declarative is where k8s take the lead regarding which are the actions needed to reach the desire state. Act -> observe -> diff -> Act or Desire vs Actual state k8s uses reconciliation to make the actual state looks like the desire state via control loop. Or by using imperative actions. k8s is usually the deployment tool for a microservices architecture, instead of monolithic application .","title":"Kubernetes"},{"location":"devops/k8s/#concepts","text":"Data is send to k8s through a yaml file, which is a declarative way to describe. Kind is equal to the object being described. Name must unique within the namespace for each Kind. spec information regarding this object representation. Create new or update resource with kubectl apply -f <file.yaml>","title":"Concepts"},{"location":"devops/k8s/#pod","text":"Can be reusable. It is a group of containers. It is the smallest unit in k8s. All containers for a pod will run in the same mode. They can talk to each other via localhost, and can share volume resources. A pod can have a container to run a web server application, where the code is located inside the image. But any time there is typo or a fix, it needs to build a new image and restart the container. This is called main container. Using side containers, we can have a shared volume between the main container to read the content from, separation od concern isolating and reusing. Each container can have different resources, like memory, cpu, etc.","title":"Pod"},{"location":"devops/k8s/#services","text":"Load balancing for Pods. Use labels to determine target pods. Like a load balancer in front opf our pods. Service are not process running in k8s, is not consuming resources, is more like configuration or metadata.","title":"Services"},{"location":"devops/k8s/#deployments","text":"Deployments are an objects in k8s designed to avoid declaring the exact same pod metadata (except by the name, as it must be unique) when you need to deploy X instance of the same pod. Deployments does not create actually any pods, Instead, deployment create an object in the middle call replica sets.. Replica sets are used to make it easy to roll from one version of a deployment to another. Each time we update a deployment, a new replica set is created that contains the latest configuration. Replica set actually create the pods. Two type of strategies: RollingUpdate: value by default. new ReplicaSet is created, then scaled up as old one is scaled down. Recreate: remove all existing pods in the existing ReplicaSet before creating new ones from new ReplicaSet. Also, it is possible to configure another different deployment strategies . In a deployment, each endpoint is the IP address of a pod that is backing that service. In case of multiple replicas, we should see 1 endpoint for each pod created for a deployment. More configuration available in the official k8s documentation.","title":"Deployments"},{"location":"devops/k8s/#statefulset","text":"It is an extension of deployment. It is used to manage stateful applications. Name is stable, unique pod identifiers and DNS names. Also, possible to set Persistent Storage and one Persistent Volume per VolumeClaim template. Stick to pod as long as the pod is declared. Pods created in asc order and deleted in desc order. It is possible to scale up, but all pods must be in Running or Read.","title":"StatefulSet"},{"location":"devops/k8s/#labels","text":"Characteristics are: map of key/value pairs, try to standardize both organizational and functional (selector on services) indexes and searchable avoid compound labels values","title":"Labels"},{"location":"devops/k8s/#architecture","text":"Worker nodes, where the containers are running and application. They have cpu, memory to handle the application. Each of the have docker installed and kubelet , which mainly determine the url of the k8s cluster. Control planes nodes: host the components that actually run k8s. like API server. Data stored is in etcd. Scheduler, determine where to run the pods. Controller manager, watch the state of the cluster and make changes to the cluster to match the desire state. Cloud controller manager, manage the interaction with the underlying cloud provider.","title":"Architecture"},{"location":"devops/k8s/#networking","text":"Traffic can happen in a cluster within a pod, pod to pod, service to pods and external to cluster. within a pod: containers within a pod can connect to each other using localhost. Containers share an IP address accessible throughout the cluster and share common port space. pod to pod: each pod has a unique IP address, so they can communicate with each other. IP address are routable anywhere within the cluster. Not common procedure. service to pod: service is a resource that provides layer-4 load balancing for a group of pods. Service discovery using the cluster's internal DNS. Types are: ClusterIP (virtual IP address that load balance request to backend pods, accessible only in the cluster). NodePort: used for external facing service, exposing a port to access externally. K8s will open the port in each worker node. external to cluster: service type LoadBalancer, external IP address is provisioned by the cloud provider. Creates and manages an external load balancer, managing traffic across all nodes. Another option is Ingress . It is a layer-7 load balancer. It is a resource that balance the traffic for one or more services. Ingress rules to balance traffic to specific service. An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting. An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic. Same rules as Nginx or Traefic can be applied to Ingress, meaning that traffic to a domain or a specific path can be redirect to the corresponding labelled service.","title":"Networking"},{"location":"devops/k8s/#resource-organization","text":"Have multiple clusters, to have highest level of isolation. Each cluster can have different purposes, like security, environments (development vs uat vs production), by geography... Namespaces are a grouping in k8s. There is no nesting, all our objects in k8s go to same namespace. Namespaces are a way to organize and isolate resources. Names of resources must be unique within a namespace. Security and access can be set per namespaces. Usually 1 namespace per project.","title":"Resource organization"},{"location":"devops/k8s/#kubeconfig","text":"It a file that contains all sections for k8s. clusters: url of the API service. Name can be use later for the kubeconfig file. context: where cluster, user and name are glue together. Names are used to identify the context. users: credentials to use to access the cluster. Name can be use later for the kubec config file.","title":"kubeconfig"},{"location":"devops/k8s/#volume","text":"It is a way to persist data. It is a directory that is accessible to containers. Volumes are exposed at pod level. By default, containers in a pod write to ephemeral storage (emptyDir), only available during the pod's lifecycle. Once pod is terminated, data is lost. But we can attach Persistent volumes to pods which persist any data written to them. They are network attached storage, independent of pod's life. We can static or dynamic volumes. Static volumes are created manually, dynamic volumes are created automatically when a pod is created. Storage is actually outside the cluster. I your k8s manifest, you will define what are your needs with a Persist Volume Claim. Then, when the request reaches the cluster, it will be matched with a Persist Volume (binding). If there is no match, it will create a new one.","title":"Volume"},{"location":"devops/k8s/#secrets-and-dynamic-configuration","text":"All runtime configuration should be able to be injected and overrides. Defaults are ok, but we should be able to override them. Secrets are a way to store sensitive information, like passwords, tokens, keys... They are stored in etcd, encrypted at rest. They are base64 encoded, but not encrypted. We can either provide defaults value if no configuration is provided or required configuration, where startup fails if no configuration is provided. ConfigMap is the object to store configuration. It is a key/value pair. Use to store configuration data and properties. We can load all keys (envFrom.configMapRef) or individual keys (env.valueFrom.configMapKeyRef) as environment variables. Another option is to mount the config map as a mounted volume, similar all keys or individual keys (changes don\u00b4t require restart). For sensitive information, Secret resource stores this kind of data. Must be Base64 encode, and they can be exposed to pod via environment variables or mounted volumes (optional, other secret mechanism system).","title":"Secrets and dynamic configuration"},{"location":"devops/k8s/#network-policies","text":"Specification of how groups of pods are allowed to communicate with each other or other network endpoints. Use of labels to select pods and define rules which specify what traffic is allowed to the selected pods. By default, traffic is allowed by default. A pod become isolated when defining a network policy that selects them in the namespace. Other pods within the namespace will continue to receive traffic, unless a network policy is defined for them.","title":"Network policies"},{"location":"devops/k8s/#workloads","text":"A Job is a resource that runs to completion, ensure it completes or retries. Based on pod, and job and pod resources will remain so logs, output can be reviewed. Manually clean up actions are required. Possible to create parallel jobs too. Use cases are batch processing, data migration, backups, etc. some process that require large amount of data to process. Also, another interesting resource is Cronjobs. Schedules one or repeated a specific time or interval. It is based on job, so it is a pod. It is a way to automate recurrent tasks. New Job resources are created for each run. By default, it will clean up Jobs, by default, keeping 3 successful and 1 failed jobs. Jobs should be idempotent. DaemonSet is a resources that ensures that all nodes run a copy of a pod. Usually used for cluster-wide logging and monitor agents.","title":"Workloads"},{"location":"devops/k8s/#security-context","text":"To give a more restrictive access to the pod. It is a way to limit access of the users.","title":"Security context"},{"location":"devops/k8s/#service-account","text":"A service account provides an identity for processes that run in a Pod, and maps to a ServiceAccount object. When you authenticate to the API server, you identify yourself as a particular user. There is a service account by default.","title":"Service account"},{"location":"devops/k8s/#role-based-access-control","text":"Once a user is authenticated, we need to authorize the user to perform actions. a Role is a collection of permissions (rules). Roles are namespace specific . We can assign rules to groups, and users belongs to groups. A more flexible way to assign rules. With ClusterRole, we can assign rules to cluster-wide resources instead to restrict to a specific namespaces. Totally reusable across entire cluster. We assign to groups and user with RoleBinding object. It is namespace specific. Get subjects and roles, and bind them together. Similar, but for entire cluster, you have the object ClusterRoleBinding. There are some built-in roles, such as cluster-admin , admin , view and edit .","title":"Role-based access control"},{"location":"devops/k8s/#local-k8s-cluster","text":"minikube quickly sets up a local Kubernetes cluster on macOS, Linux, and Windows. Easy to install and start working. Install and run minikube to have a local cluster for k8s in local. If minikube is running, automatically context/namespace is updated to use then one's from minikube. If you are working with other k8s cluster, keep in mind this change. Always double-check for namespace or context before deploying to the cluster. To work with cluster outside local, either you stop minikube or change namespace. Careful, when you stop minikube, context/namespace are set to null, so your kube commands won't be pointing to any context. You need to explicitly set again context. minikube command is just a proxy of kubectl commands, with a different command format but doing the same underneath.","title":"Local k8s cluster"},{"location":"devops/k8s/#readiness-and-liveness-startup-probes","text":"Kubernetes simply observes the pod's lifecycle and starts to route traffic to the pod when the containers move from the Pending to Succeeded state. That means, Kubernetes mark a pod as healthy and ready to get request as soon as this happens. But application may receive traffic before is actually ready, because it needs to make database connection or load some data. There is a gap between when the app is ready and when Kubernetes thinks is ready. Risk of receive traffic and return 500 errors. Similar case happens when Kubelet watches for application crashes and restarts the pod to recover. 3 different probes: Liveness probe, determines the health, and kills the pod if it fails the liveness check (deadlock situation, since the underlying process continues to run from Kubernetes's perspective). Readiness probes are used to let kubelet know when the application is ready to accept new traffic. It runs during the pod's entire lifecycle, to deal when the application is temporarily unavailable (wait for it to recover). Startup probes are similar to readiness probes but only executed at startup. They are optimized for slow starting containers or applications with unpredictable initialization processes. 3 different probe handlers: exec: run a command inside the container. Success if return code is 0. TCPSocket: TCP check , success if port is open and accepting connections. HTTPGet: invoke HTTP GET against url, success if 2XX or 3XX code. There are many options in the probe configuration, check in the official k8s documentation.","title":"Readiness and liveness, startup probes"},{"location":"devops/k8s/#resource-management","text":"Units in k8s have a different rate conversion, and different units for CPU and Memory (can be either base 2 or 10). Resource request is the minimum amount of resources that the container needs to work. Helps k8s to schedule the pod in a more efficient way. Only used for scheduling, if the sum of the resource requests less than the capacity of the node. On the other hand, we have resource limits. It is the maximum amount of resources that a pod can get. If exceed this limits, container might be terminated. It is a way to protect against a runaway app.","title":"Resource management"},{"location":"devops/k8s/#references","text":"K8s courses k8s concepts k9s commands minikube handbook minikube/kubectl commands kubernetes probes liveness probes tips","title":"References"},{"location":"devops/load-shedding/","text":"Load shedding Load shedding is a technique used to reduce the load on a system by prioritizing requests and dropping the ones that are less important. This is useful when the system is under heavy load and needs to prioritize certain requests over others. Why load shedding When a system is under heavy load, it can become unresponsive and slow down their services offered. This can lead to a poor user experience and even system failures. How to implement load shedding It is important to identify which requests are more important than others in your system. It can be implemented in different ways: Api Gateway: The Api Gateway can be configured to drop requests based on certain criteria, like the number of requests per second, the type of request, etc. Service: Each service can implement its own load shedding mechanism. This can be done by using a queue to store incoming requests and processing them based on their priority. Benefits of load shedding Improved performance : By dropping less important requests, the system can focus on processing the most important ones, like payments or critical requests. Better user experience : By prioritizing certain requests, the system can ensure that users get a better experience, even when the system is under heavy load. Cost savings : If system is crashing, more and more request can come. If replication is in place, more servers will be automatically added to handle the load. By dropping less important requests, there is no need for this replication, saving costs until system is stable again. Single cluster : No need to have multiple clusters to handle different types of requests. Everything can be handled by a single cluster with load shedding inside the services. CPU base : If the system is CPU bound, it can be configured to drop requests when CPU is above a certain threshold. And start processing again when CPU is below.","title":"Load Shedding"},{"location":"devops/load-shedding/#load-shedding","text":"Load shedding is a technique used to reduce the load on a system by prioritizing requests and dropping the ones that are less important. This is useful when the system is under heavy load and needs to prioritize certain requests over others.","title":"Load shedding"},{"location":"devops/load-shedding/#why-load-shedding","text":"When a system is under heavy load, it can become unresponsive and slow down their services offered. This can lead to a poor user experience and even system failures.","title":"Why load shedding"},{"location":"devops/load-shedding/#how-to-implement-load-shedding","text":"It is important to identify which requests are more important than others in your system. It can be implemented in different ways: Api Gateway: The Api Gateway can be configured to drop requests based on certain criteria, like the number of requests per second, the type of request, etc. Service: Each service can implement its own load shedding mechanism. This can be done by using a queue to store incoming requests and processing them based on their priority.","title":"How to implement load shedding"},{"location":"devops/load-shedding/#benefits-of-load-shedding","text":"Improved performance : By dropping less important requests, the system can focus on processing the most important ones, like payments or critical requests. Better user experience : By prioritizing certain requests, the system can ensure that users get a better experience, even when the system is under heavy load. Cost savings : If system is crashing, more and more request can come. If replication is in place, more servers will be automatically added to handle the load. By dropping less important requests, there is no need for this replication, saving costs until system is stable again. Single cluster : No need to have multiple clusters to handle different types of requests. Everything can be handled by a single cluster with load shedding inside the services. CPU base : If the system is CPU bound, it can be configured to drop requests when CPU is above a certain threshold. And start processing again when CPU is below.","title":"Benefits of load shedding"},{"location":"devops/observability/","text":"Observability Server observability refers to the practice of monitoring and gaining insights into the performance, health, and behavior of servers and the applications or services running on them. It is a critical aspect of managing and maintaining server infrastructure, especially in modern, complex, and distributed computing environments. Server observability helps organizations detect and diagnose issues, optimize performance, and ensure the reliability of their IT systems. Server observability is an essential practice in modern IT operations, especially for cloud-based and containerized environments, where servers and services are highly dynamic and interconnected. Logging Collecting and analyzing logs generated by: Server Application/ Services Logs: messages logged by your server code, databases, etc. System Logs: Generated by systems-level components like the operating system, disk errors, hardware devices, etc. Network Logs: Generated by routers, load balancers, firewalls, etc. They provide information about network activity like packet drops, connection status, traffic flow and more. Logs can provide detailed information about events, errors, and transactions, helping in troubleshooting and debugging issues. There are different log system out there, so better approach is to use a tool to standardize the output of the logs, such as Fluentd , an Open source data collector. Logs are collected in a central location, where they can be displayed. A good tool for this is Kibana , with ElasticSearch for data storage. Metrics Gathering and analyzing various metrics such as CPU utilization, memory usage, disk I/O, network traffic, and application-specific performance indicators. These metrics provide a real-time view of the server's health and performance. Prometheus is an open-source systems monitoring and alerting toolkit. It comes with a query language built-in, where powerful queries can be written to gain insight into the system's behavior. Alerts regarding this metrics can be send to another system like Sentry or Nagios . For metrics visualization, Grafana is a good option. How to use/analyze metrics Don't use averages, use percentiles. Average doesn't show the real picture of the system, because it can be skewed by a few outliers. For this reason, we use percentiles, like P50, P90, P99, etc. How to get the value, just by throwing out the bottom XX % of the points and looking the first point that remains. From a list of sorted latency times [20, 37, 45, 62, 850, 920], the average is 312.3ms. But the P50 is 62ms and the P90 is 920ms . This means that 50% of the requests are below 62ms and 90% of the requests are below 920ms. Set alarms. Manual look to metrics or schedule checks is not enough, specially if the system is quite big. You need to be notified when something goes wrong.For each metric, you should set a period to measure, a threshold (limit) and a grace period (in case error recover by itself) before triggering the alarm. You can also set Working Day Alarms, alarms that indicates early warning indicators. During a working day, people can look into the issue, but if same happens during night or weekend, no need to wake up people (until the alarm is critical). Adapt limits to cycles. It's not the same traffic during the day than during the night, or in load peaks (if you are a retailer, during Black Friday you should expect a lot of traffic). You should set different threshold for different situation (hour, day of the week, season...). Not having metrics is as bad as a bad metric. If you don't have metrics, you can't know what is happening in your system. Create alarms to alert if metrics are not present. Periodically review your metrics. The system is not static, it changes over time. You should review your metrics periodically to ensure they are still relevant and useful. If this process can be automated, better. Tracing Tracing requests as they traverse through various components of a distributed system. Distributed tracing helps in identifying latency bottlenecks and understanding the flow of requests in a microservices architecture. Jaeger is an open source end-to-end distributed system to help tracing request within the system. To implement server observability effectively, organizations often use specialized tools and platforms, such as monitoring and observability solutions like Prometheus, Grafana, Elasticsearch, Kibana, and many others. These tools help in collecting, storing, analyzing, and visualizing the data needed to maintain and improve server performance and reliability. References Observability Metric","title":"Observability"},{"location":"devops/observability/#observability","text":"Server observability refers to the practice of monitoring and gaining insights into the performance, health, and behavior of servers and the applications or services running on them. It is a critical aspect of managing and maintaining server infrastructure, especially in modern, complex, and distributed computing environments. Server observability helps organizations detect and diagnose issues, optimize performance, and ensure the reliability of their IT systems. Server observability is an essential practice in modern IT operations, especially for cloud-based and containerized environments, where servers and services are highly dynamic and interconnected.","title":"Observability"},{"location":"devops/observability/#logging","text":"Collecting and analyzing logs generated by: Server Application/ Services Logs: messages logged by your server code, databases, etc. System Logs: Generated by systems-level components like the operating system, disk errors, hardware devices, etc. Network Logs: Generated by routers, load balancers, firewalls, etc. They provide information about network activity like packet drops, connection status, traffic flow and more. Logs can provide detailed information about events, errors, and transactions, helping in troubleshooting and debugging issues. There are different log system out there, so better approach is to use a tool to standardize the output of the logs, such as Fluentd , an Open source data collector. Logs are collected in a central location, where they can be displayed. A good tool for this is Kibana , with ElasticSearch for data storage.","title":"Logging"},{"location":"devops/observability/#metrics","text":"Gathering and analyzing various metrics such as CPU utilization, memory usage, disk I/O, network traffic, and application-specific performance indicators. These metrics provide a real-time view of the server's health and performance. Prometheus is an open-source systems monitoring and alerting toolkit. It comes with a query language built-in, where powerful queries can be written to gain insight into the system's behavior. Alerts regarding this metrics can be send to another system like Sentry or Nagios . For metrics visualization, Grafana is a good option.","title":"Metrics"},{"location":"devops/observability/#how-to-useanalyze-metrics","text":"Don't use averages, use percentiles. Average doesn't show the real picture of the system, because it can be skewed by a few outliers. For this reason, we use percentiles, like P50, P90, P99, etc. How to get the value, just by throwing out the bottom XX % of the points and looking the first point that remains. From a list of sorted latency times [20, 37, 45, 62, 850, 920], the average is 312.3ms. But the P50 is 62ms and the P90 is 920ms . This means that 50% of the requests are below 62ms and 90% of the requests are below 920ms. Set alarms. Manual look to metrics or schedule checks is not enough, specially if the system is quite big. You need to be notified when something goes wrong.For each metric, you should set a period to measure, a threshold (limit) and a grace period (in case error recover by itself) before triggering the alarm. You can also set Working Day Alarms, alarms that indicates early warning indicators. During a working day, people can look into the issue, but if same happens during night or weekend, no need to wake up people (until the alarm is critical). Adapt limits to cycles. It's not the same traffic during the day than during the night, or in load peaks (if you are a retailer, during Black Friday you should expect a lot of traffic). You should set different threshold for different situation (hour, day of the week, season...). Not having metrics is as bad as a bad metric. If you don't have metrics, you can't know what is happening in your system. Create alarms to alert if metrics are not present. Periodically review your metrics. The system is not static, it changes over time. You should review your metrics periodically to ensure they are still relevant and useful. If this process can be automated, better.","title":"How to use/analyze metrics"},{"location":"devops/observability/#tracing","text":"Tracing requests as they traverse through various components of a distributed system. Distributed tracing helps in identifying latency bottlenecks and understanding the flow of requests in a microservices architecture. Jaeger is an open source end-to-end distributed system to help tracing request within the system. To implement server observability effectively, organizations often use specialized tools and platforms, such as monitoring and observability solutions like Prometheus, Grafana, Elasticsearch, Kibana, and many others. These tools help in collecting, storing, analyzing, and visualizing the data needed to maintain and improve server performance and reliability.","title":"Tracing"},{"location":"devops/observability/#references","text":"Observability Metric","title":"References"},{"location":"devops/serverless/","text":"Serverless Computing Instead of run our application in our servers, this approach is focus on let others run (and charge depending on how many resource are used) our code. Servers are hard to maintain, needs to be up-to-date, always up and running. Serverless is less headache. We can forget about security, upgrade, scalability... Some providers of Functions as a Service (FaaS) are AWS, Google and Azure. They can handle out of the box most of the events, including http requests, database events, queuing services, monitoring alerts, file uploads, scheduled events, cronjobs... It is based on a microservice architecture, not monolith. There is no context shared among them. Cold start is the bigger cons. As the container is brought up alive when there is a request, there may be some latency between request and execution. Today, this time has improved a lot. It depends on language, size of the function... References Serveless","title":"Serverless Computing"},{"location":"devops/serverless/#serverless-computing","text":"Instead of run our application in our servers, this approach is focus on let others run (and charge depending on how many resource are used) our code. Servers are hard to maintain, needs to be up-to-date, always up and running. Serverless is less headache. We can forget about security, upgrade, scalability... Some providers of Functions as a Service (FaaS) are AWS, Google and Azure. They can handle out of the box most of the events, including http requests, database events, queuing services, monitoring alerts, file uploads, scheduled events, cronjobs... It is based on a microservice architecture, not monolith. There is no context shared among them. Cold start is the bigger cons. As the container is brought up alive when there is a request, there may be some latency between request and execution. Today, this time has improved a lot. It depends on language, size of the function...","title":"Serverless Computing"},{"location":"devops/serverless/#references","text":"Serveless","title":"References"},{"location":"devops/versioning/","text":"Versioning Versioning releases is important to handle dependencies with new code. Each version must be tagged with an unique identifier. Semantic versioning Semantic versioning is a standard for versioning software. It is based on three numbers separated by dots: MAJOR.MINOR.PATCH . The version number is increased when: Major - Breaking changes are introduced. Minor - New features are added in a backwards-compatible manner. Patch - Backwards-compatible bug fixes are introduced. Additional labels can be added to the version number, such as \"dev\", \"alpha\", \"beta\", \"rc1\", and so on. Usually , you can start from 0.1.0 and increment the version number as you develop the software. When you are ready to release the first stable version, you can change the version number to 1.0.0 , and follow the semantic versioning rules from there. Calendar versioning YYYY - Full year - 2006, 2016, 2106 YY - Short year - 6, 16, 106 0Y - Zero-padded year - 06, 16, 106 MM - Short month - 1, 2 ... 11, 12 0M - Zero-padded month - 01, 02 ... 11, 12 WW - Short week (since start of year) - 1, 2, 33, 52 0W - Zero-padded week - 01, 02, 33, 52 DD - Short day - 1, 2 ... 30, 31 0D - Zero-padded day - 01, 02 ... 30, 31 References Semantic versioning Calendar versioning","title":"Versioning"},{"location":"devops/versioning/#versioning","text":"Versioning releases is important to handle dependencies with new code. Each version must be tagged with an unique identifier.","title":"Versioning"},{"location":"devops/versioning/#semantic-versioning","text":"Semantic versioning is a standard for versioning software. It is based on three numbers separated by dots: MAJOR.MINOR.PATCH . The version number is increased when: Major - Breaking changes are introduced. Minor - New features are added in a backwards-compatible manner. Patch - Backwards-compatible bug fixes are introduced. Additional labels can be added to the version number, such as \"dev\", \"alpha\", \"beta\", \"rc1\", and so on. Usually , you can start from 0.1.0 and increment the version number as you develop the software. When you are ready to release the first stable version, you can change the version number to 1.0.0 , and follow the semantic versioning rules from there.","title":"Semantic versioning"},{"location":"devops/versioning/#calendar-versioning","text":"YYYY - Full year - 2006, 2016, 2106 YY - Short year - 6, 16, 106 0Y - Zero-padded year - 06, 16, 106 MM - Short month - 1, 2 ... 11, 12 0M - Zero-padded month - 01, 02 ... 11, 12 WW - Short week (since start of year) - 1, 2, 33, 52 0W - Zero-padded week - 01, 02, 33, 52 DD - Short day - 1, 2 ... 30, 31 0D - Zero-padded day - 01, 02 ... 30, 31","title":"Calendar versioning"},{"location":"devops/versioning/#references","text":"Semantic versioning Calendar versioning","title":"References"},{"location":"machine-learning/algorithms/","text":"Algorithms Two Tower Neural Networks Two Tower Neural Networks are a type of neural network architecture that generates embedding vectors for the user and for all the content to retrieve/rank. An embedding vector contains the attributes and relationships of an item. With this architecture, called \"Two Tower\" because it has two separate towers: one for the user and one for the content. With both embedding vectors in place (user vector and content vector) the model can predict the probability the user will engage with the content. This system can be put in place outside regular business hours, and the results can be cached, making the inference extremely efficient. Collaborative Filtering Collaborative filtering is a technique used by recommender systems to make predictions about the interests of a user by collecting preferences from many users (collaborating). The underlying assumption is that if a user A has the same opinion as a user B on an issue, A is more likely to have B's opinion on a different issue than that of a randomly chosen user. References Vector Embedding Collaborative Filtering","title":"Algorithms"},{"location":"machine-learning/algorithms/#algorithms","text":"","title":"Algorithms"},{"location":"machine-learning/algorithms/#two-tower-neural-networks","text":"Two Tower Neural Networks are a type of neural network architecture that generates embedding vectors for the user and for all the content to retrieve/rank. An embedding vector contains the attributes and relationships of an item. With this architecture, called \"Two Tower\" because it has two separate towers: one for the user and one for the content. With both embedding vectors in place (user vector and content vector) the model can predict the probability the user will engage with the content. This system can be put in place outside regular business hours, and the results can be cached, making the inference extremely efficient.","title":"Two Tower Neural Networks"},{"location":"machine-learning/algorithms/#collaborative-filtering","text":"Collaborative filtering is a technique used by recommender systems to make predictions about the interests of a user by collecting preferences from many users (collaborating). The underlying assumption is that if a user A has the same opinion as a user B on an issue, A is more likely to have B's opinion on a different issue than that of a randomly chosen user.","title":"Collaborative Filtering"},{"location":"machine-learning/algorithms/#references","text":"Vector Embedding Collaborative Filtering","title":"References"},{"location":"machine-learning/deep-learning/","text":"Deep Learning Deep learning is an advance machine learning technique that emulates the way the human brain learns. The key is to create an artificial neural network, which is a mathematical model that is inspired by the structure and function of biological neural networks. Neural Networks Are make of several layers of interconnected nodes, called neurons. As other machine learning models, neural networks are trained on a set of data to learn the relationship between the input data and the output and predict new values. Each layer of the network is made of nodes that are connected to the nodes in the previous layer. The first layer is called the input layer , the last layer is called the output layer , and the layers in between are called hidden layers . Training a Neural Network The algorithm used to train the model involves iteratively feeding the feature values x in the training data forward through the layers to calculate output values for \u0177 , validating the model to evaluate how far off the calculated \u0177 values are from the known y values (which quantifies the level of error, or loss, in the model), and then modifying the weights ( w ) to reduce the loss. The process for inferencing a predicted class using this network is: The feature vector observation is fed into the input layer of the neural network, which consists of a neuron for each x value. The functions for the first layer of neurons each calculate a weighted sum by combining the x value and w weight, and pass it to an activation function that determines if it meets the threshold to be passed on to the next layer. Each neuron in a layer is connected to all of the neurons in the next layer and the results of each layer are fed forward through the network until they reach the output layer. The output layer produces a vector of values; in this case, using a softmax or similar function to calculate the probability distribution for the classes. The model predicts the value with the highest probability. How does a Neural Network Learn? Weights are the key to the learning process in a neural network. Initially, weights are assigned randomly. During training, the model learns the weights that will result in the most accurate predictions. A loss function is used to compare the predicted output with the actual output (loss is the absolute difference between the predicted and actual values). With an optimization algorithm, it evaluates the influence of the weights on the loss, and adjusted the weights (up or down) to minimize the error between the predicted output and the actual output. The process of adjusting the weights is called backpropagation , replacing the previous weights with the new ones. The process is repeat until the loss is minimized and the model predicts the output with the highest accuracy.","title":"Deep Learning"},{"location":"machine-learning/deep-learning/#deep-learning","text":"Deep learning is an advance machine learning technique that emulates the way the human brain learns. The key is to create an artificial neural network, which is a mathematical model that is inspired by the structure and function of biological neural networks.","title":"Deep Learning"},{"location":"machine-learning/deep-learning/#neural-networks","text":"Are make of several layers of interconnected nodes, called neurons. As other machine learning models, neural networks are trained on a set of data to learn the relationship between the input data and the output and predict new values. Each layer of the network is made of nodes that are connected to the nodes in the previous layer. The first layer is called the input layer , the last layer is called the output layer , and the layers in between are called hidden layers .","title":"Neural Networks"},{"location":"machine-learning/deep-learning/#training-a-neural-network","text":"The algorithm used to train the model involves iteratively feeding the feature values x in the training data forward through the layers to calculate output values for \u0177 , validating the model to evaluate how far off the calculated \u0177 values are from the known y values (which quantifies the level of error, or loss, in the model), and then modifying the weights ( w ) to reduce the loss. The process for inferencing a predicted class using this network is: The feature vector observation is fed into the input layer of the neural network, which consists of a neuron for each x value. The functions for the first layer of neurons each calculate a weighted sum by combining the x value and w weight, and pass it to an activation function that determines if it meets the threshold to be passed on to the next layer. Each neuron in a layer is connected to all of the neurons in the next layer and the results of each layer are fed forward through the network until they reach the output layer. The output layer produces a vector of values; in this case, using a softmax or similar function to calculate the probability distribution for the classes. The model predicts the value with the highest probability.","title":"Training a Neural Network"},{"location":"machine-learning/deep-learning/#how-does-a-neural-network-learn","text":"Weights are the key to the learning process in a neural network. Initially, weights are assigned randomly. During training, the model learns the weights that will result in the most accurate predictions. A loss function is used to compare the predicted output with the actual output (loss is the absolute difference between the predicted and actual values). With an optimization algorithm, it evaluates the influence of the weights on the loss, and adjusted the weights (up or down) to minimize the error between the predicted output and the actual output. The process of adjusting the weights is called backpropagation , replacing the previous weights with the new ones. The process is repeat until the loss is minimized and the model predicts the output with the highest accuracy.","title":"How does a Neural Network Learn?"},{"location":"machine-learning/fundamentals/","text":"Machine Learning Machine learning is a field of computer science that uses statistical techniques to give computer systems the ability to \"learn\" (e.g., progressively improve performance on a specific task) with data, without being explicitly programmed. In other words, an algorithm that uses data from past observations to predict unknown outcomes or values. A machine learning model is a mathematical model is a software application that encapsulates a function to calculate an output value based on one or more input values. The process of defining that function is known as training . After the function has been defined, you can use it to predict new values in a process called inferencing . Process Training data : Consists of a set data from past observations. It includes the observed attributes or features ( x ) and the known values or labels ( y ). An algorithm is applied to the data to determine the relationship between the features and the labels. The result of the algorithm is a model that can be used to predict new values ( y = f(x) ). This process of training a model involves multiple iterations in which you use an appropriate algorithm to train a model, evaluate the model's predictive performance, and refine the model by repeating the training process with different algorithms and parameters until you achieve an acceptable level of predictive accuracy. Training phase is now complete, so, the trained model can be used to predict new values in a process called inferencing . As the output of the model is a prediction, it is not guaranteed to be correct. It can be represented as a \u0177 . You can compare the known labels in the validation dataset to the labels that the model predicted. Then aggregate the differences between the predicted and actual label values to calculate a metric that indicates how accurately the model predicted for the validation data. Supervised Learning Is a type of machine learning where the model is trained on a labeled dataset, we know the correct output for each example in the training dataset. The model learns to map the input to the output. Depending on the type of output, supervised learning can be divided into: Regression The output return by the model is a numeric value. You can apply linear regression as the algorithm to train the model, which works by deriving a function that produces a straight line through the intersections of the x and y values while minimizing the average distance between the line and the plotted points. You can create a visual representation of the linear regression model to see how well the model fits the data. The line that the model generates is called the regression line. The distance between the regression line and the actual data points is called the residuals. The goal of linear regression is to minimize the residuals. The smaller the residuals, the better the model fits the data. But how good is the model? You can evaluate the model's performance by using the validation dataset and compare the predicted values ( \u0177 ) with the actual values ( y ). You can plot the predicted values against the actual values to see how well the model fits the data. And also, evaluate the model's efficiency with metrics like: Mean Absolute Error (MAE): The average of the absolute errors between the predicted and actual values. Mean Squared Error (MSE): The average of the squared errors between the predicted and actual values. Root Mean Squared Error (RMSE): The square root of the average of the squared errors between the predicted and actual values. Coefficient of Determination (R\u00b2): The proportion of the variance in the validation results (R\u00b2 = 1- \u2211(y-\u0177)\u00b2 \u00f7 \u2211(y-y\u0304)\u00b2). Value between 0 and 1. The close value is to 1, the better the model is fitting the validation data. Binary classification The output is a binary value (true/false). The algorithm calculate the probability for class assignment (true/false). To train the model, there are many algorithms to use. One of the most common is logistic regression , which derives a sigmoid function that maps the input values to a probability value between 0 and 1. The sigmoid function is a mathematical function that produces an S-shaped curve. When representing the sigmoid function, the x-axis represents the input values, and the y-axis represents the probability values. The sigmoid function produces a curve that starts at 0 when x is negative infinity and ends at 1 when x is positive infinity. The curve is steepest at x = 0, where the probability is 0.5. The diagram includes an horizontal line to indicate the threshold value that separates the two classes. For any values at this point or above, the model will predict true (1); while for any values below this point it will predict false (0). To evaluate the model, you can use the validation dataset to compare the predicted values with the actual values. Usually, by creating a matrix called confusion matrix that shows the number of true positives, true negatives, false positives, and false negatives. \u0177=0 and y=0: True negatives (TN) \u0177=1 and y=0: False positives (FP) \u0177=0 and y=1: False negatives (FN) \u0177=1 and y=1: True positives (TP) With this matrix, you can calculate metrics like: Accuracy: The proportion of correct predictions to the total number of predictions: (TN+TP) \u00f7 (TN+FN+FP+TP). Recall: The proportion of true positives to the total number of actual positives: TP \u00f7 (TP+FN). Also call true positive rate (TPR). False positive rate (FPR): contrary as above metric. It is calculated as FP \u00f7(FP+TN). Precision: The proportion of true positives to the total number of predicted: TP \u00f7 (TP+FP). F1 score: The harmonic mean of precision and recall: 2 \u00d7 (Precision \u00d7 Recall) \u00f7 (Precision + Recall). Area Under the Curve (AUC): The area under the curve of the Receiver Operating Characteristic (ROC) curve. The ROC curve is a graphical representation of the true positive rate (sensitivity) against the false positive rate (1-specificity), comparing the model's performance at different threshold . The ROC curve for a perfect model would go straight up the TPR axis on the left and then across the FPR axis at the top. Multi-class classification The output predicted is a value from a set of possible values. It follows the same process as binary classification, but the output is a value from a set of possible values. To train a model for multi-class classification, you can use an algorithm to fit the training data to a function that calculates a probability value for each possible class. There are two kinds of algorithm you can use to do this: One-vs-Rest (OvR) algorithms: trains a function for each class, predicting the probability of that class against all the other classes. Each algorithm produces a sigmoid function that maps the input values to a probability value between 0 and 1. And the model trained using this algorithm will predict the output class with the highest probability. Multinomial algorithms: creates a single functions that returns a multi-class probability distribution. The output of the model is a vector of probabilities that contains the probability distribution for all classes. No matter the algorithm you choose, the the model returns the most probable class. You can evaluate the model's performance by using similar metrics as binary classification, but you can use a confusion matrix with more rows and columns (one for each class). Same metrics as binary classification can be used to evaluate the model's. You can calculate the accuracy, recall, precision, F1 score, and AUC for each class. Additionally, you can calculate the overall values for the entire table. Unsupervised Learning Is a type of machine learning where the model is trained on an unlabeled dataset . The model learns to identify patterns in the data and group similar data together. Clustering is a common unsupervised learning technique that groups similar data into discrete clusters. One algorithm you can use to train a model for clustering is the K-means , consisting in the following steps: The feature x values are vectors in a n-dimensional space, where n is the number of features. Define the number of clusters (k) you want to group. Then, k points are randomly selected as the center of the cluster, as the initial centroids. Each point is assigned to the nearest centroid, creating k clusters. Each centroid is recalculated as the mean of all the points in the cluster. After the centroids are recalculated, the points are reassigned to the nearest centroid. The process is repeated until the centroids no longer change, or the maximum number of iterations is reached. Since there is no labeled data, you can't evaluate the model's performance using the same metrics as supervised learning. Instead, you can use the following metrics: Average distance to cluster center: on average, how close is each point in the cluster is to the centroid of the cluster. Average distance to other center: on average, how close is each point in the cluster is to the centroid of all other clusters. Maximum distance to cluster center: the furthest distance between a point in the cluster and its centroid. Silhouette: A value between -1 and 1 that summarizes the ratio of distance between points in the same cluster and points in different clusters (The closer to 1, the better the cluster separation). References Machine Learning Fundamentals","title":"Fundamentals"},{"location":"machine-learning/fundamentals/#machine-learning","text":"Machine learning is a field of computer science that uses statistical techniques to give computer systems the ability to \"learn\" (e.g., progressively improve performance on a specific task) with data, without being explicitly programmed. In other words, an algorithm that uses data from past observations to predict unknown outcomes or values. A machine learning model is a mathematical model is a software application that encapsulates a function to calculate an output value based on one or more input values. The process of defining that function is known as training . After the function has been defined, you can use it to predict new values in a process called inferencing .","title":"Machine Learning"},{"location":"machine-learning/fundamentals/#process","text":"Training data : Consists of a set data from past observations. It includes the observed attributes or features ( x ) and the known values or labels ( y ). An algorithm is applied to the data to determine the relationship between the features and the labels. The result of the algorithm is a model that can be used to predict new values ( y = f(x) ). This process of training a model involves multiple iterations in which you use an appropriate algorithm to train a model, evaluate the model's predictive performance, and refine the model by repeating the training process with different algorithms and parameters until you achieve an acceptable level of predictive accuracy. Training phase is now complete, so, the trained model can be used to predict new values in a process called inferencing . As the output of the model is a prediction, it is not guaranteed to be correct. It can be represented as a \u0177 . You can compare the known labels in the validation dataset to the labels that the model predicted. Then aggregate the differences between the predicted and actual label values to calculate a metric that indicates how accurately the model predicted for the validation data.","title":"Process"},{"location":"machine-learning/fundamentals/#supervised-learning","text":"Is a type of machine learning where the model is trained on a labeled dataset, we know the correct output for each example in the training dataset. The model learns to map the input to the output. Depending on the type of output, supervised learning can be divided into:","title":"Supervised Learning"},{"location":"machine-learning/fundamentals/#regression","text":"The output return by the model is a numeric value. You can apply linear regression as the algorithm to train the model, which works by deriving a function that produces a straight line through the intersections of the x and y values while minimizing the average distance between the line and the plotted points. You can create a visual representation of the linear regression model to see how well the model fits the data. The line that the model generates is called the regression line. The distance between the regression line and the actual data points is called the residuals. The goal of linear regression is to minimize the residuals. The smaller the residuals, the better the model fits the data. But how good is the model? You can evaluate the model's performance by using the validation dataset and compare the predicted values ( \u0177 ) with the actual values ( y ). You can plot the predicted values against the actual values to see how well the model fits the data. And also, evaluate the model's efficiency with metrics like: Mean Absolute Error (MAE): The average of the absolute errors between the predicted and actual values. Mean Squared Error (MSE): The average of the squared errors between the predicted and actual values. Root Mean Squared Error (RMSE): The square root of the average of the squared errors between the predicted and actual values. Coefficient of Determination (R\u00b2): The proportion of the variance in the validation results (R\u00b2 = 1- \u2211(y-\u0177)\u00b2 \u00f7 \u2211(y-y\u0304)\u00b2). Value between 0 and 1. The close value is to 1, the better the model is fitting the validation data.","title":"Regression"},{"location":"machine-learning/fundamentals/#binary-classification","text":"The output is a binary value (true/false). The algorithm calculate the probability for class assignment (true/false). To train the model, there are many algorithms to use. One of the most common is logistic regression , which derives a sigmoid function that maps the input values to a probability value between 0 and 1. The sigmoid function is a mathematical function that produces an S-shaped curve. When representing the sigmoid function, the x-axis represents the input values, and the y-axis represents the probability values. The sigmoid function produces a curve that starts at 0 when x is negative infinity and ends at 1 when x is positive infinity. The curve is steepest at x = 0, where the probability is 0.5. The diagram includes an horizontal line to indicate the threshold value that separates the two classes. For any values at this point or above, the model will predict true (1); while for any values below this point it will predict false (0). To evaluate the model, you can use the validation dataset to compare the predicted values with the actual values. Usually, by creating a matrix called confusion matrix that shows the number of true positives, true negatives, false positives, and false negatives. \u0177=0 and y=0: True negatives (TN) \u0177=1 and y=0: False positives (FP) \u0177=0 and y=1: False negatives (FN) \u0177=1 and y=1: True positives (TP) With this matrix, you can calculate metrics like: Accuracy: The proportion of correct predictions to the total number of predictions: (TN+TP) \u00f7 (TN+FN+FP+TP). Recall: The proportion of true positives to the total number of actual positives: TP \u00f7 (TP+FN). Also call true positive rate (TPR). False positive rate (FPR): contrary as above metric. It is calculated as FP \u00f7(FP+TN). Precision: The proportion of true positives to the total number of predicted: TP \u00f7 (TP+FP). F1 score: The harmonic mean of precision and recall: 2 \u00d7 (Precision \u00d7 Recall) \u00f7 (Precision + Recall). Area Under the Curve (AUC): The area under the curve of the Receiver Operating Characteristic (ROC) curve. The ROC curve is a graphical representation of the true positive rate (sensitivity) against the false positive rate (1-specificity), comparing the model's performance at different threshold . The ROC curve for a perfect model would go straight up the TPR axis on the left and then across the FPR axis at the top.","title":"Binary classification"},{"location":"machine-learning/fundamentals/#multi-class-classification","text":"The output predicted is a value from a set of possible values. It follows the same process as binary classification, but the output is a value from a set of possible values. To train a model for multi-class classification, you can use an algorithm to fit the training data to a function that calculates a probability value for each possible class. There are two kinds of algorithm you can use to do this: One-vs-Rest (OvR) algorithms: trains a function for each class, predicting the probability of that class against all the other classes. Each algorithm produces a sigmoid function that maps the input values to a probability value between 0 and 1. And the model trained using this algorithm will predict the output class with the highest probability. Multinomial algorithms: creates a single functions that returns a multi-class probability distribution. The output of the model is a vector of probabilities that contains the probability distribution for all classes. No matter the algorithm you choose, the the model returns the most probable class. You can evaluate the model's performance by using similar metrics as binary classification, but you can use a confusion matrix with more rows and columns (one for each class). Same metrics as binary classification can be used to evaluate the model's. You can calculate the accuracy, recall, precision, F1 score, and AUC for each class. Additionally, you can calculate the overall values for the entire table.","title":"Multi-class classification"},{"location":"machine-learning/fundamentals/#unsupervised-learning","text":"Is a type of machine learning where the model is trained on an unlabeled dataset . The model learns to identify patterns in the data and group similar data together. Clustering is a common unsupervised learning technique that groups similar data into discrete clusters. One algorithm you can use to train a model for clustering is the K-means , consisting in the following steps: The feature x values are vectors in a n-dimensional space, where n is the number of features. Define the number of clusters (k) you want to group. Then, k points are randomly selected as the center of the cluster, as the initial centroids. Each point is assigned to the nearest centroid, creating k clusters. Each centroid is recalculated as the mean of all the points in the cluster. After the centroids are recalculated, the points are reassigned to the nearest centroid. The process is repeated until the centroids no longer change, or the maximum number of iterations is reached. Since there is no labeled data, you can't evaluate the model's performance using the same metrics as supervised learning. Instead, you can use the following metrics: Average distance to cluster center: on average, how close is each point in the cluster is to the centroid of the cluster. Average distance to other center: on average, how close is each point in the cluster is to the centroid of all other clusters. Maximum distance to cluster center: the furthest distance between a point in the cluster and its centroid. Silhouette: A value between -1 and 1 that summarizes the ratio of distance between points in the same cluster and points in different clusters (The closer to 1, the better the cluster separation).","title":"Unsupervised Learning"},{"location":"machine-learning/fundamentals/#references","text":"Machine Learning Fundamentals","title":"References"},{"location":"machine-learning/input-data/","text":"Input data In order to train a machine learning model, you need to provide it with input data. Usually, data is divided into sets: training data: set of examples used to fit the parameters of the model. validation data: set used to provide an evaluation of a model fit on the training dataset while tuning model parameters. testing data: set used to provide an evaluation of a final model fit on the training dataset. It is important to split the data into these three sets to ensure that the model is able to generalize well to unseen data. Be aware of overfitting: when a model learns the detail and noise in the training, it can be less flexible when it comes to new data. The data collected, even if is coming from different sources must be consistent and in the same format. The data must be cleaned and preprocessed before training the model. This includes removing duplicates, handling missing values, normalizing the data, and transforming categorical variables into numerical features (most models work better with numerical features). References Parquet .","title":"Input Data"},{"location":"machine-learning/input-data/#input-data","text":"In order to train a machine learning model, you need to provide it with input data. Usually, data is divided into sets: training data: set of examples used to fit the parameters of the model. validation data: set used to provide an evaluation of a model fit on the training dataset while tuning model parameters. testing data: set used to provide an evaluation of a final model fit on the training dataset. It is important to split the data into these three sets to ensure that the model is able to generalize well to unseen data. Be aware of overfitting: when a model learns the detail and noise in the training, it can be less flexible when it comes to new data. The data collected, even if is coming from different sources must be consistent and in the same format. The data must be cleaned and preprocessed before training the model. This includes removing duplicates, handling missing values, normalizing the data, and transforming categorical variables into numerical features (most models work better with numerical features).","title":"Input data"},{"location":"machine-learning/input-data/#references","text":"Parquet .","title":"References"},{"location":"machine-learning/responsibilities/","text":"MLE Responsibilities The responsibilities of a machine learning engineer (MLE) can vary depending on the organization and the specific role. See below a list of the most frequent responsibilities of a MLE. Data collection The data collection is the first step in the machine learning process. Where and how is the data coming from, how is it collected, and how is it stored. It is important to understand the data, the raw material. First phase is to analyze the data, confirm data is heterogeneous, diverse and with quality. The data can come from many different sources, such as databases, APIs, or events. Data preprocessing Cleaning, transforming, and preparing the data for training the model. Optimizing the data for the model, by removing duplicates, handling missing values, coherence and normalize the data. Registry how the data is cleaned and transformed, so that it can be reproduced and used in the future. This can include things like data validation, transformation... Dataset versioning is important, to keep track of the different versions of the data and how they are used in the model. Tools like MLFlow can help with this. Feature engineering Select the features that are going to be used in the model. It is the process of using domain knowledge to extract features from the data. This can involve selecting, modifying, or creating new features that will help the model learn better. Things like the importance of the features, how they are going to be used in the model, defining the weights of the features when making the inference. Also, categorizing the data, so that the model can process it correctly (models works better with numbers rather than strings). Again, registry and versioning the features is vital that are going to be used by the model, so that they can be reproduced and used in the future. hyperparameters are parameters set before the training process begins and are not learned from the data. Model training Same logic that applies to the training has to be applied to the inference in production. It must be the same distribution of data. Classic models are deterministic. The model is not going to work the same way if the data is not the same. Other models are probabilistic, like neural networks. The model is going to work the same way, but the predictions are going to be different. A model cannot do predictions of what it has not seen. Some models/algorithms are: decision trees random forest linear regression neural networks More common frameworks for training the models are: scikit-learn tensorflow pytorch Model evaluation Evaluate the model to see how well it is performing. This can include metrics and other things like: accuracy precision recall F1 score auc confusion matrix The interpretability on ML models is the ability to understand how a complex model takes their decisions. LIME: Local Interpretable Model-Agnostic Explanations is a technique that provides explanations at the individual level about the predictions of any machine learning classifier, helping to understand which features influence a specific prediction. SHAP: SHapley Additive exPlanations is a unified approach to explain the output of any machine learning model. It connects game theory with local explanations, providing a way to understand the contribution of each feature to the final prediction. Production/model deployment Not only the model, but the whole process of deploying the solution in production. How the inference is going to be done, where the model is going to be stored, how to registry the inference, how is exposed the model (api, events...) Model packaging is the process of taking the trained model and preparing it for deployment. Some common formats for model packaging include: pickle (is just a serialization format, it doesn't have to be a model only, it can be anything.) onnx torchscript Inference is the process of using the trained model to make predictions on new data. This can be done in real-time or in batch mode, depending on the requirements of the application. Registry each inference, so it can be measured and evaluated. This can include things like logging the input data, the predictions, and the performance of the model. Again, versioning is important, to keep track of the different versions of the model and how they are used in the application. Some common tools for model versioning are: MLFlow, BentoML, TorchServe... There are different ways to deploy the model: blue/green canary shadow deployment: both models are deployed in production, but results are not exposed to users, just to get and analyze metrics. A/B testing Feedback loop Feedback loop is the way to improve the model in production, with new data. It is the process to evaluate the model, seeing if the predictions are correct or not. THe retraining/feedback loop can be done in different ways: online batch incremental Observability As we discussed in the previous sections, registry is key to be able to evaluate the model. With all the monitoring and observability, we can see how the model is performing in production. Over the time, models can degrade, and the performance can be affected. This is important to monitor, so that we can take action and retrain the model if necessary. With the registry predictions of the model yo are able to evaluate the model at any time and prevent model drift. data drift: when the input data changes or the distribution of the data changes. concept drift: when it is a change between the input and output data. model drift: degradation of the model over time. covariate shift: when properties of the input data change between the training and testing phases of a model. Having this in mind, add alerts, dashboards, and other tools to monitor the model in production to prevent this from happening. References Machine learning community","title":"Responsibilities"},{"location":"machine-learning/responsibilities/#mle-responsibilities","text":"The responsibilities of a machine learning engineer (MLE) can vary depending on the organization and the specific role. See below a list of the most frequent responsibilities of a MLE.","title":"MLE Responsibilities"},{"location":"machine-learning/responsibilities/#data-collection","text":"The data collection is the first step in the machine learning process. Where and how is the data coming from, how is it collected, and how is it stored. It is important to understand the data, the raw material. First phase is to analyze the data, confirm data is heterogeneous, diverse and with quality. The data can come from many different sources, such as databases, APIs, or events.","title":"Data collection"},{"location":"machine-learning/responsibilities/#data-preprocessing","text":"Cleaning, transforming, and preparing the data for training the model. Optimizing the data for the model, by removing duplicates, handling missing values, coherence and normalize the data. Registry how the data is cleaned and transformed, so that it can be reproduced and used in the future. This can include things like data validation, transformation... Dataset versioning is important, to keep track of the different versions of the data and how they are used in the model. Tools like MLFlow can help with this.","title":"Data preprocessing"},{"location":"machine-learning/responsibilities/#feature-engineering","text":"Select the features that are going to be used in the model. It is the process of using domain knowledge to extract features from the data. This can involve selecting, modifying, or creating new features that will help the model learn better. Things like the importance of the features, how they are going to be used in the model, defining the weights of the features when making the inference. Also, categorizing the data, so that the model can process it correctly (models works better with numbers rather than strings). Again, registry and versioning the features is vital that are going to be used by the model, so that they can be reproduced and used in the future. hyperparameters are parameters set before the training process begins and are not learned from the data.","title":"Feature engineering"},{"location":"machine-learning/responsibilities/#model-training","text":"Same logic that applies to the training has to be applied to the inference in production. It must be the same distribution of data. Classic models are deterministic. The model is not going to work the same way if the data is not the same. Other models are probabilistic, like neural networks. The model is going to work the same way, but the predictions are going to be different. A model cannot do predictions of what it has not seen. Some models/algorithms are: decision trees random forest linear regression neural networks More common frameworks for training the models are: scikit-learn tensorflow pytorch","title":"Model training"},{"location":"machine-learning/responsibilities/#model-evaluation","text":"Evaluate the model to see how well it is performing. This can include metrics and other things like: accuracy precision recall F1 score auc confusion matrix The interpretability on ML models is the ability to understand how a complex model takes their decisions. LIME: Local Interpretable Model-Agnostic Explanations is a technique that provides explanations at the individual level about the predictions of any machine learning classifier, helping to understand which features influence a specific prediction. SHAP: SHapley Additive exPlanations is a unified approach to explain the output of any machine learning model. It connects game theory with local explanations, providing a way to understand the contribution of each feature to the final prediction.","title":"Model evaluation"},{"location":"machine-learning/responsibilities/#productionmodel-deployment","text":"Not only the model, but the whole process of deploying the solution in production. How the inference is going to be done, where the model is going to be stored, how to registry the inference, how is exposed the model (api, events...) Model packaging is the process of taking the trained model and preparing it for deployment. Some common formats for model packaging include: pickle (is just a serialization format, it doesn't have to be a model only, it can be anything.) onnx torchscript Inference is the process of using the trained model to make predictions on new data. This can be done in real-time or in batch mode, depending on the requirements of the application. Registry each inference, so it can be measured and evaluated. This can include things like logging the input data, the predictions, and the performance of the model. Again, versioning is important, to keep track of the different versions of the model and how they are used in the application. Some common tools for model versioning are: MLFlow, BentoML, TorchServe... There are different ways to deploy the model: blue/green canary shadow deployment: both models are deployed in production, but results are not exposed to users, just to get and analyze metrics. A/B testing","title":"Production/model deployment"},{"location":"machine-learning/responsibilities/#feedback-loop","text":"Feedback loop is the way to improve the model in production, with new data. It is the process to evaluate the model, seeing if the predictions are correct or not. THe retraining/feedback loop can be done in different ways: online batch incremental","title":"Feedback loop"},{"location":"machine-learning/responsibilities/#observability","text":"As we discussed in the previous sections, registry is key to be able to evaluate the model. With all the monitoring and observability, we can see how the model is performing in production. Over the time, models can degrade, and the performance can be affected. This is important to monitor, so that we can take action and retrain the model if necessary. With the registry predictions of the model yo are able to evaluate the model at any time and prevent model drift. data drift: when the input data changes or the distribution of the data changes. concept drift: when it is a change between the input and output data. model drift: degradation of the model over time. covariate shift: when properties of the input data change between the training and testing phases of a model. Having this in mind, add alerts, dashboards, and other tools to monitor the model in production to prevent this from happening.","title":"Observability"},{"location":"machine-learning/responsibilities/#references","text":"Machine learning community","title":"References"},{"location":"programming/algorithm-complexity/","text":"Algorithm complexity Algorithm complexity is how we can measure the resource an algorithm uses to executes. Time complexity How much time takes an algorithm to complete its execution. Constant time - O(1) Execution time does not depend on the size of the input data. Access an element in an array by index. Insert/delete an element at the end of an array. Math operations. Logarithmic time - O(log n) Execution times decrease by half with each iteration, as algorithm eliminates half of the input data at each step. Binary search in a sorted array. Linear time - O(n) Execution time increases linearly with the size of the input data. (in the worst case, we need to check every item in the input data). Iterate through an array. Find an item in an unsorted array. Count frequency of items in an array. Log-linear time - O(n log n) Algorithms that divide the input data into smaller parts and then combine the results. Merge sort. Quick sort. Quadratic time - O(n^2) Execution time increases quadratically with the size of the input data (if data is double, execution time is quadrupled). Nested loops iterating through the same input data. Bubble sort. Selection sort. Exponential time - O(2^n) Execution time doubles with each additional item in the input data. O(n!) Execution time increases factorially with the size of the input data. Calculating permutations of a set of items. Space complexity How much additional memory an algorithm uses to complete its execution. Constant space - O(1) Execution space does not depend on the size of the input data. Using a fixed number of variables. Linear space - O(log n) Execution space increases slowly with the size of the input data. Linear space - O(n) Execution space increases linearly with the size of the input data. Copying an array. Using a dict/set to store all input data. Quadratic space - O(n^2) Execution space increases quadratically with the size of the input data. Creating a 2D array to store results of operations on pairs of items. Big O notation When we talk about algorithm complexity, we use Big O notation to describe the worst-case scenario of an algorithm's performance. It offers a high-level understanding of how an algorithm behaves as the input size grows.","title":"Algorithm Complexity"},{"location":"programming/algorithm-complexity/#algorithm-complexity","text":"Algorithm complexity is how we can measure the resource an algorithm uses to executes.","title":"Algorithm complexity"},{"location":"programming/algorithm-complexity/#time-complexity","text":"How much time takes an algorithm to complete its execution.","title":"Time complexity"},{"location":"programming/algorithm-complexity/#constant-time-o1","text":"Execution time does not depend on the size of the input data. Access an element in an array by index. Insert/delete an element at the end of an array. Math operations.","title":"Constant time - O(1)"},{"location":"programming/algorithm-complexity/#logarithmic-time-olog-n","text":"Execution times decrease by half with each iteration, as algorithm eliminates half of the input data at each step. Binary search in a sorted array.","title":"Logarithmic time - O(log n)"},{"location":"programming/algorithm-complexity/#linear-time-on","text":"Execution time increases linearly with the size of the input data. (in the worst case, we need to check every item in the input data). Iterate through an array. Find an item in an unsorted array. Count frequency of items in an array.","title":"Linear time - O(n)"},{"location":"programming/algorithm-complexity/#log-linear-time-on-log-n","text":"Algorithms that divide the input data into smaller parts and then combine the results. Merge sort. Quick sort.","title":"Log-linear time - O(n log n)"},{"location":"programming/algorithm-complexity/#quadratic-time-on2","text":"Execution time increases quadratically with the size of the input data (if data is double, execution time is quadrupled). Nested loops iterating through the same input data. Bubble sort. Selection sort.","title":"Quadratic time - O(n^2)"},{"location":"programming/algorithm-complexity/#exponential-time-o2n","text":"Execution time doubles with each additional item in the input data.","title":"Exponential time - O(2^n)"},{"location":"programming/algorithm-complexity/#on","text":"Execution time increases factorially with the size of the input data. Calculating permutations of a set of items.","title":"O(n!)"},{"location":"programming/algorithm-complexity/#space-complexity","text":"How much additional memory an algorithm uses to complete its execution.","title":"Space complexity"},{"location":"programming/algorithm-complexity/#constant-space-o1","text":"Execution space does not depend on the size of the input data. Using a fixed number of variables.","title":"Constant space - O(1)"},{"location":"programming/algorithm-complexity/#linear-space-olog-n","text":"Execution space increases slowly with the size of the input data.","title":"Linear space - O(log n)"},{"location":"programming/algorithm-complexity/#linear-space-on","text":"Execution space increases linearly with the size of the input data. Copying an array. Using a dict/set to store all input data.","title":"Linear space - O(n)"},{"location":"programming/algorithm-complexity/#quadratic-space-on2","text":"Execution space increases quadratically with the size of the input data. Creating a 2D array to store results of operations on pairs of items.","title":"Quadratic space - O(n^2)"},{"location":"programming/algorithm-complexity/#big-o-notation","text":"When we talk about algorithm complexity, we use Big O notation to describe the worst-case scenario of an algorithm's performance. It offers a high-level understanding of how an algorithm behaves as the input size grows.","title":"Big O notation"},{"location":"programming/conventional-commits/","text":"Conventional Commits Conventional Commits is a generic convention of writing commit messages. It provides a set of simple rules for creating new commits, easier to automate tools on top, such as changelog or semantic versioning. Goal of this convention is to make commit messages more readable for developers or other stakeholder, consistent among developers and easier to automate. References Conventional commits","title":"Conventional Commits"},{"location":"programming/conventional-commits/#conventional-commits","text":"Conventional Commits is a generic convention of writing commit messages. It provides a set of simple rules for creating new commits, easier to automate tools on top, such as changelog or semantic versioning. Goal of this convention is to make commit messages more readable for developers or other stakeholder, consistent among developers and easier to automate.","title":"Conventional Commits"},{"location":"programming/conventional-commits/#references","text":"Conventional commits","title":"References"},{"location":"programming/development-workflows/","text":"Development Workflows Version control system is a must for any serious software development. It allows you to track changes in your code, collaborate with other developers and easily revert changes if needed. The most popular system is git. It is a distributed version control system, which means that every developer has a full copy of the repository on his computer. This allows you to work offline and commit changes locally. When you are ready to share your changes with other developers, you push your commits to the remote repository. To allow work in parallel, there are two different workflows. git flow There is one main development branch, usually called develop . All new features are created for this branch, and several commits can be made. Once they are ready, a pull request is created towards develop . Once the pull request is reviewed by other developers, it is merged into develop . Then, a new release is created, ready to be deployed to production, and later to master branch, with a tag versioning of the release. Ideally, use this workflow when there are a lot of junior developers, or new joiners. ALso, then product is stable and any changes can affect heavily in daily operations by the final user. In the order hand, can cause bottle neck if the product is starting up and need top be iterated quickly. Trunk-based All developers work in the same branch, usually master . When a new feature is needed, a new branch is created from master . Once the feature is ready, it is merged into master . This workflow is ideal for small teams, where there is a lot of trust between developers, and the product is still in development. It allows to iterate quickly, and to have a fast feedback loop. It allows continuous integration, so when build and tests pass, the code is deployed to production every single commit. Smaller commits allow faster code revision, and easier to revert changes if something wrong happens. Increases confidences in developers. Combine with feature flags , where code can be in production but inactive to the final user. Bigger features are not separated for long time to master branch, reducing the risk of conflicts when merging. Almost daily merges to master are done, increasing agile release with CI/CD. References Git flow workflow Trunk-based workflow","title":"Development Workflows"},{"location":"programming/development-workflows/#development-workflows","text":"Version control system is a must for any serious software development. It allows you to track changes in your code, collaborate with other developers and easily revert changes if needed. The most popular system is git. It is a distributed version control system, which means that every developer has a full copy of the repository on his computer. This allows you to work offline and commit changes locally. When you are ready to share your changes with other developers, you push your commits to the remote repository. To allow work in parallel, there are two different workflows.","title":"Development Workflows"},{"location":"programming/development-workflows/#git-flow","text":"There is one main development branch, usually called develop . All new features are created for this branch, and several commits can be made. Once they are ready, a pull request is created towards develop . Once the pull request is reviewed by other developers, it is merged into develop . Then, a new release is created, ready to be deployed to production, and later to master branch, with a tag versioning of the release. Ideally, use this workflow when there are a lot of junior developers, or new joiners. ALso, then product is stable and any changes can affect heavily in daily operations by the final user. In the order hand, can cause bottle neck if the product is starting up and need top be iterated quickly.","title":"git flow"},{"location":"programming/development-workflows/#trunk-based","text":"All developers work in the same branch, usually master . When a new feature is needed, a new branch is created from master . Once the feature is ready, it is merged into master . This workflow is ideal for small teams, where there is a lot of trust between developers, and the product is still in development. It allows to iterate quickly, and to have a fast feedback loop. It allows continuous integration, so when build and tests pass, the code is deployed to production every single commit. Smaller commits allow faster code revision, and easier to revert changes if something wrong happens. Increases confidences in developers. Combine with feature flags , where code can be in production but inactive to the final user. Bigger features are not separated for long time to master branch, reducing the risk of conflicts when merging. Almost daily merges to master are done, increasing agile release with CI/CD.","title":"Trunk-based"},{"location":"programming/development-workflows/#references","text":"Git flow workflow Trunk-based workflow","title":"References"},{"location":"programming/django/","text":"Django Installation to install django, lets create a requirements.txt file, with desired version of django. django==1.11.13 then, install libraries in file: pip install -r requirements.txt check everything went ok: django-admin -h Starting new project First, on Django, you'll need to create a project by: django-admin startproject <project> that's basic configuration. To check everything looks good, run: python manage.py runserver if we want to test via shell, run: python manage.py shell basic schema of django app A project can contains one or more apps. Also, an app can be used in several projects. To create a new app in our project: python manage.py startapp <app> New folder is created. It contains basic schema: <app>/views.py represents the controller. One function per view. Get data, processed it, and render template if needed. <app>/urls.py where url is mapped to corresponding function in views file <app>/models.py represent the models. Django use an ORM, that means, tables and database corresponds to Model classes. Add app configuration to project/settings.py : INSTALLED_APPS = [ '<app>.apps.<App>Config', Migrations Every changes on models, needs to be migrate to database schema. After add/update information to models.py file, changes needs to be done on database level Make migrations: python manage.py makemigrations <app> See migration on sql: python manage.py sqlmigrate <app> <number> Run migrations: python manage.py migrate django admin - basic commands Creating an admin user: python manage.py createsuperuser Tests needs to be created. Run with: python manage.py test <app>","title":"Django"},{"location":"programming/django/#django","text":"","title":"Django"},{"location":"programming/django/#installation","text":"to install django, lets create a requirements.txt file, with desired version of django. django==1.11.13 then, install libraries in file: pip install -r requirements.txt check everything went ok: django-admin -h","title":"Installation"},{"location":"programming/django/#starting-new-project","text":"First, on Django, you'll need to create a project by: django-admin startproject <project> that's basic configuration. To check everything looks good, run: python manage.py runserver if we want to test via shell, run: python manage.py shell","title":"Starting new project"},{"location":"programming/django/#basic-schema-of-django-app","text":"A project can contains one or more apps. Also, an app can be used in several projects. To create a new app in our project: python manage.py startapp <app> New folder is created. It contains basic schema: <app>/views.py represents the controller. One function per view. Get data, processed it, and render template if needed. <app>/urls.py where url is mapped to corresponding function in views file <app>/models.py represent the models. Django use an ORM, that means, tables and database corresponds to Model classes. Add app configuration to project/settings.py : INSTALLED_APPS = [ '<app>.apps.<App>Config',","title":"basic schema of django app"},{"location":"programming/django/#migrations","text":"Every changes on models, needs to be migrate to database schema. After add/update information to models.py file, changes needs to be done on database level Make migrations: python manage.py makemigrations <app> See migration on sql: python manage.py sqlmigrate <app> <number> Run migrations: python manage.py migrate","title":"Migrations"},{"location":"programming/django/#django-admin-basic-commands","text":"Creating an admin user: python manage.py createsuperuser Tests needs to be created. Run with: python manage.py test <app>","title":"django admin - basic commands"},{"location":"programming/fastapi/","text":"FastApi It is a python framework to build quickly restful API's. It is nice, modern, and with good performance. It has Openapi implementation by default, you don't need to manually create and maintain openapi.yaml. By default, fastApi generates automatically openapi document. Uses pydantic for models, for data validations. Support async by default too, enriching performance even faster. Careful when using async + DB. References FastAPI Pydantic models sqlmodel FastAPI def vs async def","title":"FastApi"},{"location":"programming/fastapi/#fastapi","text":"It is a python framework to build quickly restful API's. It is nice, modern, and with good performance. It has Openapi implementation by default, you don't need to manually create and maintain openapi.yaml. By default, fastApi generates automatically openapi document. Uses pydantic for models, for data validations. Support async by default too, enriching performance even faster. Careful when using async + DB.","title":"FastApi"},{"location":"programming/fastapi/#references","text":"FastAPI Pydantic models sqlmodel FastAPI def vs async def","title":"References"},{"location":"programming/flask/","text":"Flask It is nice framework to quickly build API from scratch. Specially to work with databases, it has a robust library to work with SQLAlchemy Schema validation with marshmallow, HTTP code responses standardized. References Flask SQLALchemy Blueprints Marshmallow","title":"Flask"},{"location":"programming/flask/#flask","text":"It is nice framework to quickly build API from scratch. Specially to work with databases, it has a robust library to work with SQLAlchemy Schema validation with marshmallow, HTTP code responses standardized.","title":"Flask"},{"location":"programming/flask/#references","text":"Flask SQLALchemy Blueprints Marshmallow","title":"References"},{"location":"programming/golang/","text":"Golang Golang or commonly known as Go, it is a programming language designed by Google. statically typed: once a variable is set, you cannot change its type. compiled programming language: Go is compiled directly to machine code, making it faster than interpreted languages. garbage-collected: Go has a garbage collector that automatically manages memory. concurrent: Go has built-in support for concurrent programming. You can run multiple tasks concurrently , making more efficient code. Go uses goroutines and channels to achieve this. You need to give a little bit of thought how you program your code. References Golang official website Go live reload Go Style Guide Go by example","title":"Golang"},{"location":"programming/golang/#golang","text":"Golang or commonly known as Go, it is a programming language designed by Google. statically typed: once a variable is set, you cannot change its type. compiled programming language: Go is compiled directly to machine code, making it faster than interpreted languages. garbage-collected: Go has a garbage collector that automatically manages memory. concurrent: Go has built-in support for concurrent programming. You can run multiple tasks concurrently , making more efficient code. Go uses goroutines and channels to achieve this. You need to give a little bit of thought how you program your code.","title":"Golang"},{"location":"programming/golang/#references","text":"Golang official website Go live reload Go Style Guide Go by example","title":"References"},{"location":"programming/openapi/","text":"OpenAPI Specification OpenAPI Specification is an API description format for REST APIs. It is in a machine-readable format (but also very human friendly), so it can have data, description and linting validation, making sure format is correct. Generates a documentation for humans, which is always up-to-date. Why is needed? It provides human and machine readable information about your REST API you are devolving. It is a nice tool to share information internally and externally. People knows in which stage is the project, are, what kind of response are expected... everything is shared in a common place visible for everyone involved in the project. Contracts are available (and testable). Even if code underneath is not fully ready, contracts are and will (should) not change whatsoever. OpenAPI description should be committed to source control, and, in fact, they should be among the first files to be committed. From there, they should also participate in Continuous Integration processes. Make the OpenAPI Documents Available to the Users. Beautifully-rendered documents can be very useful for the users of an API, but sometimes they might want to access the source OpenAPI description. For instance, to use tools to generate client code for them, or to build automated bindings for some language. Therefore, making the OpenAPI documents available to the users is an added bonus for them. The document can even be made available through the same API to allow runtime discovery. No need to write document by hand, there are nice tools or even frameworks like FastAPI that generates OpenAPI specification on the fly. Approaches There are two main different approaches regarding OpenAPI implementation. Apply one of the approaches regarding the project needs. Use a Design-First Approach Without any line of code, start the API design. OpenAPI specification is created to show the structure of the API, and then code is implemented to match the specification. Some tools can automatically build boilerplate from OpenAPI specification. It can be slower, but if requirements are clear, it is a good starting point. Contracts will be available from the beginning, and models can be reused easily. Use a Code-First Approach API is implemented together with the code, and OpenAPI specification is created with code comments, annotation and description in it. It is faster, but can lead to changes in the contract (may not be an issue if the project is starting up). References Online swagger editor Swagger specification Learn OpenAPI","title":"OpenAPI Specification"},{"location":"programming/openapi/#openapi-specification","text":"OpenAPI Specification is an API description format for REST APIs. It is in a machine-readable format (but also very human friendly), so it can have data, description and linting validation, making sure format is correct. Generates a documentation for humans, which is always up-to-date.","title":"OpenAPI Specification"},{"location":"programming/openapi/#why-is-needed","text":"It provides human and machine readable information about your REST API you are devolving. It is a nice tool to share information internally and externally. People knows in which stage is the project, are, what kind of response are expected... everything is shared in a common place visible for everyone involved in the project. Contracts are available (and testable). Even if code underneath is not fully ready, contracts are and will (should) not change whatsoever. OpenAPI description should be committed to source control, and, in fact, they should be among the first files to be committed. From there, they should also participate in Continuous Integration processes. Make the OpenAPI Documents Available to the Users. Beautifully-rendered documents can be very useful for the users of an API, but sometimes they might want to access the source OpenAPI description. For instance, to use tools to generate client code for them, or to build automated bindings for some language. Therefore, making the OpenAPI documents available to the users is an added bonus for them. The document can even be made available through the same API to allow runtime discovery. No need to write document by hand, there are nice tools or even frameworks like FastAPI that generates OpenAPI specification on the fly.","title":"Why is needed?"},{"location":"programming/openapi/#approaches","text":"There are two main different approaches regarding OpenAPI implementation. Apply one of the approaches regarding the project needs.","title":"Approaches"},{"location":"programming/openapi/#use-a-design-first-approach","text":"Without any line of code, start the API design. OpenAPI specification is created to show the structure of the API, and then code is implemented to match the specification. Some tools can automatically build boilerplate from OpenAPI specification. It can be slower, but if requirements are clear, it is a good starting point. Contracts will be available from the beginning, and models can be reused easily.","title":"Use a Design-First Approach"},{"location":"programming/openapi/#use-a-code-first-approach","text":"API is implemented together with the code, and OpenAPI specification is created with code comments, annotation and description in it. It is faster, but can lead to changes in the contract (may not be an issue if the project is starting up).","title":"Use a Code-First Approach"},{"location":"programming/openapi/#references","text":"Online swagger editor Swagger specification Learn OpenAPI","title":"References"},{"location":"programming/python/","text":"Python Python is the language I am more comfortable with. I have been using it for long time already. It is an interpreted language, so it is easy to play with it (it doesn't require any compilation or generate a executable file, like java). This allows to use python from the command line, to more complex files or web services. Below are brief explanations of python, but I recommend to check the references section for more information. Where to start First of all, you should install python in your computer. There are many ways to install, but a good premise is to use the latest stable version available, as contains the latest features, performance improvements and bug fixes. Also, consider use virtualenvs or poetry to avoid install packages/dependencies globally in your machine, allowing to use different versions per project. Good practices Use pep8 as a guide to write code. Keep code clean is important, as it will be easier to read and understand. It will also help to keep code consistent, as it will be easier to follow the same pattern. pep8 are a set of rules to follow. There are some tools that can help to check the quality of the code, such as flake8, pylint, ruff... Beauty of pep8, is that rules are flexible to meet the needs of the project. async When you are running python code, code is executing sequentially in the same process/thread. That means, process is blocked until code execution stopped. If another process enters, it acts like a queue. Until process 1 doesn't end, process 2 won't start execution. It can causes performance problem, as if for any reason process 1 is waiting (db connection, external request...), during those milliseconds CPU is waiting doing nothing. This problem is solved with concurrence. In python, is called async , allowing parallel execution of process, within the same CPU. But don worry, code execution is lineal inside the same process. So, whatever code you are running, code order is respected. Async works when there are many execution of the same code, each goes to a different thread inside the same process/CPU. Testing python code supports testing under the hook. unittest is the standard library, but there are others such as pytest . pytest pytest is a library that makes it easy to write simple tests. It has different plugins for the most popular frameworks, such as Django or Flask that can help you keep the same testing pattern across different projects. Simplify the process of set up and tear down with the use of fixtures. Supports assert statements, which makes it easier to write tests. Supports marking tests with different attributes, such as @pytest.mark.skip or @pytest.mark.e2e , which makes it easier to run groups of tests. Other testing libraries mutmut : mutation testing to help you enrich your tests. Hypothesis : generates random data to test your code. polyfactory : library to generate mocks with random data for dataclasses, pydantic models... References Python Python introduction Learn python Online Python exercises My python lessons Sample of async vs sync pytest plugin list","title":"Python"},{"location":"programming/python/#python","text":"Python is the language I am more comfortable with. I have been using it for long time already. It is an interpreted language, so it is easy to play with it (it doesn't require any compilation or generate a executable file, like java). This allows to use python from the command line, to more complex files or web services. Below are brief explanations of python, but I recommend to check the references section for more information.","title":"Python"},{"location":"programming/python/#where-to-start","text":"First of all, you should install python in your computer. There are many ways to install, but a good premise is to use the latest stable version available, as contains the latest features, performance improvements and bug fixes. Also, consider use virtualenvs or poetry to avoid install packages/dependencies globally in your machine, allowing to use different versions per project.","title":"Where to start"},{"location":"programming/python/#good-practices","text":"Use pep8 as a guide to write code. Keep code clean is important, as it will be easier to read and understand. It will also help to keep code consistent, as it will be easier to follow the same pattern. pep8 are a set of rules to follow. There are some tools that can help to check the quality of the code, such as flake8, pylint, ruff... Beauty of pep8, is that rules are flexible to meet the needs of the project.","title":"Good practices"},{"location":"programming/python/#async","text":"When you are running python code, code is executing sequentially in the same process/thread. That means, process is blocked until code execution stopped. If another process enters, it acts like a queue. Until process 1 doesn't end, process 2 won't start execution. It can causes performance problem, as if for any reason process 1 is waiting (db connection, external request...), during those milliseconds CPU is waiting doing nothing. This problem is solved with concurrence. In python, is called async , allowing parallel execution of process, within the same CPU. But don worry, code execution is lineal inside the same process. So, whatever code you are running, code order is respected. Async works when there are many execution of the same code, each goes to a different thread inside the same process/CPU.","title":"async"},{"location":"programming/python/#testing","text":"python code supports testing under the hook. unittest is the standard library, but there are others such as pytest .","title":"Testing"},{"location":"programming/python/#pytest","text":"pytest is a library that makes it easy to write simple tests. It has different plugins for the most popular frameworks, such as Django or Flask that can help you keep the same testing pattern across different projects. Simplify the process of set up and tear down with the use of fixtures. Supports assert statements, which makes it easier to write tests. Supports marking tests with different attributes, such as @pytest.mark.skip or @pytest.mark.e2e , which makes it easier to run groups of tests.","title":"pytest"},{"location":"programming/python/#other-testing-libraries","text":"mutmut : mutation testing to help you enrich your tests. Hypothesis : generates random data to test your code. polyfactory : library to generate mocks with random data for dataclasses, pydantic models...","title":"Other testing libraries"},{"location":"programming/python/#references","text":"Python Python introduction Learn python Online Python exercises My python lessons Sample of async vs sync pytest plugin list","title":"References"},{"location":"programming/sqlalchemy/","text":"SQL Alchemy References alembic Asyncio scoped session ORM ORM session sample","title":"SQLAlchemy"},{"location":"programming/sqlalchemy/#sql-alchemy","text":"","title":"SQL Alchemy"},{"location":"programming/sqlalchemy/#references","text":"alembic Asyncio scoped session ORM ORM session sample","title":"References"},{"location":"programming/virtualenvs/","text":"Virtualenvs What is virtualenv Most important thing in python, is http://python-guide-pt-br.readthedocs.io/en/latest/dev/virtualenvs/ virtualenv is a tool to create isolated Python environments. virtualenv creates a folder which contains all the necessary executables and libraries to use the packages that a Python project would need. Installation To install virtualenv tool: pip install virtualenv Test your installation: virtualenv --version Basic Usage how to create a virtual environment for a project: cd /home/<user>/.virtualenvs/ virtualenv <my_project> --python=/usr/bin/<python-version> To use a particular virtual env, it needs to be activated: source ~/.virtualenvs/<my_project>/bin/activate Everything you import or install in an activated virtualenv, will be just for this environment To deactivate a virtual environment: deactivate Poetry Poetry is a dependency management. Easy installation, and management. Instead of a mix of files related to installation, everything is managed in file pyproject.toml . Also allows to differentiate between library installation for dev and production. According to pep518 , pyproject.toml is the file where the build system dependencies should be stored in TOML format. Toml is basic key-vales pair format. It can group common configuration, and it can be use for multiple purposes.","title":"Virtualenvs"},{"location":"programming/virtualenvs/#virtualenvs","text":"","title":"Virtualenvs"},{"location":"programming/virtualenvs/#what-is-virtualenv","text":"Most important thing in python, is http://python-guide-pt-br.readthedocs.io/en/latest/dev/virtualenvs/ virtualenv is a tool to create isolated Python environments. virtualenv creates a folder which contains all the necessary executables and libraries to use the packages that a Python project would need.","title":"What is virtualenv"},{"location":"programming/virtualenvs/#installation","text":"To install virtualenv tool: pip install virtualenv Test your installation: virtualenv --version","title":"Installation"},{"location":"programming/virtualenvs/#basic-usage","text":"how to create a virtual environment for a project: cd /home/<user>/.virtualenvs/ virtualenv <my_project> --python=/usr/bin/<python-version> To use a particular virtual env, it needs to be activated: source ~/.virtualenvs/<my_project>/bin/activate Everything you import or install in an activated virtualenv, will be just for this environment To deactivate a virtual environment: deactivate","title":"Basic Usage"},{"location":"programming/virtualenvs/#poetry","text":"Poetry is a dependency management. Easy installation, and management. Instead of a mix of files related to installation, everything is managed in file pyproject.toml . Also allows to differentiate between library installation for dev and production. According to pep518 , pyproject.toml is the file where the build system dependencies should be stored in TOML format. Toml is basic key-vales pair format. It can group common configuration, and it can be use for multiple purposes.","title":"Poetry"},{"location":"project-management/c4-model/","text":"C4 Model C4 model represents different diagrams for different roles working together for the same ecosystem. Not everyone needs the same level of details on a system/service. Different levels reaches to different roles. And it is easy to zoom-in/zoom-out to understand more/less details. L1 - System contexts Big boxes, where different system of the ecosystem working together L2 - container diagram Zoom in on a system. What is inside, different services running inside it (web page, database mobile app...) and relationships L3 - component diagram Zoom in a container. How the code will be divided, major building blocks and their interactions L4 - Code Zoom in a component, usually is not recommend, as it contains too much details, and it is not worthy. Can be generated on demand by IDE Notations and tips Title, short and meaningful Keep Visual consistency Avoid acronyms Schema of a box Name [What represent] Description Lines, usually uni-directional. Shows data flows, with explicit notation on it Show bi-directional lines when intents are different Add words to make the intent explicit Key/Legend to explain shapes, lines, colors, border... even if they seem obvious Increase the readability of software architecture diagrams, so they can stand alone References C4 modelling tool","title":"C4 Model"},{"location":"project-management/c4-model/#c4-model","text":"C4 model represents different diagrams for different roles working together for the same ecosystem. Not everyone needs the same level of details on a system/service. Different levels reaches to different roles. And it is easy to zoom-in/zoom-out to understand more/less details.","title":"C4 Model"},{"location":"project-management/c4-model/#l1-system-contexts","text":"Big boxes, where different system of the ecosystem working together","title":"L1 - System contexts"},{"location":"project-management/c4-model/#l2-container-diagram","text":"Zoom in on a system. What is inside, different services running inside it (web page, database mobile app...) and relationships","title":"L2 - container diagram"},{"location":"project-management/c4-model/#l3-component-diagram","text":"Zoom in a container. How the code will be divided, major building blocks and their interactions","title":"L3 - component diagram"},{"location":"project-management/c4-model/#l4-code","text":"Zoom in a component, usually is not recommend, as it contains too much details, and it is not worthy. Can be generated on demand by IDE","title":"L4 - Code"},{"location":"project-management/c4-model/#notations-and-tips","text":"Title, short and meaningful Keep Visual consistency Avoid acronyms Schema of a box Name [What represent] Description Lines, usually uni-directional. Shows data flows, with explicit notation on it Show bi-directional lines when intents are different Add words to make the intent explicit Key/Legend to explain shapes, lines, colors, border... even if they seem obvious Increase the readability of software architecture diagrams, so they can stand alone","title":"Notations and tips"},{"location":"project-management/c4-model/#references","text":"C4 modelling tool","title":"References"},{"location":"project-management/diagram-flows/","text":"Diagram Flows Diagram flows are used to represent the flow of a process. It is a visual representation of the steps and decisions that are made in a process, specially useful for complex processes. It is a useful tool for understanding the process and identifying the areas that need improvement. There are many different types of diagram. Each type has its own strengths and weaknesses, and is best suited for different types of processes. Flowchart Diagram that represents a workflow or process, showing the steps as boxes of various kinds, and their order by connecting them with arrows. The key is acknowledging your audience, so you can make simple or complex flows, with more or less details. See common flowchart symbols . Swimlane Type of flowchart that delineates who does what in a process, showing connection and communication between different departments or individuals. References Auto generation of diagram Flowchart Swimlane diagram","title":"Diagram Flows"},{"location":"project-management/diagram-flows/#diagram-flows","text":"Diagram flows are used to represent the flow of a process. It is a visual representation of the steps and decisions that are made in a process, specially useful for complex processes. It is a useful tool for understanding the process and identifying the areas that need improvement. There are many different types of diagram. Each type has its own strengths and weaknesses, and is best suited for different types of processes.","title":"Diagram Flows"},{"location":"project-management/diagram-flows/#flowchart","text":"Diagram that represents a workflow or process, showing the steps as boxes of various kinds, and their order by connecting them with arrows. The key is acknowledging your audience, so you can make simple or complex flows, with more or less details. See common flowchart symbols .","title":"Flowchart"},{"location":"project-management/diagram-flows/#swimlane","text":"Type of flowchart that delineates who does what in a process, showing connection and communication between different departments or individuals.","title":"Swimlane"},{"location":"project-management/diagram-flows/#references","text":"Auto generation of diagram Flowchart Swimlane diagram","title":"References"},{"location":"project-management/scrum/","text":"SCRUM Scrum is a lightweight framework for agile development, one of the most used. Agile is a methodology that encourage fast inspection and adaptation. Encourage teamwork, self-organization, to delivery high quality software. Scrum differentiates from other frameworks by having a set of roles, artifacts and time boxes. Scrum is most often used to manage complex software and product development, using iterative and incremental practices. Scrum significantly increases productivity and reduces time to benefits relative to classic waterfall processes. Scrum Roles ScrumMaster Is responsible for running the process smoothly, for removing obstacles that impact productivity, and for organizing and facilitating the critical meetings. The ScrumMaster should maintain a constant awareness of the status of the project (its progress to date) relative to the expected progress, investigate and facilitate resolution of any roadblocks that hold back progress, and generally be flexible enough to identify and deal with any issues that arise. The ScrumMaster is not a manager, but a facilitator. Product Owner The Product Owner is the keeper of the requirements. The Product Owner provides the \"single source of truth\" for the Team, regarding requirements and their planned order of implementation. Also, in charge to determine the objective, of the sprint. Team Is a self-organizing and cross-functional group of people who do the hands-on work of developing and testing the product. Since the Team is responsible for producing the product, it must also have the authority to make decisions about how to perform the work. The Team is therefore self-organizing: Team members decide how to break work into tasks, and how to allocate tasks to individuals, throughout the Sprint. Scrum Artifacts Product Backlog Board where are gather all the requirements of the project. It is a living document, that can be updated at any time. It is a list of features, bugs, technical work or knowledge acquisition. Sprint Backlog List of items to be tackled during a sprint. It is a subset of the Product Backlog. Agreed by the Team and the Product Owner in the planning session. Scrum Events Daily Meeting With a duration of no more of 15 minutes, Team owns the meeting and should speak in order to comment what is the plan for today, focus on meeting the objective of the Sprint. Avoid doing reporting, discussion should be based on the objective of the sprint, as Daily is not a status meeting. Raise if there are any blockers in the near future. Refinement session Meeting to decide what are the tasks planed for the next sprint, so during sprint there is no mismatch. Discussion can happens at this stage, but it should be as clear for everyone. Clear Acceptance Criteria from the business point of view, adding value to the project. It should be handle before the start next sprint. Planning session Where estimation of tickets come to place. Define and confirm the Goal for the sprint. Goal should be always be achievable. Review the capacity of the Team. Just before sprint is started. Review session With stakeholder and after sprint. Show delivery, progress and value added during the sprint. Retro session Discussion about what happened in last sprint. It is a safe space to talk about what went wrong, and how process can be improved. Other Concepts User stories/Use cases Description of a feature in a narrative form. Written by the Product Owner, and responsibility of the same to keep up-to-date. The elements in this User Story are: Name: The Name is a descriptive phrase or sentence. The example uses a basic \"Role-Action-Reason\" organization. It is important to have workable standard of some kind. Description: This is a high-level (low-detail) description of the need to be met. For functional (user-facing) requirements, the description is put in narrative form. For non-functional requirements, the description can be worded in any form that is easy to understand. In both cases, the key is that the level of detail is modest, because the fine details are worked out during the implementation phase, in discussions between team members, product owners, and anyone else who is involved. (This is one of the core concepts of Scrum: Requirements are specified at a level that allows rough estimation of the work required to implement them, not in detail.) Screens and External Documents: If the Story requires user-interface changes (especially non-trivial ones), the Story should contain or link to a prototype of the changes. Any external documents required to implement the Story should also be listed. How to test: The implementation of a Story is defined to be complete if, and only if, it passes all acceptance tests developed for it. This section provides a brief description of how the story will be tested. As for the feature itself, the description of testing methods is short, with the details to be worked out during implementation, but we need at least a summary to guide the estimation process. Story Not all requirements for new development represent user-facing features, but do represent significant work that must be done. These requirements often, but not always, represent work that must be done to support user-facing features. We call these non-functional requirements Technical Stories. Technical Stories have the same elements as User Stories, but need not be cast into narrative form if there is no benefit in doing so. Technical Stories are usually written by Team members, and are added to the Product Backlog. The Product Owner must be familiar with these Stories, and understand the dependencies between these and User Stories in order to rank (sequence) all Stories for implementation. Defect A Defect, or bug report, is a description of a failure of the product to behave in the expected fashion. Defects are stored in a bug-tracking system, which may or may not be physically the same system used to store the Product Backlog. If not, then someone (usually the Product Owner) must enter each Defect into the Product Backlog, for sequencing and scheduling. Sprint Fixed time duration to accomplish an agreed objective, with value by the Team, which can be potentially be deliver to production/final stage. Usually 2 weeks. Story Points Measurement of effort of a certain Story. experience, complexity and load of work are taken into account. It must be taken by the Team. Objective, is to have an agreement of the effort of the Story. TL usually explain task to the team, doubts are solved. TL is not the one who decided, but Team. Notes are taken. Once is clear, values from a Fibonacci sequence are assigned to the Story. 1, 2, 3, 5, 8, 13, 21. 1 is the lowest effort, 21 is the greatest effort. More than 21, should be split in smaller User Stories. Discuss with team regarding any particular difference on estimation, and agreed the effort. Personal experience Not everything is closed at the beginning of the sprint. Flexibility to learn. Focus on objective of the sprint, complete task and give value to the product. Communication is key. It is not about the person, is the Team. References Agile manifesto What is Scrum? How it works? Online platform for scrum meetings Icebreaker retrospective JIRA tutorial","title":"SCRUM"},{"location":"project-management/scrum/#scrum","text":"Scrum is a lightweight framework for agile development, one of the most used. Agile is a methodology that encourage fast inspection and adaptation. Encourage teamwork, self-organization, to delivery high quality software. Scrum differentiates from other frameworks by having a set of roles, artifacts and time boxes. Scrum is most often used to manage complex software and product development, using iterative and incremental practices. Scrum significantly increases productivity and reduces time to benefits relative to classic waterfall processes.","title":"SCRUM"},{"location":"project-management/scrum/#scrum-roles","text":"","title":"Scrum Roles"},{"location":"project-management/scrum/#scrummaster","text":"Is responsible for running the process smoothly, for removing obstacles that impact productivity, and for organizing and facilitating the critical meetings. The ScrumMaster should maintain a constant awareness of the status of the project (its progress to date) relative to the expected progress, investigate and facilitate resolution of any roadblocks that hold back progress, and generally be flexible enough to identify and deal with any issues that arise. The ScrumMaster is not a manager, but a facilitator.","title":"ScrumMaster"},{"location":"project-management/scrum/#product-owner","text":"The Product Owner is the keeper of the requirements. The Product Owner provides the \"single source of truth\" for the Team, regarding requirements and their planned order of implementation. Also, in charge to determine the objective, of the sprint.","title":"Product Owner"},{"location":"project-management/scrum/#team","text":"Is a self-organizing and cross-functional group of people who do the hands-on work of developing and testing the product. Since the Team is responsible for producing the product, it must also have the authority to make decisions about how to perform the work. The Team is therefore self-organizing: Team members decide how to break work into tasks, and how to allocate tasks to individuals, throughout the Sprint.","title":"Team"},{"location":"project-management/scrum/#scrum-artifacts","text":"","title":"Scrum Artifacts"},{"location":"project-management/scrum/#product-backlog","text":"Board where are gather all the requirements of the project. It is a living document, that can be updated at any time. It is a list of features, bugs, technical work or knowledge acquisition.","title":"Product Backlog"},{"location":"project-management/scrum/#sprint-backlog","text":"List of items to be tackled during a sprint. It is a subset of the Product Backlog. Agreed by the Team and the Product Owner in the planning session.","title":"Sprint Backlog"},{"location":"project-management/scrum/#scrum-events","text":"","title":"Scrum Events"},{"location":"project-management/scrum/#daily-meeting","text":"With a duration of no more of 15 minutes, Team owns the meeting and should speak in order to comment what is the plan for today, focus on meeting the objective of the Sprint. Avoid doing reporting, discussion should be based on the objective of the sprint, as Daily is not a status meeting. Raise if there are any blockers in the near future.","title":"Daily Meeting"},{"location":"project-management/scrum/#refinement-session","text":"Meeting to decide what are the tasks planed for the next sprint, so during sprint there is no mismatch. Discussion can happens at this stage, but it should be as clear for everyone. Clear Acceptance Criteria from the business point of view, adding value to the project. It should be handle before the start next sprint.","title":"Refinement session"},{"location":"project-management/scrum/#planning-session","text":"Where estimation of tickets come to place. Define and confirm the Goal for the sprint. Goal should be always be achievable. Review the capacity of the Team. Just before sprint is started.","title":"Planning session"},{"location":"project-management/scrum/#review-session","text":"With stakeholder and after sprint. Show delivery, progress and value added during the sprint.","title":"Review session"},{"location":"project-management/scrum/#retro-session","text":"Discussion about what happened in last sprint. It is a safe space to talk about what went wrong, and how process can be improved.","title":"Retro session"},{"location":"project-management/scrum/#other-concepts","text":"","title":"Other Concepts"},{"location":"project-management/scrum/#user-storiesuse-cases","text":"Description of a feature in a narrative form. Written by the Product Owner, and responsibility of the same to keep up-to-date. The elements in this User Story are: Name: The Name is a descriptive phrase or sentence. The example uses a basic \"Role-Action-Reason\" organization. It is important to have workable standard of some kind. Description: This is a high-level (low-detail) description of the need to be met. For functional (user-facing) requirements, the description is put in narrative form. For non-functional requirements, the description can be worded in any form that is easy to understand. In both cases, the key is that the level of detail is modest, because the fine details are worked out during the implementation phase, in discussions between team members, product owners, and anyone else who is involved. (This is one of the core concepts of Scrum: Requirements are specified at a level that allows rough estimation of the work required to implement them, not in detail.) Screens and External Documents: If the Story requires user-interface changes (especially non-trivial ones), the Story should contain or link to a prototype of the changes. Any external documents required to implement the Story should also be listed. How to test: The implementation of a Story is defined to be complete if, and only if, it passes all acceptance tests developed for it. This section provides a brief description of how the story will be tested. As for the feature itself, the description of testing methods is short, with the details to be worked out during implementation, but we need at least a summary to guide the estimation process.","title":"User stories/Use cases"},{"location":"project-management/scrum/#story","text":"Not all requirements for new development represent user-facing features, but do represent significant work that must be done. These requirements often, but not always, represent work that must be done to support user-facing features. We call these non-functional requirements Technical Stories. Technical Stories have the same elements as User Stories, but need not be cast into narrative form if there is no benefit in doing so. Technical Stories are usually written by Team members, and are added to the Product Backlog. The Product Owner must be familiar with these Stories, and understand the dependencies between these and User Stories in order to rank (sequence) all Stories for implementation.","title":"Story"},{"location":"project-management/scrum/#defect","text":"A Defect, or bug report, is a description of a failure of the product to behave in the expected fashion. Defects are stored in a bug-tracking system, which may or may not be physically the same system used to store the Product Backlog. If not, then someone (usually the Product Owner) must enter each Defect into the Product Backlog, for sequencing and scheduling.","title":"Defect"},{"location":"project-management/scrum/#sprint","text":"Fixed time duration to accomplish an agreed objective, with value by the Team, which can be potentially be deliver to production/final stage. Usually 2 weeks.","title":"Sprint"},{"location":"project-management/scrum/#story-points","text":"Measurement of effort of a certain Story. experience, complexity and load of work are taken into account. It must be taken by the Team. Objective, is to have an agreement of the effort of the Story. TL usually explain task to the team, doubts are solved. TL is not the one who decided, but Team. Notes are taken. Once is clear, values from a Fibonacci sequence are assigned to the Story. 1, 2, 3, 5, 8, 13, 21. 1 is the lowest effort, 21 is the greatest effort. More than 21, should be split in smaller User Stories. Discuss with team regarding any particular difference on estimation, and agreed the effort.","title":"Story Points"},{"location":"project-management/scrum/#personal-experience","text":"Not everything is closed at the beginning of the sprint. Flexibility to learn. Focus on objective of the sprint, complete task and give value to the product. Communication is key. It is not about the person, is the Team.","title":"Personal experience"},{"location":"project-management/scrum/#references","text":"Agile manifesto What is Scrum? How it works? Online platform for scrum meetings Icebreaker retrospective JIRA tutorial","title":"References"},{"location":"project-management/tech-lead/","text":"Technical Leader The job of a tech lead is to make sure the team work with quality. Part of this job is to plan, design, learn and execute technical solutions and improvements. Interact with other tech leads, sharing information and good practices. Also, clarifying technical doubts with stakeholders. Aside from that, a tech lead is also responsible for the team, understanding member\u2019s strengths, weaknesses. Mentoring and guide them. Some actions that a tech lead can do are: Keep in touch with popular tech conferences and events Read tech blogs daily cult.honeypot , Quastor Attend meet-ups and also maintain constant communication with other Tech Leads around you. Responsibilities Automate and improve the development process as much as possible, removing the need for manual intervention when possible. Ensure the team is following best practices described below: Code quality assurance A code quality culture is essential, at organizational or individual level. High code quality ensures your codebase is maintainable, scalable, and efficient, allowing to deliver new features faster. Establish agreed code styles, such as PEP8 . Keep code consistent by establishing conventions for things like naming, spacing and indentation. No linters issues. Find bugs, duplicity of code, bad practices, TODO/FIXME... use tools like pre-commit hooks , jenkins, sonarqube, etc. Good test code coverage with unit and functional test. Over 95% if possible. TDD (testing design development) approach when possible. Start implementing tests before the code. Testing plan to ensure the code delivered to live environment is bug free and satisfy the expected results. Tracking high-quality issues to manage tech debt properly. Prioritize and schedule tech debt. Have a definition of DONE. A checklist of things that need to be done before a ticket is considered done. Track code quality metrics and code complexity. Peer mentoring when needed. Code Comments Establish conventions to ensure comments are useful, improving engineering velocity and code quality. Managing and reducing technical debt While TODO/FIXME are good for single player, but avoid using in teams. Use them to aid your personal code development process and never push them to main code base branch. Focus on the why . Provide context and explain the intent of the code in classes or methods ( docs strings ) Avoid obvious comments. Provide architectural and design direction Follow SOLID principles. Avoid code rewritten or delete not long after creation. Reduce code smells by refactoring code. Control Technical Debt Keeping a low number of technical debt. Ensure code review healthiness Ensure code reviews are done and are effective. Code reviews should be done by a peer, not by the author. The author should be able to explain the code to the reviewer. The reviewer should be able to understand the code without the author's help. Applying good pull request practices. Reducing time to merge. Avoid huge pull request. Commit lint online Docstring pydoc Ship good quality code fast Deploying to production frequently. SMall deployments. Keep a relation of bugs per deployment. Are detected fast by the team or user? Are fixed fast? Motivation From external party working with a client, we are the providers, the ones that gives the solution. Objective of a ticket is to give value to the product, not just close tickets. Adapt our work methodology. Client can guide, give us rules. But our work is our project. And needs to be the best possible. We as external need to be lead. References Responsibilities of a Tech Lead How to build quality code","title":"Technical Leader"},{"location":"project-management/tech-lead/#technical-leader","text":"The job of a tech lead is to make sure the team work with quality. Part of this job is to plan, design, learn and execute technical solutions and improvements. Interact with other tech leads, sharing information and good practices. Also, clarifying technical doubts with stakeholders. Aside from that, a tech lead is also responsible for the team, understanding member\u2019s strengths, weaknesses. Mentoring and guide them. Some actions that a tech lead can do are: Keep in touch with popular tech conferences and events Read tech blogs daily cult.honeypot , Quastor Attend meet-ups and also maintain constant communication with other Tech Leads around you.","title":"Technical Leader"},{"location":"project-management/tech-lead/#responsibilities","text":"Automate and improve the development process as much as possible, removing the need for manual intervention when possible. Ensure the team is following best practices described below:","title":"Responsibilities"},{"location":"project-management/tech-lead/#code-quality-assurance","text":"A code quality culture is essential, at organizational or individual level. High code quality ensures your codebase is maintainable, scalable, and efficient, allowing to deliver new features faster. Establish agreed code styles, such as PEP8 . Keep code consistent by establishing conventions for things like naming, spacing and indentation. No linters issues. Find bugs, duplicity of code, bad practices, TODO/FIXME... use tools like pre-commit hooks , jenkins, sonarqube, etc. Good test code coverage with unit and functional test. Over 95% if possible. TDD (testing design development) approach when possible. Start implementing tests before the code. Testing plan to ensure the code delivered to live environment is bug free and satisfy the expected results. Tracking high-quality issues to manage tech debt properly. Prioritize and schedule tech debt. Have a definition of DONE. A checklist of things that need to be done before a ticket is considered done. Track code quality metrics and code complexity. Peer mentoring when needed.","title":"Code quality assurance"},{"location":"project-management/tech-lead/#code-comments","text":"Establish conventions to ensure comments are useful, improving engineering velocity and code quality. Managing and reducing technical debt While TODO/FIXME are good for single player, but avoid using in teams. Use them to aid your personal code development process and never push them to main code base branch. Focus on the why . Provide context and explain the intent of the code in classes or methods ( docs strings ) Avoid obvious comments.","title":"Code Comments"},{"location":"project-management/tech-lead/#provide-architectural-and-design-direction","text":"Follow SOLID principles. Avoid code rewritten or delete not long after creation. Reduce code smells by refactoring code.","title":"Provide architectural and design direction"},{"location":"project-management/tech-lead/#control-technical-debt","text":"Keeping a low number of technical debt.","title":"Control Technical Debt"},{"location":"project-management/tech-lead/#ensure-code-review-healthiness","text":"Ensure code reviews are done and are effective. Code reviews should be done by a peer, not by the author. The author should be able to explain the code to the reviewer. The reviewer should be able to understand the code without the author's help. Applying good pull request practices. Reducing time to merge. Avoid huge pull request. Commit lint online Docstring pydoc","title":"Ensure code review healthiness"},{"location":"project-management/tech-lead/#ship-good-quality-code-fast","text":"Deploying to production frequently. SMall deployments. Keep a relation of bugs per deployment. Are detected fast by the team or user? Are fixed fast?","title":"Ship good quality code fast"},{"location":"project-management/tech-lead/#motivation","text":"From external party working with a client, we are the providers, the ones that gives the solution. Objective of a ticket is to give value to the product, not just close tickets. Adapt our work methodology. Client can guide, give us rules. But our work is our project. And needs to be the best possible. We as external need to be lead.","title":"Motivation"},{"location":"project-management/tech-lead/#references","text":"Responsibilities of a Tech Lead How to build quality code","title":"References"},{"location":"project-management/waterfall-methodology/","text":"Waterfall Methodology Waterfall methodology is a linear project management, when requirements are defined at beginning. Task are defined sequential in a project plan (Gantt Chart) to accommodate those requirements The Phases of the Waterfall Model The waterfall approach has, at least, five to seven phases that follow in strict linear order, where a phase can\u2019t begin until the previous phase has been completed. The specific names of the waterfall steps vary, but they were originally defined by its inventor, Winston W. Royce, in the following way: Requirements The key aspect of the waterfall methodology is that all customer requirements are gathered at the beginning of the project, allowing every other phase to be planned without further customer correspondence until the product is complete. It is assumed that all requirements can be gathered at this waterfall management phase. Design The design phase of the waterfall process is best broken up into two sub-phases: logical design and physical design. The logical design sub-phase is when possible solutions are brainstormed and theorized. The physical design sub-phase is when those theoretical ideas and schemas are made into concrete specifications. Implementation The implementation phase is when programmers assimilate the requirements and specifications from the previous phases and produce actual code. Verification This phase is when the customer reviews the product to make sure that it meets the requirements laid out at the beginning of the waterfall project. This is done by releasing the completed product to the customer. Maintenance The customer is regularly using the product during the maintenance phase, discovering bugs, inadequate features and other errors that occurred during production. The production team applies these fixes as necessary until the customer is satisfied.","title":"Waterfall Methodology"},{"location":"project-management/waterfall-methodology/#waterfall-methodology","text":"Waterfall methodology is a linear project management, when requirements are defined at beginning. Task are defined sequential in a project plan (Gantt Chart) to accommodate those requirements","title":"Waterfall Methodology"},{"location":"project-management/waterfall-methodology/#the-phases-of-the-waterfall-model","text":"The waterfall approach has, at least, five to seven phases that follow in strict linear order, where a phase can\u2019t begin until the previous phase has been completed. The specific names of the waterfall steps vary, but they were originally defined by its inventor, Winston W. Royce, in the following way:","title":"The Phases of the Waterfall Model"},{"location":"project-management/waterfall-methodology/#requirements","text":"The key aspect of the waterfall methodology is that all customer requirements are gathered at the beginning of the project, allowing every other phase to be planned without further customer correspondence until the product is complete. It is assumed that all requirements can be gathered at this waterfall management phase.","title":"Requirements"},{"location":"project-management/waterfall-methodology/#design","text":"The design phase of the waterfall process is best broken up into two sub-phases: logical design and physical design. The logical design sub-phase is when possible solutions are brainstormed and theorized. The physical design sub-phase is when those theoretical ideas and schemas are made into concrete specifications.","title":"Design"},{"location":"project-management/waterfall-methodology/#implementation","text":"The implementation phase is when programmers assimilate the requirements and specifications from the previous phases and produce actual code.","title":"Implementation"},{"location":"project-management/waterfall-methodology/#verification","text":"This phase is when the customer reviews the product to make sure that it meets the requirements laid out at the beginning of the waterfall project. This is done by releasing the completed product to the customer.","title":"Verification"},{"location":"project-management/waterfall-methodology/#maintenance","text":"The customer is regularly using the product during the maintenance phase, discovering bugs, inadequate features and other errors that occurred during production. The production team applies these fixes as necessary until the customer is satisfied.","title":"Maintenance"},{"location":"project-management/work-remotely/","text":"Work Remotely In modern world, and specially after COVID-19 pandemic, work remotely is a reality for IT sector. Luckily, I have been working in companies with high compromise and respect for work remotely. And I like it :) Some key notes to take into consideration: Trust people Data driven Think beyond schedule Promote async communication Better planning with less distractions","title":"Work Remotely"},{"location":"project-management/work-remotely/#work-remotely","text":"In modern world, and specially after COVID-19 pandemic, work remotely is a reality for IT sector. Luckily, I have been working in companies with high compromise and respect for work remotely. And I like it :) Some key notes to take into consideration: Trust people Data driven Think beyond schedule Promote async communication Better planning with less distractions","title":"Work Remotely"},{"location":"utils/daemon/","text":"Linux Daemon How to create a daemon in linux Create new file in path /etc/init.d/ Open and copy sample in url. Update some of the variables, like name description and path to python file. On python path, point to the python created in the virtualenv, so it will have installed corresponding libraries. All operations below may be required to be executed with sudo Give writes privileges: chmod +x /etc/init.d/<daemon-name> -v Reload daemon configuration: systemctl daemon-reload Start service: service <daemon-name> start References Instructions Running on startup","title":"Daemon"},{"location":"utils/daemon/#linux-daemon","text":"","title":"Linux Daemon"},{"location":"utils/daemon/#how-to-create-a-daemon-in-linux","text":"Create new file in path /etc/init.d/ Open and copy sample in url. Update some of the variables, like name description and path to python file. On python path, point to the python created in the virtualenv, so it will have installed corresponding libraries. All operations below may be required to be executed with sudo Give writes privileges: chmod +x /etc/init.d/<daemon-name> -v Reload daemon configuration: systemctl daemon-reload Start service: service <daemon-name> start","title":"How to create a daemon in linux"},{"location":"utils/daemon/#references","text":"Instructions Running on startup","title":"References"},{"location":"utils/databases/","text":"Databases Relational databases PostgreSQL Similar to MySql. Once installed, run psql to access via command line to the instances. An instance of postgresql can have multiple databases. Another interesting command is pg_dump , it helps you to generate a backup of your tables into a .sql file. Connection options: -h, --host=HOSTNAME database server host or socket directory (default: \"/var/run/postgresql\") -p, --port=PORT database server port (default: \"5432\") -U, --username=USERNAME database user name (default: \"<user>\") -w, --no-password never prompt for password -W, --password force password prompt (should happen automatically) Google cloud SQL From Google cloud you can create an instance of mysql/postgres for you desire database. You can connect via socket or url to database. Online configuration, require code changes. In addition, you can have a connection using the Cloud SQL Auth proxy Non-relational databases Firestore NoSQL database from Google cloud. python libraries to use and access documents. Graphs databases Graphs are a very popular way of representing relationships and connections within your data. They\u2019re composed of two main components: Vertices : These represent entities/nodes in your data. Edges : These represent connections between entities/nodes. Graph databases are particularly useful when you have a lot of relationships, because its faster processing of relationships: each node has direct references to its neighbors, so you don\u2019t need to scan the entire dataset to find relationships. It also allows you to perform complex queries in a more natural way, thanks to graph query language such as Cypher. Neo4j Neo4j is a popular graph database that uses the Cypher query language to interact with the database. Your data is stored without restricting it to a pre-defined model, allowing a very flexible way of thinking about and using it. Columnar Databases Columnar databases store data tables by column rather than by row, where the values of a data column are serialized together. By storing data in columns rather than rows, the database can more precisely access the data it needs to answer a query rather than scanning and discarding unwanted data in rows. Popular in data warehousing and machine learning . Some columnar databases are: Big Query Snowflake . Columnar database have a series of advantages: Compression: Columnar databases can compress data more effectively than row-based databases because similar data is stored together. Batch transactions. Complex query over data. References Database 101 PostgreSQL PostgreSQL Cheatsheet PostgreSQL Backup MongoDB Google Cloud Postgres Firestore Neo4j","title":"Databases"},{"location":"utils/databases/#databases","text":"","title":"Databases"},{"location":"utils/databases/#relational-databases","text":"","title":"Relational databases"},{"location":"utils/databases/#postgresql","text":"Similar to MySql. Once installed, run psql to access via command line to the instances. An instance of postgresql can have multiple databases. Another interesting command is pg_dump , it helps you to generate a backup of your tables into a .sql file. Connection options: -h, --host=HOSTNAME database server host or socket directory (default: \"/var/run/postgresql\") -p, --port=PORT database server port (default: \"5432\") -U, --username=USERNAME database user name (default: \"<user>\") -w, --no-password never prompt for password -W, --password force password prompt (should happen automatically)","title":"PostgreSQL"},{"location":"utils/databases/#google-cloud-sql","text":"From Google cloud you can create an instance of mysql/postgres for you desire database. You can connect via socket or url to database. Online configuration, require code changes. In addition, you can have a connection using the Cloud SQL Auth proxy","title":"Google cloud SQL"},{"location":"utils/databases/#non-relational-databases","text":"","title":"Non-relational databases"},{"location":"utils/databases/#firestore","text":"NoSQL database from Google cloud. python libraries to use and access documents.","title":"Firestore"},{"location":"utils/databases/#graphs-databases","text":"Graphs are a very popular way of representing relationships and connections within your data. They\u2019re composed of two main components: Vertices : These represent entities/nodes in your data. Edges : These represent connections between entities/nodes. Graph databases are particularly useful when you have a lot of relationships, because its faster processing of relationships: each node has direct references to its neighbors, so you don\u2019t need to scan the entire dataset to find relationships. It also allows you to perform complex queries in a more natural way, thanks to graph query language such as Cypher.","title":"Graphs databases"},{"location":"utils/databases/#neo4j","text":"Neo4j is a popular graph database that uses the Cypher query language to interact with the database. Your data is stored without restricting it to a pre-defined model, allowing a very flexible way of thinking about and using it.","title":"Neo4j"},{"location":"utils/databases/#columnar-databases","text":"Columnar databases store data tables by column rather than by row, where the values of a data column are serialized together. By storing data in columns rather than rows, the database can more precisely access the data it needs to answer a query rather than scanning and discarding unwanted data in rows. Popular in data warehousing and machine learning . Some columnar databases are: Big Query Snowflake . Columnar database have a series of advantages: Compression: Columnar databases can compress data more effectively than row-based databases because similar data is stored together. Batch transactions. Complex query over data.","title":"Columnar Databases"},{"location":"utils/databases/#references","text":"Database 101 PostgreSQL PostgreSQL Cheatsheet PostgreSQL Backup MongoDB Google Cloud Postgres Firestore Neo4j","title":"References"},{"location":"utils/db-table-indexes/","text":"Database Table Indexes An important aspect of database design is the creation of indexes, specially those ones that are unique. UUIDs UUIDS are a 128-bit number used to uniquely identify objects in a computer. In a database, UUIDs can be used as primary keys for tables. But the are some issues to take a look at: Performance : to organize indexes, the database needs to sort the data. This is a problem when using UUIDs because they are not sequential. Storage : UUIDs are 128 bits long, which is more than the 32 bits used by by auto-incrementing IDs. In a large database, this can lead to a significant amount of storage. On the other hand, UUIDs are unique across different databases and can be generated in different machines without the need to coordinate with each other. Auto-incrementing IDs Auto-incrementing IDs are a good choice for primary keys in a database. They are sequential and can be used to organize indexes quickly and efficiently. They don't take up much storage space. It is a natural sequence which is easy to predict and identify. Ideally, when those ids are not exposed to the user. UUIDsv7 UUIDv7 is a new version of UUIDs that are sequential, solving performance issue on insert new records. It offers the best of both worlds: the uniqueness of UUIDs and the performance of auto-incrementing IDs. ULID ULID offers a timestamp prefix followed by random characters. It allows efficient sorting and is more compact than UUIDs. References Usage of uuids for database keys","title":"Database table indexes"},{"location":"utils/db-table-indexes/#database-table-indexes","text":"An important aspect of database design is the creation of indexes, specially those ones that are unique.","title":"Database Table Indexes"},{"location":"utils/db-table-indexes/#uuids","text":"UUIDS are a 128-bit number used to uniquely identify objects in a computer. In a database, UUIDs can be used as primary keys for tables. But the are some issues to take a look at: Performance : to organize indexes, the database needs to sort the data. This is a problem when using UUIDs because they are not sequential. Storage : UUIDs are 128 bits long, which is more than the 32 bits used by by auto-incrementing IDs. In a large database, this can lead to a significant amount of storage. On the other hand, UUIDs are unique across different databases and can be generated in different machines without the need to coordinate with each other.","title":"UUIDs"},{"location":"utils/db-table-indexes/#auto-incrementing-ids","text":"Auto-incrementing IDs are a good choice for primary keys in a database. They are sequential and can be used to organize indexes quickly and efficiently. They don't take up much storage space. It is a natural sequence which is easy to predict and identify. Ideally, when those ids are not exposed to the user.","title":"Auto-incrementing IDs"},{"location":"utils/db-table-indexes/#uuidsv7","text":"UUIDv7 is a new version of UUIDs that are sequential, solving performance issue on insert new records. It offers the best of both worlds: the uniqueness of UUIDs and the performance of auto-incrementing IDs.","title":"UUIDsv7"},{"location":"utils/db-table-indexes/#ulid","text":"ULID offers a timestamp prefix followed by random characters. It allows efficient sorting and is more compact than UUIDs.","title":"ULID"},{"location":"utils/db-table-indexes/#references","text":"Usage of uuids for database keys","title":"References"},{"location":"utils/ide/","text":"IDE (Integrated Development Environment) An IDE, or Integrated Development Environment, enables programmers to consolidate the different aspects of writing a computer program. It helps you editing your source code, debugging, and auto-completing. Sublime Simple, light and robust. It has everything I need to work with python. Installation by default on my .dotfiles VS Code VS Code is the perfect IDE for web development. It has a lot of plugins, customizable themes, and a lot of features. It can be used for other languages, it includes a terminal, and it is very fast. References VSCode Runme Sublime setup Sublime cheatsheet","title":"IDE"},{"location":"utils/ide/#ide-integrated-development-environment","text":"An IDE, or Integrated Development Environment, enables programmers to consolidate the different aspects of writing a computer program. It helps you editing your source code, debugging, and auto-completing.","title":"IDE (Integrated Development Environment)"},{"location":"utils/ide/#sublime","text":"Simple, light and robust. It has everything I need to work with python. Installation by default on my .dotfiles","title":"Sublime"},{"location":"utils/ide/#vs-code","text":"VS Code is the perfect IDE for web development. It has a lot of plugins, customizable themes, and a lot of features. It can be used for other languages, it includes a terminal, and it is very fast.","title":"VS Code"},{"location":"utils/ide/#references","text":"VSCode Runme Sublime setup Sublime cheatsheet","title":"References"},{"location":"utils/regex/","text":"REGEX Regex is short for Regular Expression. It helps to match, find or manage text. Start by typing OK in the Regex field to proceed to the first step and access the more detailed description. What is Regular Expressions Regex? Regular Expressions are a string of characters that express a search pattern. Often abbreviated as Regex or Regexp. It is especially used to find or replace words in texts. In addition, we can test whether a text complies with the rules we set. For example, let's say you have a list of filenames. And you only want to find files with the pdf extension. Following typing an expression ^\\w+\\.pdf$ will work. The meaning of the definitions in this expression will become clearer as the steps progress. Basic Matchers The character or word we want to find is written directly. It is similar to a normal search process. For example, to find the word curious in the text, type the same. The period . allows selecting any character, including special characters and spaces. Character Sets [abc] If one of the characters in a word can be various characters, we write it in square brackets [] with all alternative characters. For example, to write an expression that can find all the words in the text, type the characters a, e, i, o, u adjacently within square brackets [] . Negated Character Sets [^abc] To find all words in the text below, except for ber and bor, type e and o side by side after the caret ^ character inside square brackets [] . Character Sets: Alphanumeric Range Letter Range [a-z] To find the letters in the specified range, the starting letter and the ending letter are written in square brackets [] with a dash between them - . It is case-sensitive. Type the expression that will select all lowercase letters between e and o , including themselves. Character Sets: Digit Range Number Range [0-9] To find the numbers in the specified range, the starting number and the ending number are written in square brackets [] with a dash - between them. Write an expression that will select all numbers between 3 and 6, including themselves. Repetitions Some special characters are used to specify how many times a character will be repeated in the text. These special characters are the plus + , the asterisk * , and the question mark ? . Repetitions: Asterisk Asterisk * We put an asterisk * after a character to indicate that the character may either not match at all or can match many times. For example, indicate that the letter e should never occur in the text, or it can occur once or more side by side. Repetitions: The Plus Plus Sign + To indicate that a character can occur one or more times, we put a plus sign + after a character. For example, indicate that the letter e can occur one or more times in the text. Repetitions: The Question Mark Question Mark ? To indicate that a character is optional, we put a ? question mark after a character. For example, indicate that the following letter u is optional. Repetitions: Curly Braces To express a certain number of occurrences of a character, we write curly braces {n} along with how many times we want it to occur at the end. For example, indicate that the following letter e can occur only 2 times. To express at least a certain number of occurrences of a character, we write the end of the character at least how many times we want it to occur, with a comma , at the end, and inside curly braces {n, } . For example, indicate that the following letter e can occur at least 3 times. To express the occurrence of a character in a certain number range, we write curly braces {x,y} with the interval we want to go to the end. For example, indicate that the following letter e can only occur between 1 and 3. Grouping Parentheses ( ): Grouping We can group an expression and use these groups to reference or enforce some rules. To group an expression, we enclose () in parentheses. For now just group haa below. Group References Referencing a Group The words ha and haa are grouped below. The first group is used by writing \\1 to avoid rewriting. Here 1 denotes the order of grouping. Type \\2 at the end of the expression to refer to the second group. Non-capturing Grouping Parentheses (?: ) : Non-capturing Grouping You can group an expression and ensure that it is not captured by references. For example, below are two groups. However, the first group reference we denote with \\1 actually indicates the second group, as the first is a non-capturing group. Pipe Character | It allows to specify that an expression can be in different expressions. Thus, all possible statements are written separated by the pipe sign | . This differs from charset [abc] , charsets operate at the character level. Alternatives are at the expression level. For example, the following expression would select both cat and Cat. Add another pipe sign | to the end of the expression and type rat so that all words are selected. ### Escape Character \\ There are special characters that we use when writing regex. { } [ ] / \\ + * . $^ | ? Before we can select these characters themselves, we need to use an escape character \\ . For example, to select the dot . and asterisk * characters in the text, let's add an escape character \\ before it. Start of The String Caret Sign ^ : Selecting by Line Start. We were using [0-9] to find numbers. To find only numbers at the beginning of a line, prefix this expression with the ^ sign. End of The String Dollar Sign $ : Selecting by End of Line. Let's use the $ sign after the html value to find the html texts only at the end of the line. Alphanumeric Word Character \\w : Letter, Number and Underscore The expression \\w is used to find letters, numbers and underscore characters. Let's use the expression \\w to find word characters in the text. Non-alphanumeric Except Word Character \\W The expression \\W is used to find characters other than letters, numbers, and underscores. Digits Number Character \\d \\d is used to find only number characters. Non-digits Except Number Character \\D \\D is used to find non-numeric characters. Whitespace Characters Space Character \\s \\s is used to find only space characters. Non-whitespace Characters Except Space Character \\S \\S is used to find non-space characters. Lookaround If we want the phrase we're writing to come before or after another phrase, we need to \"lookaround\". Take the next step to learn how to \"lookaround\". Lookaround: Positive Lookahead Positive Lookahead: (?=) For example, we want to select the hour value in the text. Therefore, to select only the numerical values that have PM after them, we need to write the positive look-ahead expression (?=) after our expression. Include PM after the = sign inside the parentheses. Lookaround: Negative Lookahead Negative Lookahead: (?!) For example, we want to select numbers other than the hour value in the text. Therefore, we need to write the negative look-ahead (?!) expression after our expression to select only the numerical values that do not have PM after them. Include PM after the ! sign inside the parentheses. Lookaround: Positive Lookbehind Positive Lookbehind: (?<=) For example, we want to select the price value in the text. Therefore, to select only the number values that preceded by $ , we need to write the positive lookbehind expression (?<=) before our expression. Add \\$ after the = sign inside the brackets. Lookaround: Negative Lookbehind Negative Lookbehind: (?<!) For example, we want to select numbers in the text other than the price value. Therefore, to select only numeric values that are not preceded by $ , we need to write the negative lookbehind (?<!) before our expression. Add \\$ after the ! inside the brackets. Flags Flags change the output of the expression. That's why flags are also called modifiers. Determines whether the typed expression treats text as separate lines, is case sensitive, or finds all matches. Continue to the next step to learn the flags. Flags: Global The global flag causes the expression to select all matches. If not used it will only select the first match. Now enable the global flag to be able to select all matches. /g all matches Flags: Multiline Regex sees all text as one line. But we use the multiline flag to handle each line separately. In this way, the expressions we write according to the end of the linework separately for each line. Now enable the multiline flag to find all matches. /m multiline Flags: Case Insensitive In order to remove the case-sensitiveness of the expression we have written, we must activate the case-insensitive flag. /i case insensitive Greedy Matching Regex does a greedy match by default. This means that the matchmaking will be as long as possible. Check out the example below. It refers to any match that ends in r and can be any character preceded by it. But it does not stop at the first letter r . Lazy Matching Lazy matchmaking, unlike greedy matching, stops at the first matching. For example, in the example below, add a ? after * to find the first match that ends with the letter r and is preceded by any character. It means that this match will stop at the first letter r . References Learn Regex Cheatsheet Regex Crossword Regex library","title":"Regex"},{"location":"utils/regex/#regex","text":"Regex is short for Regular Expression. It helps to match, find or manage text. Start by typing OK in the Regex field to proceed to the first step and access the more detailed description.","title":"REGEX"},{"location":"utils/regex/#what-is-regular-expressions-regex","text":"Regular Expressions are a string of characters that express a search pattern. Often abbreviated as Regex or Regexp. It is especially used to find or replace words in texts. In addition, we can test whether a text complies with the rules we set. For example, let's say you have a list of filenames. And you only want to find files with the pdf extension. Following typing an expression ^\\w+\\.pdf$ will work. The meaning of the definitions in this expression will become clearer as the steps progress.","title":"What is Regular Expressions Regex?"},{"location":"utils/regex/#basic-matchers","text":"The character or word we want to find is written directly. It is similar to a normal search process. For example, to find the word curious in the text, type the same. The period . allows selecting any character, including special characters and spaces.","title":"Basic Matchers"},{"location":"utils/regex/#character-sets-abc","text":"If one of the characters in a word can be various characters, we write it in square brackets [] with all alternative characters. For example, to write an expression that can find all the words in the text, type the characters a, e, i, o, u adjacently within square brackets [] .","title":"Character Sets [abc]"},{"location":"utils/regex/#negated-character-sets-abc","text":"To find all words in the text below, except for ber and bor, type e and o side by side after the caret ^ character inside square brackets [] .","title":"Negated Character Sets [^abc]"},{"location":"utils/regex/#character-sets-alphanumeric-range","text":"Letter Range [a-z] To find the letters in the specified range, the starting letter and the ending letter are written in square brackets [] with a dash between them - . It is case-sensitive. Type the expression that will select all lowercase letters between e and o , including themselves.","title":"Character Sets: Alphanumeric Range"},{"location":"utils/regex/#character-sets-digit-range","text":"Number Range [0-9] To find the numbers in the specified range, the starting number and the ending number are written in square brackets [] with a dash - between them. Write an expression that will select all numbers between 3 and 6, including themselves.","title":"Character Sets: Digit Range"},{"location":"utils/regex/#repetitions","text":"Some special characters are used to specify how many times a character will be repeated in the text. These special characters are the plus + , the asterisk * , and the question mark ? .","title":"Repetitions"},{"location":"utils/regex/#repetitions-asterisk","text":"Asterisk * We put an asterisk * after a character to indicate that the character may either not match at all or can match many times. For example, indicate that the letter e should never occur in the text, or it can occur once or more side by side.","title":"Repetitions: Asterisk"},{"location":"utils/regex/#repetitions-the-plus","text":"Plus Sign + To indicate that a character can occur one or more times, we put a plus sign + after a character. For example, indicate that the letter e can occur one or more times in the text.","title":"Repetitions: The Plus"},{"location":"utils/regex/#repetitions-the-question-mark","text":"Question Mark ? To indicate that a character is optional, we put a ? question mark after a character. For example, indicate that the following letter u is optional.","title":"Repetitions: The Question Mark"},{"location":"utils/regex/#repetitions-curly-braces","text":"To express a certain number of occurrences of a character, we write curly braces {n} along with how many times we want it to occur at the end. For example, indicate that the following letter e can occur only 2 times. To express at least a certain number of occurrences of a character, we write the end of the character at least how many times we want it to occur, with a comma , at the end, and inside curly braces {n, } . For example, indicate that the following letter e can occur at least 3 times. To express the occurrence of a character in a certain number range, we write curly braces {x,y} with the interval we want to go to the end. For example, indicate that the following letter e can only occur between 1 and 3.","title":"Repetitions: Curly Braces"},{"location":"utils/regex/#grouping","text":"","title":"Grouping"},{"location":"utils/regex/#parentheses-grouping","text":"We can group an expression and use these groups to reference or enforce some rules. To group an expression, we enclose () in parentheses. For now just group haa below.","title":"Parentheses ( ): Grouping"},{"location":"utils/regex/#group-references","text":"Referencing a Group The words ha and haa are grouped below. The first group is used by writing \\1 to avoid rewriting. Here 1 denotes the order of grouping. Type \\2 at the end of the expression to refer to the second group.","title":"Group References"},{"location":"utils/regex/#non-capturing-grouping","text":"Parentheses (?: ) : Non-capturing Grouping You can group an expression and ensure that it is not captured by references. For example, below are two groups. However, the first group reference we denote with \\1 actually indicates the second group, as the first is a non-capturing group.","title":"Non-capturing Grouping"},{"location":"utils/regex/#pipe-character","text":"It allows to specify that an expression can be in different expressions. Thus, all possible statements are written separated by the pipe sign | . This differs from charset [abc] , charsets operate at the character level. Alternatives are at the expression level. For example, the following expression would select both cat and Cat. Add another pipe sign | to the end of the expression and type rat so that all words are selected. ### Escape Character \\ There are special characters that we use when writing regex. { } [ ] / \\ + * . $^ | ? Before we can select these characters themselves, we need to use an escape character \\ . For example, to select the dot . and asterisk * characters in the text, let's add an escape character \\ before it.","title":"Pipe Character |"},{"location":"utils/regex/#start-of-the-string","text":"Caret Sign ^ : Selecting by Line Start. We were using [0-9] to find numbers. To find only numbers at the beginning of a line, prefix this expression with the ^ sign.","title":"Start of The String"},{"location":"utils/regex/#end-of-the-string","text":"Dollar Sign $ : Selecting by End of Line. Let's use the $ sign after the html value to find the html texts only at the end of the line.","title":"End of The String"},{"location":"utils/regex/#alphanumeric","text":"Word Character \\w : Letter, Number and Underscore The expression \\w is used to find letters, numbers and underscore characters. Let's use the expression \\w to find word characters in the text.","title":"Alphanumeric"},{"location":"utils/regex/#non-alphanumeric","text":"Except Word Character \\W The expression \\W is used to find characters other than letters, numbers, and underscores.","title":"Non-alphanumeric"},{"location":"utils/regex/#digits","text":"Number Character \\d \\d is used to find only number characters.","title":"Digits"},{"location":"utils/regex/#non-digits","text":"Except Number Character \\D \\D is used to find non-numeric characters.","title":"Non-digits"},{"location":"utils/regex/#whitespace-characters","text":"Space Character \\s \\s is used to find only space characters.","title":"Whitespace Characters"},{"location":"utils/regex/#non-whitespace-characters","text":"Except Space Character \\S \\S is used to find non-space characters.","title":"Non-whitespace Characters"},{"location":"utils/regex/#lookaround","text":"If we want the phrase we're writing to come before or after another phrase, we need to \"lookaround\". Take the next step to learn how to \"lookaround\".","title":"Lookaround"},{"location":"utils/regex/#lookaround-positive-lookahead","text":"Positive Lookahead: (?=) For example, we want to select the hour value in the text. Therefore, to select only the numerical values that have PM after them, we need to write the positive look-ahead expression (?=) after our expression. Include PM after the = sign inside the parentheses.","title":"Lookaround: Positive Lookahead"},{"location":"utils/regex/#lookaround-negative-lookahead","text":"Negative Lookahead: (?!) For example, we want to select numbers other than the hour value in the text. Therefore, we need to write the negative look-ahead (?!) expression after our expression to select only the numerical values that do not have PM after them. Include PM after the ! sign inside the parentheses.","title":"Lookaround: Negative Lookahead"},{"location":"utils/regex/#lookaround-positive-lookbehind","text":"Positive Lookbehind: (?<=) For example, we want to select the price value in the text. Therefore, to select only the number values that preceded by $ , we need to write the positive lookbehind expression (?<=) before our expression. Add \\$ after the = sign inside the brackets.","title":"Lookaround: Positive Lookbehind"},{"location":"utils/regex/#lookaround-negative-lookbehind","text":"Negative Lookbehind: (?<!) For example, we want to select numbers in the text other than the price value. Therefore, to select only numeric values that are not preceded by $ , we need to write the negative lookbehind (?<!) before our expression. Add \\$ after the ! inside the brackets.","title":"Lookaround: Negative Lookbehind"},{"location":"utils/regex/#flags","text":"Flags change the output of the expression. That's why flags are also called modifiers. Determines whether the typed expression treats text as separate lines, is case sensitive, or finds all matches. Continue to the next step to learn the flags.","title":"Flags"},{"location":"utils/regex/#flags-global","text":"The global flag causes the expression to select all matches. If not used it will only select the first match. Now enable the global flag to be able to select all matches. /g all matches","title":"Flags: Global"},{"location":"utils/regex/#flags-multiline","text":"Regex sees all text as one line. But we use the multiline flag to handle each line separately. In this way, the expressions we write according to the end of the linework separately for each line. Now enable the multiline flag to find all matches. /m multiline","title":"Flags: Multiline"},{"location":"utils/regex/#flags-case-insensitive","text":"In order to remove the case-sensitiveness of the expression we have written, we must activate the case-insensitive flag. /i case insensitive","title":"Flags: Case Insensitive"},{"location":"utils/regex/#greedy-matching","text":"Regex does a greedy match by default. This means that the matchmaking will be as long as possible. Check out the example below. It refers to any match that ends in r and can be any character preceded by it. But it does not stop at the first letter r .","title":"Greedy Matching"},{"location":"utils/regex/#lazy-matching","text":"Lazy matchmaking, unlike greedy matching, stops at the first matching. For example, in the example below, add a ? after * to find the first match that ends with the letter r and is preceded by any character. It means that this match will stop at the first letter r .","title":"Lazy Matching"},{"location":"utils/regex/#references","text":"Learn Regex Cheatsheet Regex Crossword Regex library","title":"References"},{"location":"utils/supervisor/","text":"Supervisor Service How to create new supervisor service First of all, you need to install supervisor service: sudo apt-get install supervisor -y Path to copy configurations files: /etc/supervisor/conf.d/* After copy file, reread supervisor configuration: sudo supervisorctl -c /etc/supervisor/supervisord.conf reread sudo supervisorctl -c /etc/supervisor/supervisord.conf reload If something went wrong, sock may be deleted in /var/run/supervisor.conf, and supervisorctl status will show an error. To solve, check new configuration files, if something is buggy, fix it or remove conf and run command again. Then, run, to reload new configuration and start service again with: sudo supervisorctl reload Now, your supervisor service should be up and running References Some interesting samples and documentation Configuration Sample Documentation","title":"Supervisor"},{"location":"utils/supervisor/#supervisor-service","text":"","title":"Supervisor Service"},{"location":"utils/supervisor/#how-to-create-new-supervisor-service","text":"First of all, you need to install supervisor service: sudo apt-get install supervisor -y Path to copy configurations files: /etc/supervisor/conf.d/* After copy file, reread supervisor configuration: sudo supervisorctl -c /etc/supervisor/supervisord.conf reread sudo supervisorctl -c /etc/supervisor/supervisord.conf reload If something went wrong, sock may be deleted in /var/run/supervisor.conf, and supervisorctl status will show an error. To solve, check new configuration files, if something is buggy, fix it or remove conf and run command again. Then, run, to reload new configuration and start service again with: sudo supervisorctl reload Now, your supervisor service should be up and running","title":"How to create new supervisor service"},{"location":"utils/supervisor/#references","text":"Some interesting samples and documentation Configuration Sample Documentation","title":"References"},{"location":"utils/task-runner/","text":"Task Runner A task runner is a tool that automates and simplify the execution of repetitive tasks. It is a command-line tool that can be used to run tasks defined in a configuration file. The tasks can be anything, from compiling code to running tests to deploying applications. As configuration file is pushed to the repository together with the code, it is easy to share the tasks with other members of the team, to ensure that everyone is running tasks in the same way. This can help to reduce errors and improve consistency in the way that tasks are run. Makefile Makefile is a build automation tool that helps to compile and build projects. It is a simple way to define tasks and dependencies between them. Makefile is a good choice for simple projects, but it can become complex and hard to maintain as the project grows. Only supported in Linux and MacOS. Task file Task file is a task runner that is designed to be a simpler version of Makefile. It is a YAML-based task runner that is easy to read and write. Task file is a good choice for projects that need a simple way to define tasks and dependencies between them. It is written in go, and supports Linux, MacOS, and Windows.","title":"Task runner"},{"location":"utils/task-runner/#task-runner","text":"A task runner is a tool that automates and simplify the execution of repetitive tasks. It is a command-line tool that can be used to run tasks defined in a configuration file. The tasks can be anything, from compiling code to running tests to deploying applications. As configuration file is pushed to the repository together with the code, it is easy to share the tasks with other members of the team, to ensure that everyone is running tasks in the same way. This can help to reduce errors and improve consistency in the way that tasks are run.","title":"Task Runner"},{"location":"utils/task-runner/#makefile","text":"Makefile is a build automation tool that helps to compile and build projects. It is a simple way to define tasks and dependencies between them. Makefile is a good choice for simple projects, but it can become complex and hard to maintain as the project grows. Only supported in Linux and MacOS.","title":"Makefile"},{"location":"utils/task-runner/#task-file","text":"Task file is a task runner that is designed to be a simpler version of Makefile. It is a YAML-based task runner that is easy to read and write. Task file is a good choice for projects that need a simple way to define tasks and dependencies between them. It is written in go, and supports Linux, MacOS, and Windows.","title":"Task file"},{"location":"utils/testing/","text":"Testing Testing is equally important as writing the code. It is a way to ensure that the code is working as expected and to catch any bugs that may have been introduced before going to a live environment and affect the final user. It can be either manual or automated. Manual tests are usually done by people vs automated tests are done by machines. No matter if you are in a small team or in a big company, testing is a must . We are human after all, and mistakes can happens. There are different types of tests, where each layer described is important to cover because each has its own purpose. The most common types of tests are: Unit test Cover the smallest piece of code, usually a function or a method. The goal is to test the code in isolation, without any dependencies. Making sure any changes in the code will not break the expected behavior. Test should be fast and deterministic (same result over and over). It should not depend on any external resource, such as a database, network, or file system. Coverage Coverage is a metric that measures the amount of code that is covered by tests. It generates a report showing which are the total lines of code, which are covered by tests and which are not. The higher the coverage, the better. But it is important to note that 100% coverage does not mean that the code is bug free. It only means that all lines are covered by tests. It is a good idea to check the coverage on your tests. If any line is not covered, means that test may not be completed or that the code is not being used, given you the opportunity to remove it. Mocking When testing logic that depends on external resources, such as a database or a external HTTP request, it is a good practice to mock the response. This way, the test is not dependent on the external resource, giving faster and deterministic tests. Fixtures Fixtures are a way to set up and tear down resources that are used in the tests. It is a way to avoid code repetition and to keep the tests clean, ensuring that data is consistent across all tests. Parametrized tests Parametrized tests are a way to run the same test with different inputs. Integration test Integration tests are used to test the interaction between different parts of the application. It is a way to ensure that the different parts are working together as expected. End-to-end test Tests the whole application against a live production-like server. It is a way to ensure that the application is working as expected from the user's perspective. Usually, it is done with a browser automation tool, such as Selenium. In case of an API, you can automate with Postman. If is not possible to automate, it can be done manually. These tests are slow and hard to create and maintain, but are important to ensure that the application is working as expected. Smoke test Smoke tests are a subset of end-to-end tests. They are used to ensure that the the most important features of the application are working as expected, preventing the application to be usable. Smoked tests have a limited scope, completed in a short time. Regression test Confirms that previously developed and tested features in an application still works correctly after any changes are introduced. Regression tests have wider scope and covers all areas in the application, taking longer to complete. Performance test Performance tests are testing methods to measure how the system behaves and performs under a particular load. Load test Load testing is the process of checking the behavior of the system under an anticipated load. Stress test The goal of stress testing is to identify the saturation point and the first bottleneck of the application under test. Once is identified, it is possible to patch that part of the system to avoid the bottleneck. Spike Tests Test the system's reaction to sudden and extreme changes in load. References Testing in python Load testing","title":"Testing"},{"location":"utils/testing/#testing","text":"Testing is equally important as writing the code. It is a way to ensure that the code is working as expected and to catch any bugs that may have been introduced before going to a live environment and affect the final user. It can be either manual or automated. Manual tests are usually done by people vs automated tests are done by machines. No matter if you are in a small team or in a big company, testing is a must . We are human after all, and mistakes can happens. There are different types of tests, where each layer described is important to cover because each has its own purpose. The most common types of tests are:","title":"Testing"},{"location":"utils/testing/#unit-test","text":"Cover the smallest piece of code, usually a function or a method. The goal is to test the code in isolation, without any dependencies. Making sure any changes in the code will not break the expected behavior. Test should be fast and deterministic (same result over and over). It should not depend on any external resource, such as a database, network, or file system.","title":"Unit test"},{"location":"utils/testing/#coverage","text":"Coverage is a metric that measures the amount of code that is covered by tests. It generates a report showing which are the total lines of code, which are covered by tests and which are not. The higher the coverage, the better. But it is important to note that 100% coverage does not mean that the code is bug free. It only means that all lines are covered by tests. It is a good idea to check the coverage on your tests. If any line is not covered, means that test may not be completed or that the code is not being used, given you the opportunity to remove it.","title":"Coverage"},{"location":"utils/testing/#mocking","text":"When testing logic that depends on external resources, such as a database or a external HTTP request, it is a good practice to mock the response. This way, the test is not dependent on the external resource, giving faster and deterministic tests.","title":"Mocking"},{"location":"utils/testing/#fixtures","text":"Fixtures are a way to set up and tear down resources that are used in the tests. It is a way to avoid code repetition and to keep the tests clean, ensuring that data is consistent across all tests.","title":"Fixtures"},{"location":"utils/testing/#parametrized-tests","text":"Parametrized tests are a way to run the same test with different inputs.","title":"Parametrized tests"},{"location":"utils/testing/#integration-test","text":"Integration tests are used to test the interaction between different parts of the application. It is a way to ensure that the different parts are working together as expected.","title":"Integration test"},{"location":"utils/testing/#end-to-end-test","text":"Tests the whole application against a live production-like server. It is a way to ensure that the application is working as expected from the user's perspective. Usually, it is done with a browser automation tool, such as Selenium. In case of an API, you can automate with Postman. If is not possible to automate, it can be done manually. These tests are slow and hard to create and maintain, but are important to ensure that the application is working as expected.","title":"End-to-end test"},{"location":"utils/testing/#smoke-test","text":"Smoke tests are a subset of end-to-end tests. They are used to ensure that the the most important features of the application are working as expected, preventing the application to be usable. Smoked tests have a limited scope, completed in a short time.","title":"Smoke test"},{"location":"utils/testing/#regression-test","text":"Confirms that previously developed and tested features in an application still works correctly after any changes are introduced. Regression tests have wider scope and covers all areas in the application, taking longer to complete.","title":"Regression test"},{"location":"utils/testing/#performance-test","text":"Performance tests are testing methods to measure how the system behaves and performs under a particular load.","title":"Performance test"},{"location":"utils/testing/#load-test","text":"Load testing is the process of checking the behavior of the system under an anticipated load.","title":"Load test"},{"location":"utils/testing/#stress-test","text":"The goal of stress testing is to identify the saturation point and the first bottleneck of the application under test. Once is identified, it is possible to patch that part of the system to avoid the bottleneck.","title":"Stress test"},{"location":"utils/testing/#spike-tests","text":"Test the system's reaction to sudden and extreme changes in load.","title":"Spike Tests"},{"location":"utils/testing/#references","text":"Testing in python Load testing","title":"References"},{"location":"utils/transmission/","text":"Transmission Transmission is a torrent client easy to install and configure to access via interface from outside raspberry. How to install and configure Execute following command in raspberry via ssh: sudo apt-get install transmission-daemon Transmission daemon needs to be updated with some of our configuration. For this, we need to edit configuration file. First, stop daemon sudo systemctl stop transmission-daemon Open and edit configuration file path should be either ~/.config/transmission-daemon/services.json or /etc/transmission/daemon.json After file is updated, we need to restart daemon so configuration is loaded and also start transmission daemon sudo systemctl daemon-reload sudo systemctl start transmission-daemon Now everything should be ready to go, and transmission will be accessible via interface from another computer sharing network with raspberry. References Edit transmission config file Useful info","title":"Transmission"},{"location":"utils/transmission/#transmission","text":"Transmission is a torrent client easy to install and configure to access via interface from outside raspberry.","title":"Transmission"},{"location":"utils/transmission/#how-to-install-and-configure","text":"Execute following command in raspberry via ssh: sudo apt-get install transmission-daemon Transmission daemon needs to be updated with some of our configuration. For this, we need to edit configuration file. First, stop daemon sudo systemctl stop transmission-daemon Open and edit configuration file path should be either ~/.config/transmission-daemon/services.json or /etc/transmission/daemon.json After file is updated, we need to restart daemon so configuration is loaded and also start transmission daemon sudo systemctl daemon-reload sudo systemctl start transmission-daemon Now everything should be ready to go, and transmission will be accessible via interface from another computer sharing network with raspberry.","title":"How to install and configure"},{"location":"utils/transmission/#references","text":"Edit transmission config file Useful info","title":"References"},{"location":"utils/ubuntu/","text":"Ubuntu I am using ubuntu, a linux distro based on Debian. It is open source, with a big community and support for common issues. Oh my zsh Ubuntu has a default shell, to run different commands, but there are others like zsh. To add more power to the shell, I used Oh my zsh on top. It is open source, big collaborative community, customizable and with many plugins to enable to enrich the experience of working with a terminal. dotfiles Private files in a Linux os. Usually, they contain configuration used by different tools, like terminal, profile, k8s, git... It can be used to speed up installation of a new OS from scratch. Totally customizable with my needs. References Linux commands cheatsheet Oh my zsh Bash/zsh shortcuts Getting started with .dotfiles","title":"Ubuntu"},{"location":"utils/ubuntu/#ubuntu","text":"I am using ubuntu, a linux distro based on Debian. It is open source, with a big community and support for common issues.","title":"Ubuntu"},{"location":"utils/ubuntu/#oh-my-zsh","text":"Ubuntu has a default shell, to run different commands, but there are others like zsh. To add more power to the shell, I used Oh my zsh on top. It is open source, big collaborative community, customizable and with many plugins to enable to enrich the experience of working with a terminal.","title":"Oh my zsh"},{"location":"utils/ubuntu/#dotfiles","text":"Private files in a Linux os. Usually, they contain configuration used by different tools, like terminal, profile, k8s, git... It can be used to speed up installation of a new OS from scratch. Totally customizable with my needs.","title":"dotfiles"},{"location":"utils/ubuntu/#references","text":"Linux commands cheatsheet Oh my zsh Bash/zsh shortcuts Getting started with .dotfiles","title":"References"},{"location":"utils/working-environment/","text":"Working Environment Structure Create a folder named work in home directory: mkdir $HOME/work Inside, create following structure: mkdir $HOME/work/src : to store repositories mkdir $HOME/work/projects/<company> : to store info from company, onboarding, one to one... folder for Company. Store date in the name of the file, such as meeting-20XX-XX-XX.md mkdir $HOME/work/projects/<project-name> : One extra folder for each project. Some common subfolder may be: scripts , documents , sublime mkdir $HOME/work/utils : to store programs, binary, or other packages References Project template","title":"Working Environment"},{"location":"utils/working-environment/#working-environment","text":"","title":"Working Environment"},{"location":"utils/working-environment/#structure","text":"Create a folder named work in home directory: mkdir $HOME/work Inside, create following structure: mkdir $HOME/work/src : to store repositories mkdir $HOME/work/projects/<company> : to store info from company, onboarding, one to one... folder for Company. Store date in the name of the file, such as meeting-20XX-XX-XX.md mkdir $HOME/work/projects/<project-name> : One extra folder for each project. Some common subfolder may be: scripts , documents , sublime mkdir $HOME/work/utils : to store programs, binary, or other packages","title":"Structure"},{"location":"utils/working-environment/#references","text":"Project template","title":"References"}]}